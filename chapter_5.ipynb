{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccfc270b",
   "metadata": {},
   "source": [
    "# Handling Local Data\n",
    "To load datasets that are stored either on your laptop or on a remote server, we can still use the `load_dataset()` function. This time, we just need to specify the type of loading script in the `load_dataset()` function, along with a `data_files=''` argument that specifies the path to one or more files.\n",
    "\n",
    "![\"load_dataset()\"](data/chapter_5/load_dataset.png \"load_dataset()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99872279",
   "metadata": {},
   "source": [
    "### Loading a local dataset\n",
    "\n",
    "| Data format | Loading script | Example |\n",
    "|-------------|----------------|---------|\n",
    "| CSV & TSV |`csv`|`load_dataset(\"csv\", data_files=\"my_file.csv\")`|\n",
    "| Text files |`text`|`load_dataset(\"text\", data_files=\"my_file.txt\")`|\n",
    "| JSON & JSON Lines |`json`|`load_dataset(\"json\", data_files=\"my_file.json\")`|\n",
    "| Pickled DataFrames |`pandas`|`load_dataset(\"pandas\", data_files=\"my_dataframe.pkl\")`|\n",
    "\n",
    "For this example, let's use the [SQuAD-it](https://github.com/crux82/squad-it/) dataset, which is a large-scale **json** dataset for question answering in Italian. It's hosted on GitHub, let's first download it in our `data/chapter_5` dir using `wget` and then decompress these compressed files `SQuAD_it-train.json.gz`, `SQuAD_it-test.json.gz` using `gzip`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef67e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd data/chapter_5 && wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-train.json.gz\n",
    "!cd data/chapter_5 && wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-test.json.gz\n",
    "\n",
    "!cd data/chapter_5 && gzip -dkv SQuAD_it-*.json.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfbe4aa",
   "metadata": {},
   "source": [
    "Now that we have our data in the `JSON` format, we can simply use the `load_dataset()` function, we just need to know if we’re dealing with **ordinary JSON** (*similar to a nested dictionary*) or **JSON Lines** (*line-separated JSON*). Like many question answering datasets, **SQuAD-it** uses the *nested format*, with all the text stored in a **data field**. This means we can load the dataset by specifying the `field='data'` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102036ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "squad_it_dataset = load_dataset(\"json\", data_files=\"data/chapter_5/SQuAD_it-train.json\", field=\"data\")\n",
    "\n",
    "squad_it_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c461d993",
   "metadata": {},
   "source": [
    "As we can see, by default, loading local files creates a `DatasetDict` object with only a **train** split. But, what we really want is to include both the **train** and **test** splits in a single `DatasetDict` object so we can apply `Dataset.map()` functions across both splits at once. To do this, we can provide a dictionary to the \n",
    "```python\n",
    "data_files={\"train\":\"path to the training data\", \"test\":\"path to the testing data\"}\n",
    "```\n",
    "argument that maps each split name to a file associated with that split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc636f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {\n",
    "    \"train\":\"data/chapter_5/SQuAD_it-train.json\",\n",
    "    \"test\":\"data/chapter_5/SQuAD_it-test.json\"\n",
    "}\n",
    "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")\n",
    "squad_it_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb86667",
   "metadata": {},
   "source": [
    "The loading scripts in Datasets actually support automatic decompression of the input files, so we could have skipped the use of gzip by pointing the `data_files` argument directly to the compressed files:\n",
    "```python\n",
    "data_files = {\n",
    "    \"train\": \"data/chapter_5/SQuAD_it-train.json.gz\", \n",
    "    \"test\": \"data/chapter_5/SQuAD_it-test.json.gz\"\n",
    "}\n",
    "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")\n",
    "```\n",
    "This can be useful if you don’t want to manually decompress many `GZIP` files. The automatic decompression also applies to other common formats like `ZIP` and `TAR`, so you just need to point `data_files` to the compressed files.\n",
    "\n",
    "> The `data_files` argument is also quite flexible and can be either *a single file path*, *a list of file paths*, or *a dictionary* that maps split names to file paths. You can also *glob files* that match a *specified pattern* according to the rules used by the `Unix shell` (e.g., you can glob all the `JSON` files in a directory as a single split by setting `data_files=\"*.json\"`). See the [Datasets documentation](https://huggingface.co/docs/datasets/loading#local-and-remote-files) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a5facc",
   "metadata": {},
   "source": [
    "### Loading a remote dataset\n",
    "\n",
    "Fortunately, loading *remote files* is just as simple as loading *local* ones!\n",
    "<br />\n",
    "Instead of providing a path to *local files*, we point the `data_files` argument to **one or more URLs** where the *remote files* are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf78445",
   "metadata": {},
   "outputs": [],
   "source": [
    "url =  \"https://github.com/crux82/squad-it/raw/master/\"\n",
    "\n",
    "data_files = {\n",
    "    \"train\": url + \"SQuAD_it-train.json.gz\",\n",
    "    \"test\": url + \"SQuAD_it-test.json.gz\",\n",
    "}\n",
    "\n",
    "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")\n",
    "squad_it_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7a6b66",
   "metadata": {},
   "source": [
    "# Data Manipulation\n",
    "\n",
    "The `DatasetDict` object comes with a lot of functionalities to manipulate the original dataset.\n",
    "<br />\n",
    "For this example, we’ll use the [Drug Review Dataset](https://archive.ics.uci.edu/ml/datasets/Drug+Review+Dataset+%28Drugs.com%29) that’s hosted on the [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php), which contains patient reviews on various drugs, along with the condition being treated and a 10-star rating of the patient’s satisfaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d60332",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd data/chapter_5/ && wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip\"\n",
    "!cd data/chapter_5/ && unzip drugsCom_raw.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bef2b6b",
   "metadata": {},
   "source": [
    "As we can see, this the data is in the `TSV` format which is a variant of `CSV` that uses tabs instead of commas as the separator. So, when loading these files using `load_dataset()`, we use the specify `csv` as the *loading script* and most importantly the `delimiter=\\t` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a62b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_files = {\n",
    "    \"train\" : \"data/chapter_5/drugsComTrain_raw.tsv\",\n",
    "    \"test\" : \"data/chapter_5/drugsComTest_raw.tsv\"\n",
    "}\n",
    "\n",
    "drug_dataset = load_dataset(\"csv\", data_files=data_files, delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901948e7",
   "metadata": {},
   "source": [
    "Now that we have the `DatasetDict` object, we can create a random sample to get a quick feel for the type of data you’re working with and to do so we simply have to chain the `Dataset.shuffle()` and `Dataset.select()` function to first randomly shuffle the data  (we can also pass the `seed` argument to later use the same shuffle) and select/see the first *n* data elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd508b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_sample = drug_dataset[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "\n",
    "drug_sample[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0004751",
   "metadata": {},
   "source": [
    "From above we can see before passing this data to the model or even for tokenisation we need to perform few pre-processing steps:\n",
    "  + The `Unnamed: 0` column needs to be renamed to `patient_id`.\n",
    "  + The `condition` column includes a mix of *uppercase* and *lowercase* labels.\n",
    "  + The `reviews` are of varying length and contain a mix of Python line separators `(\\r\\n)` as well as HTML character codes like `&\\#039;`.\n",
    "\n",
    "So, we can use the in-built functions like the, `rename_column()` - to rename the column name, `map()` and `filter()` - to map all the `condition` column values to lowercase, and also filter out the special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7b5c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "\n",
    "# rename the column name\n",
    "drug_dataset = drug_dataset.rename_column(\n",
    "    original_column_name=\"Unnamed: 0\",\n",
    "    new_column_name=\"patient_id\"\n",
    ")\n",
    "\n",
    "# map conditon column values to lowercase\n",
    "def lowercase_condition(data):\n",
    "    return {\"condition\": [row.lower() for row in data[\"condition\"]]}\n",
    "    # return {\"condition\": data[\"condition\"].lower()} # if not using batched=True in the map() function\n",
    "    \n",
    "\n",
    "# let's first remove all the rows with null values, otherwise the above\n",
    "# function will throw an error\n",
    "drug_dataset = drug_dataset.filter(lambda x: x[\"condition\"] is not None)\n",
    "\n",
    "# map lowercasse\n",
    "drug_dataset = drug_dataset.map(lowercase_condition, batched=True)\n",
    "\n",
    "\n",
    "# unescape all the HTML special characters in our corpus\n",
    "drug_dataset =  drug_dataset.map(\n",
    "    lambda x: {\"review\": [html.unescape(row) for row in x[\"review\"]]},\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "\n",
    "drug_dataset[\"train\"][:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a75b07",
   "metadata": {},
   "source": [
    ">In Python, `lambda` functions are small functions that you can define without explicitly naming them. They take the general form `lambda <arguments> : <expression>`,\n",
    "where `lambda` is one of Python’s special keywords, `<arguments>` is a list/set of *comma-separated values* that define the *inputs* to the function, and `<expression>` represents the operations you wish to execute. For example, we can define a simple lambda function that squares a number as follows: `lambda x : x * x`\n",
    "To apply this function to an input, we need to wrap it and the input in parentheses:\n",
    "`(lambda x: x * x)(3) -> 9`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d1d981",
   "metadata": {},
   "source": [
    "### From Datasets to DataFrames and back\n",
    "\n",
    "We can use the the `set_format()` function of the `DatasetDict` object to convert it into a different dataframe such as *Pandas*, *NumPy*, *PyTorch*, *TensorFlow*, and *JAX*. To convert it back to the `DatasetDict` object, we simply need to call the `reset_format()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a106953",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_dataset.set_format(\"pandas\")\n",
    "\n",
    "drug_dataset[\"train\"][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce5b9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_dataset.reset_format()\n",
    "\n",
    "drug_dataset[\"train\"][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cd2206",
   "metadata": {},
   "source": [
    "### Creating a validation set\n",
    "The `DatasetDict` object also provides a `Dataset.train_test_split()` function that is based on the famous functionality from `scikit-learn` which can be used to further split the data into a train-validation-test format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26ca1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80-20 percent train-validation split on the training dataset\n",
    "drug_dataset_clean = drug_dataset[\"train\"].train_test_split(train_size=0.8, seed=41)\n",
    "\n",
    "# name the 20% split data as the validation\n",
    "drug_dataset_clean[\"validation\"] = drug_dataset_clean.pop(\"test\")\n",
    "\n",
    "# Add the orignal test dataset\n",
    "drug_dataset_clean[\"test\"] = drug_dataset[\"test\"]\n",
    "\n",
    "drug_dataset_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5393b117",
   "metadata": {},
   "source": [
    "### Saving a dataset\n",
    "To save a dataset to disk:\n",
    "\n",
    "| Data format | Function |\n",
    "|-------------|----------|\n",
    "|*Arrow*|`Dataset.save_to_disk()`|\n",
    "|*CSV*|`Dataset.to_csv()`|\n",
    "|*JSON*|`Dataset.to_json()`|\n",
    "\n",
    "For example, let’s save our cleaned dataset in the Arrow format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fa6c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_dataset_clean.save_to_disk(\"data/chapter_5/drug-reviews\")\n",
    "\n",
    "!ls data/chapter_5/drug-reviews/*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dba53d",
   "metadata": {},
   "source": [
    "Once the dataset is saved, we can load it by using the load_from_disk() function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16a19ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "drug_dataset_reloaded = load_from_disk(\"data/chapter_5/drug-reviews\")\n",
    "drug_dataset_reloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7a153a",
   "metadata": {},
   "source": [
    "For the **CSV** and **JSON** formats, we have to store each split as a separate file. One way to do this is by iterating over the keys and values in the `DatasetDict` object. This saves each split in JSON Lines format, where each row in the dataset is stored as a single line of JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feec7411",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split, dataset in drug_dataset_clean.items():\n",
    "    dataset.to_json(f\"data/chapter_5/drug-reviews-{split}.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d5062f",
   "metadata": {},
   "source": [
    "And to load the data we can simply use the `load_dataset()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474c446e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {\n",
    "    \"train\": \"data/chapter_5/drug-reviews-train.jsonl\",\n",
    "    \"validation\": \"data/chapter_5/drug-reviews-validation.jsonl\",\n",
    "    \"test\": \"data/chapter_5/drug-reviews-test.jsonl\",\n",
    "}\n",
    "drug_dataset_reloaded = load_dataset(\"json\", data_files=data_files)\n",
    "\n",
    "drug_dataset_reloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e41097",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "Let's train a classifier that can predict the patient condition based on the drug review.\n",
    "\n",
    "1. Downloading and preparing the data (no need to run the cell below, if `drug_dataset_reloaded` object is still there or simply run the above cell if the right files are there):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8ea7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import html\n",
    "\n",
    "# download the data\n",
    "!cd data/chapter_5/ && curl -O \"https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip\"\n",
    "!cd data/chapter_5/ && unzip -o drugsCom_raw.zip\n",
    "\n",
    "\n",
    "# load the data\n",
    "data_files = {\n",
    "    \"train\" : \"data/chapter_5/drugsComTrain_raw.tsv\",\n",
    "    \"test\" : \"data/chapter_5/drugsComTest_raw.tsv\"\n",
    "}\n",
    "\n",
    "drug_dataset = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files=data_files,\n",
    "    delimiter='\\t'\n",
    ")\n",
    "\n",
    "\n",
    "# filter out the data\n",
    "## rename the column\n",
    "drug_dataset = drug_dataset.rename_column(\n",
    "    original_column_name=\"Unnamed: 0\",\n",
    "    new_column_name=\"patient_id\"\n",
    ")\n",
    "\n",
    "## map condition column values to lowercase\n",
    "def lowercase_condition(data):\n",
    "    return {\"condition\": [row.lower() for row in data[\"condition\"]]}\n",
    "\n",
    "## filter out the null rows\n",
    "drug_dataset = drug_dataset.filter(lambda x: x[\"condition\"] is not None)\n",
    "\n",
    "## map lowercase\n",
    "drug_dataset = drug_dataset.map(lowercase_condition, batched=True)\n",
    "\n",
    "\n",
    "# unescape all the special characters\n",
    "drug_dataset = drug_dataset.map(\n",
    "    lambda x: {\"review\": [html.unescape(review_row) for review_row in x[\"review\"]]},\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "drug_dataset = drug_dataset.map(\n",
    "    lambda x: {\"condition\": [html.unescape(condition_row) for condition_row in x[\"condition\"]]},\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "\n",
    "# Creating validation set\n",
    "drug_dataset_clean = drug_dataset[\"train\"].train_test_split(\n",
    "    train_size=0.8,\n",
    "    seed=41\n",
    ")\n",
    "\n",
    "drug_dataset_clean[\"validation\"] = drug_dataset_clean.pop(\"test\")\n",
    "\n",
    "\n",
    "drug_dataset_clean[\"test\"] = drug_dataset[\"test\"]\n",
    "\n",
    "drug_dataset_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd98748",
   "metadata": {},
   "source": [
    "Since there are many values in the `condition` column, let's first have a look at all of them and only select the first 5 conditions that occurs the most as the *labels* for this *classification task* and mark all of the *others* as `other`. We can use `Counter()` method from the `collections` class to get the distribution over the `condition` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2ec450",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# simple case – one label per example\n",
    "train_counts = Counter(drug_dataset_clean[\"train\"][\"condition\"])\n",
    "print(f\"Train dataset condition distribution:\\n\\t{train_counts}\")      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a30d58",
   "metadata": {},
   "source": [
    "We can see that, `birth control`, `depression`, `pain`, `anxiety` and `acne`, are the top 5 conidtions that occurs the most in our dataset. \n",
    "In Machine Learning, there is a concept called **stratification**, which states that a model will only perform well when the data distribution between the **train**, **validation** and **test** follows the same pattern, i.e., stratified. So always make sure the distribution stratified when splitting the data or choosing `labels`. Let's also make sure that is the case here, meaning for the `validation` and `test` dataset we would also like to see that  `birth control`, `depression`, `pain`, `anxiety` and `acne` are the top 5 conidtions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ceebb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_counts = Counter(drug_dataset_clean[\"validation\"][\"condition\"])\n",
    "print(f\"Validation dataset condition distribution:\\n\\t{validation_counts}\")\n",
    "\n",
    "test_counts = Counter(drug_dataset_clean[\"test\"][\"condition\"])\n",
    "print(f\"Test dataset condition distribution:\\n\\t{test_counts}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdac516",
   "metadata": {},
   "source": [
    "Now that we know the data will be well stratified when we will only take the first 5 `condition` value for the classification task. Let's create a new columns `labels` where we transfer these values only if the row's `condition` column value corresponds to one of these 5 conditions, otherwise we will put `other` there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d44ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_conditions = ['birth control', 'depression', 'pain', 'anxiety', 'acne']\n",
    "\n",
    "drug_dataset_clean = drug_dataset_clean.map(\n",
    "    lambda x: {\"labels\": [c if c in allowed_conditions else 'other' for c in x['condition']]},\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "drug_dataset_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89aae48",
   "metadata": {},
   "source": [
    "Now let's use the in-built `unique()` method to see how many values are there in the `labels` columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8387492e",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_dataset_clean[\"train\"].unique('labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e3cc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label_counts = Counter(drug_dataset_clean[\"train\"][\"labels\"])\n",
    "print(f\"train dataset labels distribution:\\n\\t{train_label_counts}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d120492",
   "metadata": {},
   "source": [
    "Now, since this task is a *Multi-label classification* task, therefore we need to convert the text values in the `labels` columns, `other`, `birth control`, `depression`, `pain`, `anxiety` and `acne` into discreet numerical values to represent them as **labels** for the model. Luckily, the `DatasetDict` object has `class_encode_column()` function to handle this task for us in-place:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713ecdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the labels to the right form\n",
    "drug_dataset_clean = drug_dataset_clean.class_encode_column(\"labels\")\n",
    "\n",
    "print(drug_dataset_clean[\"train\"].features[\"labels\"])\n",
    "\n",
    "print(drug_dataset_clean[\"train\"].unique(\"labels\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22be13da",
   "metadata": {},
   "source": [
    "Now, whenever we are dealing with customer reviews, it is a good practice to check the number of words in each review. A review might be just a single word like “Great!” or a full-blown essay with thousands of words, and depending on the use case you’ll need to handle these extremes differently. In our case, some reviews containing just a single word, which, although it may be okay for **sentiment analysis**, would not be informative when preddicting the condition.\n",
    "<br />\n",
    "So, to compute the number of words in each review, we’ll use a rough heuristic based on splitting each text by whitespace and use the `filter()` function to remove reviews that contain fewer than **30 words**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99edf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before review filtering:\")\n",
    "print(drug_dataset_clean)\n",
    "\n",
    "# returns a new column with row's review corresponding length\n",
    "def compute_review_length(data):\n",
    "    return {\"review_length\": [len(row.split()) for row in data[\"review\"]]}\n",
    "\n",
    "# map the review_length column\n",
    "drug_dataset_clean  = drug_dataset_clean.map(\n",
    "    compute_review_length,\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "# filter out rows that has review_length length less than and qual to 30\n",
    "drug_dataset_clean = drug_dataset_clean.filter(\n",
    "    lambda x: x[\"review_length\"] > 30\n",
    ")\n",
    "\n",
    "print(\"After review filtering:\")\n",
    "drug_dataset_clean "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4800da",
   "metadata": {},
   "source": [
    "Now that we have cleaned our data, let's train a model using it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff02b90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding, AutoModelForSequenceClassification\n",
    "from torch.utils.data import DataLoader\n",
    "from pprint import pprint\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokeniser = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokeniser)\n",
    "\n",
    "def tokenisation_function(data):\n",
    "    return tokeniser(data['review'], truncation=True)\n",
    "\n",
    "tokenised_datasets = drug_dataset_clean.map(\n",
    "    tokenisation_function,\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "\n",
    "pprint(tokenised_datasets)\n",
    "\n",
    "\n",
    "tokenised_datasets = tokenised_datasets.remove_columns(\n",
    "    column_names=['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length']\n",
    ")\n",
    "\n",
    "tokenised_datasets.set_format(\"torch\")\n",
    "\n",
    "pprint(tokenised_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cece146c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 32\n",
    "eval_batch_size = min(128, len(tokenised_datasets[\"validation\"]))\n",
    "test_batch_size = min(128, len(tokenised_datasets[\"test\"]))\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=tokenised_datasets[\"train\"],\n",
    "    batch_size=train_batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    dataset=tokenised_datasets[\"validation\"],\n",
    "    batch_size=eval_batch_size,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=tokenised_datasets[\"test\"],\n",
    "    batch_size=test_batch_size,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"So there are, {len(train_dataloader)} batches of size {train_batch_size} in the training dataset,\\n\\t{len(eval_dataloader)} batches of size {eval_batch_size} in the evaluation dataset, and\\n\\t {len(test_dataloader)} batches of size {test_batch_size} in the test dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fdb301",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "\n",
    "optimiser = AdamW(\n",
    "    params=model.parameters(),\n",
    "    lr=2e-5\n",
    ")\n",
    "\n",
    "accelerator = Accelerator()\n",
    "train_dl, eval_dl, test_dl, model, optimiser = accelerator.prepare(\n",
    "    train_dataloader,\n",
    "    eval_dataloader,\n",
    "    test_dataloader,\n",
    "    model,\n",
    "    optimiser\n",
    ")\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dl)\n",
    "\n",
    "num_warmup_steps = .01 * num_training_steps\n",
    "\n",
    "lr_schedular = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimiser,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
