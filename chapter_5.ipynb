{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccfc270b",
   "metadata": {},
   "source": [
    "# Handling Local Data\n",
    "To load datasets that are stored either on your laptop or on a remote server, we can still use the `load_dataset()` function. This time, we just need to specify the type of loading script in the `load_dataset()` function, along with a `data_files=''` argument that specifies the path to one or more files.\n",
    "\n",
    "![\"load_dataset()\"](data/chapter_5/load_dataset.png \"load_dataset()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99872279",
   "metadata": {},
   "source": [
    "### Loading a local dataset\n",
    "\n",
    "| Data format | Loading script | Example |\n",
    "|-------------|----------------|---------|\n",
    "| CSV & TSV |`csv`|`load_dataset(\"csv\", data_files=\"my_file.csv\")`|\n",
    "| Text files |`text`|`load_dataset(\"text\", data_files=\"my_file.txt\")`|\n",
    "| JSON & JSON Lines |`json`|`load_dataset(\"json\", data_files=\"my_file.json\")`|\n",
    "| Pickled DataFrames |`pandas`|`load_dataset(\"pandas\", data_files=\"my_dataframe.pkl\")`|\n",
    "\n",
    "For this example, let's use the [SQuAD-it](https://github.com/crux82/squad-it/) dataset, which is a large-scale **json** dataset for question answering in Italian. It's hosted on GitHub, let's first download it in our `data/chapter_5` dir using `wget` and then decompress these compressed files `SQuAD_it-train.json.gz`, `SQuAD_it-test.json.gz` using `gzip`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef67e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd data/chapter_5 && wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-train.json.gz\n",
    "!cd data/chapter_5 && wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-test.json.gz\n",
    "\n",
    "!cd data/chapter_5 && gzip -dkv SQuAD_it-*.json.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfbe4aa",
   "metadata": {},
   "source": [
    "Now that we have our data in the `JSON` format, we can simply use the `load_dataset()` function, we just need to know if we’re dealing with **ordinary JSON** (*similar to a nested dictionary*) or **JSON Lines** (*line-separated JSON*). Like many question answering datasets, **SQuAD-it** uses the *nested format*, with all the text stored in a **data field**. This means we can load the dataset by specifying the `field='data'` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102036ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "squad_it_dataset = load_dataset(\"json\", data_files=\"data/chapter_5/SQuAD_it-train.json\", field=\"data\")\n",
    "\n",
    "squad_it_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c461d993",
   "metadata": {},
   "source": [
    "As we can see, by default, loading local files creates a `DatasetDict` object with only a **train** split. But, what we really want is to include both the **train** and **test** splits in a single `DatasetDict` object so we can apply `Dataset.map()` functions across both splits at once. To do this, we can provide a dictionary to the \n",
    "```python\n",
    "data_files={\"train\":\"path to the training data\", \"test\":\"path to the testing data\"}\n",
    "```\n",
    "argument that maps each split name to a file associated with that split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc636f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {\n",
    "    \"train\":\"data/chapter_5/SQuAD_it-train.json\",\n",
    "    \"test\":\"data/chapter_5/SQuAD_it-test.json\"\n",
    "}\n",
    "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")\n",
    "squad_it_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb86667",
   "metadata": {},
   "source": [
    "The loading scripts in Datasets actually support automatic decompression of the input files, so we could have skipped the use of gzip by pointing the `data_files` argument directly to the compressed files:\n",
    "```python\n",
    "data_files = {\n",
    "    \"train\": \"data/chapter_5/SQuAD_it-train.json.gz\", \n",
    "    \"test\": \"data/chapter_5/SQuAD_it-test.json.gz\"\n",
    "}\n",
    "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")\n",
    "```\n",
    "This can be useful if you don’t want to manually decompress many `GZIP` files. The automatic decompression also applies to other common formats like `ZIP` and `TAR`, so you just need to point `data_files` to the compressed files.\n",
    "\n",
    "> The `data_files` argument is also quite flexible and can be either *a single file path*, *a list of file paths*, or *a dictionary* that maps split names to file paths. You can also *glob files* that match a *specified pattern* according to the rules used by the `Unix shell` (e.g., you can glob all the `JSON` files in a directory as a single split by setting `data_files=\"*.json\"`). See the [Datasets documentation](https://huggingface.co/docs/datasets/loading#local-and-remote-files) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a5facc",
   "metadata": {},
   "source": [
    "### Loading a remote dataset\n",
    "\n",
    "Fortunately, loading *remote files* is just as simple as loading *local* ones!\n",
    "<br />\n",
    "Instead of providing a path to *local files*, we point the `data_files` argument to **one or more URLs** where the *remote files* are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf78445",
   "metadata": {},
   "outputs": [],
   "source": [
    "url =  \"https://github.com/crux82/squad-it/raw/master/\"\n",
    "\n",
    "data_files = {\n",
    "    \"train\": url + \"SQuAD_it-train.json.gz\",\n",
    "    \"test\": url + \"SQuAD_it-test.json.gz\",\n",
    "}\n",
    "\n",
    "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")\n",
    "squad_it_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7a6b66",
   "metadata": {},
   "source": [
    "# Data Manipulation\n",
    "\n",
    "The `DatasetDict` object comes with a lot of functionalities to manipulate the original dataset.\n",
    "<br />\n",
    "For this example, we’ll use the [Drug Review Dataset](https://archive.ics.uci.edu/ml/datasets/Drug+Review+Dataset+%28Drugs.com%29) that’s hosted on the [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php), which contains patient reviews on various drugs, along with the condition being treated and a 10-star rating of the patient’s satisfaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d60332",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd data/chapter_5/ && wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip\"\n",
    "!cd data/chapter_5/ && unzip drugsCom_raw.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bef2b6b",
   "metadata": {},
   "source": [
    "As we can see, this the data is in the `TSV` format which is a variant of `CSV` that uses tabs instead of commas as the separator. So, when loading these files using `load_dataset()`, we use the specify `csv` as the *loading script* and most importantly the `delimiter=\\t` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a62b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_files = {\n",
    "    \"train\" : \"data/chapter_5/drugsComTrain_raw.tsv\",\n",
    "    \"test\" : \"data/chapter_5/drugsComTest_raw.tsv\"\n",
    "}\n",
    "\n",
    "drug_dataset = load_dataset(\"csv\", data_files=data_files, delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901948e7",
   "metadata": {},
   "source": [
    "Now that we have the `DatasetDict` object, we can create a random sample to get a quick feel for the type of data you’re working with and to do so we simply have to chain the `Dataset.shuffle()` and `Dataset.select()` function to first randomly shuffle the data  (we can also pass the `seed` argument to later use the same shuffle) and select/see the first *n* data elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd508b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_sample = drug_dataset[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "\n",
    "drug_sample[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0004751",
   "metadata": {},
   "source": [
    "From above we can see before passing this data to the model or even for tokenisation we need to perform few pre-processing steps:\n",
    "  + The `Unnamed: 0` column needs to be renamed to `patient_id`.\n",
    "  + The `condition` column includes a mix of *uppercase* and *lowercase* labels.\n",
    "  + The `reviews` are of varying length and contain a mix of Python line separators `(\\r\\n)` as well as HTML character codes like `&\\#039;`.\n",
    "\n",
    "So, we can use the in-built functions like the, `rename_column()` - to rename the column name, `map()` and `filter()` - to map all the `condition` column values to lowercase, and also filter out the special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7b5c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "\n",
    "# rename the column name\n",
    "drug_dataset = drug_dataset.rename_column(\n",
    "    original_column_name=\"Unnamed: 0\",\n",
    "    new_column_name=\"patient_id\"\n",
    ")\n",
    "\n",
    "# map conditon column values to lowercase\n",
    "def lowercase_condition(data):\n",
    "    return {\"condition\": [row.lower() for row in data[\"condition\"]]}\n",
    "    # return {\"condition\": data[\"condition\"].lower()} # if not using batched=True in the map() function\n",
    "    \n",
    "\n",
    "# let's first remove all the rows with null values, otherwise the above\n",
    "# function will throw an error\n",
    "drug_dataset = drug_dataset.filter(lambda x: x[\"condition\"] is not None)\n",
    "\n",
    "# map lowercasse\n",
    "drug_dataset = drug_dataset.map(lowercase_condition, batched=True)\n",
    "\n",
    "\n",
    "# unescape all the HTML special characters in our corpus\n",
    "drug_dataset =  drug_dataset.map(\n",
    "    lambda x: {\"review\": [html.unescape(row) for row in x[\"review\"]]},\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "\n",
    "drug_dataset[\"train\"][:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a75b07",
   "metadata": {},
   "source": [
    ">In Python, `lambda` functions are small functions that you can define without explicitly naming them. They take the general form `lambda <arguments> : <expression>`,\n",
    "where `lambda` is one of Python’s special keywords, `<arguments>` is a list/set of *comma-separated values* that define the *inputs* to the function, and `<expression>` represents the operations you wish to execute. For example, we can define a simple lambda function that squares a number as follows: `lambda x : x * x`\n",
    "To apply this function to an input, we need to wrap it and the input in parentheses:\n",
    "`(lambda x: x * x)(3) -> 9`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d1d981",
   "metadata": {},
   "source": [
    "### From Datasets to DataFrames and back\n",
    "\n",
    "We can use the the `set_format()` function of the `DatasetDict` object to convert it into a different dataframe such as *Pandas*, *NumPy*, *PyTorch*, *TensorFlow*, and *JAX*. To convert it back to the `DatasetDict` object, we simply need to call the `reset_format()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a106953",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_dataset.set_format(\"pandas\")\n",
    "\n",
    "drug_dataset[\"train\"][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce5b9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_dataset.reset_format()\n",
    "\n",
    "drug_dataset[\"train\"][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cd2206",
   "metadata": {},
   "source": [
    "### Creating a validation set\n",
    "The `DatasetDict` object also provides a `Dataset.train_test_split()` function that is based on the famous functionality from `scikit-learn` which can be used to further split the data into a train-validation-test format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26ca1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80-20 percent train-validation split on the training dataset\n",
    "drug_dataset_clean = drug_dataset[\"train\"].train_test_split(train_size=0.8, seed=41)\n",
    "\n",
    "# name the 20% split data as the validation\n",
    "drug_dataset_clean[\"validation\"] = drug_dataset_clean.pop(\"test\")\n",
    "\n",
    "# Add the orignal test dataset\n",
    "drug_dataset_clean[\"test\"] = drug_dataset[\"test\"]\n",
    "\n",
    "drug_dataset_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5393b117",
   "metadata": {},
   "source": [
    "### Saving a dataset\n",
    "To save a dataset to disk:\n",
    "\n",
    "| Data format | Function |\n",
    "|-------------|----------|\n",
    "|*Arrow*|`Dataset.save_to_disk()`|\n",
    "|*CSV*|`Dataset.to_csv()`|\n",
    "|*JSON*|`Dataset.to_json()`|\n",
    "\n",
    "For example, let’s save our cleaned dataset in the Arrow format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fa6c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_dataset_clean.save_to_disk(\"data/chapter_5/drug-reviews\")\n",
    "\n",
    "!ls data/chapter_5/drug-reviews/*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dba53d",
   "metadata": {},
   "source": [
    "Once the dataset is saved, we can load it by using the load_from_disk() function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16a19ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "drug_dataset_reloaded = load_from_disk(\"data/chapter_5/drug-reviews\")\n",
    "drug_dataset_reloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7a153a",
   "metadata": {},
   "source": [
    "For the **CSV** and **JSON** formats, we have to store each split as a separate file. One way to do this is by iterating over the keys and values in the `DatasetDict` object. This saves each split in JSON Lines format, where each row in the dataset is stored as a single line of JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feec7411",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split, dataset in drug_dataset_clean.items():\n",
    "    dataset.to_json(f\"data/chapter_5/drug-reviews-{split}.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d5062f",
   "metadata": {},
   "source": [
    "And to load the data we can simply use the `load_dataset()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474c446e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {\n",
    "    \"train\": \"data/chapter_5/drug-reviews-train.jsonl\",\n",
    "    \"validation\": \"data/chapter_5/drug-reviews-validation.jsonl\",\n",
    "    \"test\": \"data/chapter_5/drug-reviews-test.jsonl\",\n",
    "}\n",
    "drug_dataset_reloaded = load_dataset(\"json\", data_files=data_files)\n",
    "\n",
    "drug_dataset_reloaded"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
