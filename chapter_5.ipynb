{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccfc270b",
   "metadata": {},
   "source": [
    "# Handling Local Data\n",
    "To load datasets that are stored either on your laptop or on a remote server, we can still use the `load_dataset()` function. This time, we just need to specify the type of loading script in the `load_dataset()` function, along with a `data_files=''` argument that specifies the path to one or more files.\n",
    "\n",
    "![\"load_dataset()\"](data/chapter_5/load_dataset.png \"load_dataset()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99872279",
   "metadata": {},
   "source": [
    "### Loading a local dataset\n",
    "\n",
    "| Data format | Loading script | Example |\n",
    "|-------------|----------------|---------|\n",
    "| CSV & TSV |`csv`|`load_dataset(\"csv\", data_files=\"my_file.csv\")`|\n",
    "| Text files |`text`|`load_dataset(\"text\", data_files=\"my_file.txt\")`|\n",
    "| JSON & JSON Lines |`json`|`load_dataset(\"json\", data_files=\"my_file.json\")`|\n",
    "| Pickled DataFrames |`pandas`|`load_dataset(\"pandas\", data_files=\"my_dataframe.pkl\")`|\n",
    "\n",
    "For this example, let's use the [SQuAD-it](https://github.com/crux82/squad-it/) dataset, which is a large-scale **json** dataset for question answering in Italian. It's hosted on GitHub, let's first download it in our `data/chapter_5` dir using `wget` and then decompress these compressed files `SQuAD_it-train.json.gz`, `SQuAD_it-test.json.gz` using `gzip`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef67e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd data/chapter_5 && wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-train.json.gz\n",
    "!cd data/chapter_5 && wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-test.json.gz\n",
    "\n",
    "!cd data/chapter_5 && gzip -dkv SQuAD_it-*.json.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfbe4aa",
   "metadata": {},
   "source": [
    "Now that we have our data in the `JSON` format, we can simply use the `load_dataset()` function, we just need to know if we’re dealing with **ordinary JSON** (*similar to a nested dictionary*) or **JSON Lines** (*line-separated JSON*). Like many question answering datasets, **SQuAD-it** uses the *nested format*, with all the text stored in a **data field**. This means we can load the dataset by specifying the `field='data'` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102036ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "squad_it_dataset = load_dataset(\"json\", data_files=\"data/chapter_5/SQuAD_it-train.json\", field=\"data\")\n",
    "\n",
    "squad_it_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c461d993",
   "metadata": {},
   "source": [
    "As we can see, by default, loading local files creates a `DatasetDict` object with only a **train** split. But, what we really want is to include both the **train** and **test** splits in a single `DatasetDict` object so we can apply `Dataset.map()` functions across both splits at once. To do this, we can provide a dictionary to the \n",
    "```python\n",
    "data_files={\"train\":\"path to the training data\", \"test\":\"path to the testing data\"}\n",
    "```\n",
    "argument that maps each split name to a file associated with that split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc636f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {\n",
    "    \"train\":\"data/chapter_5/SQuAD_it-train.json\",\n",
    "    \"test\":\"data/chapter_5/SQuAD_it-test.json\"\n",
    "}\n",
    "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")\n",
    "squad_it_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb86667",
   "metadata": {},
   "source": [
    "The loading scripts in Datasets actually support automatic decompression of the input files, so we could have skipped the use of gzip by pointing the `data_files` argument directly to the compressed files:\n",
    "```python\n",
    "data_files = {\n",
    "    \"train\": \"data/chapter_5/SQuAD_it-train.json.gz\", \n",
    "    \"test\": \"data/chapter_5/SQuAD_it-test.json.gz\"\n",
    "}\n",
    "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")\n",
    "```\n",
    "This can be useful if you don’t want to manually decompress many `GZIP` files. The automatic decompression also applies to other common formats like `ZIP` and `TAR`, so you just need to point `data_files` to the compressed files.\n",
    "\n",
    "> The `data_files` argument is also quite flexible and can be either *a single file path*, *a list of file paths*, or *a dictionary* that maps split names to file paths. You can also *glob files* that match a *specified pattern* according to the rules used by the `Unix shell` (e.g., you can glob all the `JSON` files in a directory as a single split by setting `data_files=\"*.json\"`). See the [Datasets documentation](https://huggingface.co/docs/datasets/loading#local-and-remote-files) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a5facc",
   "metadata": {},
   "source": [
    "### Loading a remote dataset\n",
    "\n",
    "Fortunately, loading *remote files* is just as simple as loading *local* ones!\n",
    "<br />\n",
    "Instead of providing a path to *local files*, we point the `data_files` argument to **one or more URLs** where the *remote files* are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf78445",
   "metadata": {},
   "outputs": [],
   "source": [
    "url =  \"https://github.com/crux82/squad-it/raw/master/\"\n",
    "\n",
    "data_files = {\n",
    "    \"train\": url + \"SQuAD_it-train.json.gz\",\n",
    "    \"test\": url + \"SQuAD_it-test.json.gz\",\n",
    "}\n",
    "\n",
    "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")\n",
    "squad_it_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7a6b66",
   "metadata": {},
   "source": [
    "# Data Manipulation\n",
    "\n",
    "The `DatasetDict` object comes with a lot of functionalities to manipulate the original dataset.\n",
    "<br />\n",
    "For this example, we’ll use the [Drug Review Dataset](https://archive.ics.uci.edu/ml/datasets/Drug+Review+Dataset+%28Drugs.com%29) that’s hosted on the [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php), which contains patient reviews on various drugs, along with the condition being treated and a 10-star rating of the patient’s satisfaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d60332",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd data/chapter_5/ && wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip\"\n",
    "!cd data/chapter_5/ && unzip drugsCom_raw.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bef2b6b",
   "metadata": {},
   "source": [
    "As we can see, this the data is in the `TSV` format which is a variant of `CSV` that uses tabs instead of commas as the separator. So, when loading these files using `load_dataset()`, we use the specify `csv` as the *loading script* and most importantly the `delimiter=\\t` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a62b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_files = {\n",
    "    \"train\" : \"data/chapter_5/drugsComTrain_raw.tsv\",\n",
    "    \"test\" : \"data/chapter_5/drugsComTest_raw.tsv\"\n",
    "}\n",
    "\n",
    "drug_dataset = load_dataset(\"csv\", data_files=data_files, delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901948e7",
   "metadata": {},
   "source": [
    "Now that we have the `DatasetDict` object, we can create a random sample to get a quick feel for the type of data you’re working with and to do so we simply have to chain the `Dataset.shuffle()` and `Dataset.select()` function to first randomly shuffle the data  (we can also pass the `seed` argument to later use the same shuffle) and select/see the first *n* data elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd508b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_sample = drug_dataset[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "\n",
    "drug_sample[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0004751",
   "metadata": {},
   "source": [
    "From above we can see before passing this data to the model or even for tokenisation we need to perform few pre-processing steps:\n",
    "  + The `Unnamed: 0` column needs to be renamed to `patient_id`.\n",
    "  + The `condition` column includes a mix of *uppercase* and *lowercase* labels.\n",
    "  + The `reviews` are of varying length and contain a mix of Python line separators `(\\r\\n)` as well as HTML character codes like `&\\#039;`.\n",
    "\n",
    "So, we can use the in-built functions like the, `rename_column()` - to rename the column name, `map()` and `filter()` - to map all the `condition` column values to lowercase, and also filter out the special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7b5c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "\n",
    "# rename the column name\n",
    "drug_dataset = drug_dataset.rename_column(\n",
    "    original_column_name=\"Unnamed: 0\",\n",
    "    new_column_name=\"patient_id\"\n",
    ")\n",
    "\n",
    "# map conditon column values to lowercase\n",
    "def lowercase_condition(data):\n",
    "    return {\"condition\": [row.lower() for row in data[\"condition\"]]}\n",
    "    # return {\"condition\": data[\"condition\"].lower()} # if not using batched=True in the map() function\n",
    "    \n",
    "\n",
    "# let's first remove all the rows with null values, otherwise the above\n",
    "# function will throw an error\n",
    "drug_dataset = drug_dataset.filter(lambda x: x[\"condition\"] is not None)\n",
    "\n",
    "# map lowercasse\n",
    "drug_dataset = drug_dataset.map(lowercase_condition, batched=True)\n",
    "\n",
    "\n",
    "# unescape all the HTML special characters in our corpus\n",
    "drug_dataset =  drug_dataset.map(\n",
    "    lambda x: {\"review\": [html.unescape(row) for row in x[\"review\"]]},\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "\n",
    "drug_dataset[\"train\"][:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a75b07",
   "metadata": {},
   "source": [
    ">In Python, `lambda` functions are small functions that you can define without explicitly naming them. They take the general form `lambda <arguments> : <expression>`,\n",
    "where `lambda` is one of Python’s special keywords, `<arguments>` is a list/set of *comma-separated values* that define the *inputs* to the function, and `<expression>` represents the operations you wish to execute. For example, we can define a simple lambda function that squares a number as follows: `lambda x : x * x`\n",
    "To apply this function to an input, we need to wrap it and the input in parentheses:\n",
    "`(lambda x: x * x)(3) -> 9`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d1d981",
   "metadata": {},
   "source": [
    "### From Datasets to DataFrames and back\n",
    "\n",
    "We can use the the `set_format()` function of the `DatasetDict` object to convert it into a different dataframe such as *Pandas*, *NumPy*, *PyTorch*, *TensorFlow*, and *JAX*. To convert it back to the `DatasetDict` object, we simply need to call the `reset_format()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a106953",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_dataset.set_format(\"pandas\")\n",
    "\n",
    "drug_dataset[\"train\"][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce5b9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_dataset.reset_format()\n",
    "\n",
    "drug_dataset[\"train\"][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cd2206",
   "metadata": {},
   "source": [
    "### Creating a validation set\n",
    "The `DatasetDict` object also provides a `Dataset.train_test_split()` function that is based on the famous functionality from `scikit-learn` which can be used to further split the data into a train-validation-test format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26ca1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80-20 percent train-validation split on the training dataset\n",
    "drug_dataset_clean = drug_dataset[\"train\"].train_test_split(train_size=0.8, seed=41)\n",
    "\n",
    "# name the 20% split data as the validation\n",
    "drug_dataset_clean[\"validation\"] = drug_dataset_clean.pop(\"test\")\n",
    "\n",
    "# Add the orignal test dataset\n",
    "drug_dataset_clean[\"test\"] = drug_dataset[\"test\"]\n",
    "\n",
    "drug_dataset_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5393b117",
   "metadata": {},
   "source": [
    "### Saving a dataset\n",
    "To save a dataset to disk:\n",
    "\n",
    "| Data format | Function |\n",
    "|-------------|----------|\n",
    "|*Arrow*|`Dataset.save_to_disk()`|\n",
    "|*CSV*|`Dataset.to_csv()`|\n",
    "|*JSON*|`Dataset.to_json()`|\n",
    "\n",
    "For example, let’s save our cleaned dataset in the Arrow format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fa6c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_dataset_clean.save_to_disk(\"data/chapter_5/drug-reviews\")\n",
    "\n",
    "!ls data/chapter_5/drug-reviews/*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dba53d",
   "metadata": {},
   "source": [
    "Once the dataset is saved, we can load it by using the load_from_disk() function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16a19ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "drug_dataset_reloaded = load_from_disk(\"data/chapter_5/drug-reviews\")\n",
    "drug_dataset_reloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7a153a",
   "metadata": {},
   "source": [
    "For the **CSV** and **JSON** formats, we have to store each split as a separate file. One way to do this is by iterating over the keys and values in the `DatasetDict` object. This saves each split in JSON Lines format, where each row in the dataset is stored as a single line of JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feec7411",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split, dataset in drug_dataset_clean.items():\n",
    "    dataset.to_json(f\"data/chapter_5/drug-reviews-{split}.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d5062f",
   "metadata": {},
   "source": [
    "And to load the data we can simply use the `load_dataset()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474c446e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {\n",
    "    \"train\": \"data/chapter_5/drug-reviews-train.jsonl\",\n",
    "    \"validation\": \"data/chapter_5/drug-reviews-validation.jsonl\",\n",
    "    \"test\": \"data/chapter_5/drug-reviews-test.jsonl\",\n",
    "}\n",
    "drug_dataset_reloaded = load_dataset(\"json\", data_files=data_files)\n",
    "\n",
    "drug_dataset_clean = drug_dataset_reloaded\n",
    "\n",
    "drug_dataset_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e41097",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "Let's train a classifier that can predict the patient condition based on the drug review.\n",
    "### 1. Download the data.\n",
    "We are re-downloaing the data because we want to clean it more deeply this time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8ea7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# download the data\n",
    "!cd data/chapter_5/ && curl -O \"https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip\"\n",
    "!cd data/chapter_5/ && unzip -o drugsCom_raw.zip\n",
    "\n",
    "\n",
    "# load the data\n",
    "data_files = {\n",
    "    \"train\" : \"data/chapter_5/drugsComTrain_raw.tsv\",\n",
    "    \"test\" : \"data/chapter_5/drugsComTest_raw.tsv\"\n",
    "}\n",
    "\n",
    "drug_dataset = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files=data_files,\n",
    "    delimiter='\\t'\n",
    ")\n",
    "\n",
    "drug_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3887f889",
   "metadata": {},
   "source": [
    "### 2. Merge the split togther"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11de2255",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "# pop the testing data out of the drug dataset\n",
    "testing_data = drug_dataset.pop(\"test\")\n",
    "# merge the splits\n",
    "drug_dataset[\"train\"] = concatenate_datasets([drug_dataset[\"train\"], testing_data])\n",
    "\n",
    "drug_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6535b045",
   "metadata": {},
   "source": [
    "### 3. Intial data filteration phase, where we are:\n",
    "\n",
    "+ Changing the column name from `Unnamed: 0` to `patient_id`\n",
    "+ Removing the rows that does not having anything in thier `condition` column.\n",
    "+ Setting all the values inside the `condition` column to *lowecase*.\n",
    "+ Converting the html characters in the `condition` column into readable format, i.e., `unescape`.\n",
    "+ Remove the rows with `review` column length less than a certain number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07db4547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "\n",
    "# rename the column\n",
    "drug_dataset = drug_dataset.rename_column(\n",
    "    original_column_name=\"Unnamed: 0\",\n",
    "    new_column_name=\"patient_id\"\n",
    ")\n",
    "\n",
    "\n",
    "# removing out the empty condition rows\n",
    "drug_dataset = drug_dataset.filter(\n",
    "    lambda batch: [condition is not None for condition in batch[\"condition\"]],\n",
    "    batched=True,\n",
    "    desc=\"Removing empty Condition rows\"\n",
    ")\n",
    "\n",
    "\n",
    "## lowercase function\n",
    "def lowercase_condition(data):\n",
    "    return {\"condition\": [row.lower() for row in data[\"condition\"]]}\n",
    "\n",
    "## map lowercase\n",
    "drug_dataset = drug_dataset.map(\n",
    "    lowercase_condition,\n",
    "    batched=True,\n",
    "    desc=\"Mapping Condition values to lowercase\"\n",
    ")\n",
    "\n",
    "\n",
    "# unescape all the special characters\n",
    "drug_dataset = drug_dataset.map(\n",
    "    lambda x: {\"review\": [html.unescape(review_row) for review_row in x[\"review\"]]},\n",
    "    batched=True,\n",
    "    desc=\"Mapping HTML Unescape over Condition values\"\n",
    ")\n",
    "\n",
    "drug_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cafbe9d",
   "metadata": {},
   "source": [
    "Now, whenever we are dealing with *customer reviews*, it is a good practice to check the *number of words* in each *review*. A *review* might be just a *single word* like *“Great!”* or a *full-blown essay with thousands of words*, and depending on the use case you’ll need to handle these extremes differently.\n",
    "<br />\n",
    "In our case, some *reviews* containing just a single word, which, although it may be okay for **sentiment analysis**, would not be informative when predicting a *condition*. So, to compute the number of words in each review, we’ll use a rough heuristic based on splitting each text by whitespace and use the `filter()` function to remove reviews that contain fewer than **30 words**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a42250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a new column with row's review corresponding length\n",
    "def compute_review_length(data):\n",
    "    return {\"review_length\": [len(row.split()) for row in data[\"review\"]]}\n",
    "\n",
    "# map the review_length column\n",
    "drug_dataset  = drug_dataset.map(\n",
    "    compute_review_length,\n",
    "    batched=True,\n",
    "    desc=\"Mapping review_length column\"\n",
    ")\n",
    "\n",
    "# filter out rows that has review_length length less than and qual to 30\n",
    "drug_dataset = drug_dataset.filter(\n",
    "    lambda batch: [review_length >= 30 for review_length in batch[\"review_length\"]],\n",
    "    batched=True,\n",
    "    desc=\"Removing rows with review length less than 30\"\n",
    ")\n",
    "\n",
    "drug_dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd98748",
   "metadata": {},
   "source": [
    "### 4. Setting up the `labels` column.\n",
    "\n",
    "Let's first see how macny unique *conditions* there are in the dataset, using the in-built `unique()` function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754a9ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {len(drug_dataset.unique(\"condition\")[\"train\"])} unique conditions in the dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f82939",
   "metadata": {},
   "source": [
    "As we can see, there are `853` unique conditions in the `condition` column. Let's have a look at thier distribution and only select the first 5 conditions that occurs the most as the *labels* for this *multi-class classification task* and remove all of the others.\n",
    "<br />\n",
    "We can use the `Counter()` method from the `collections` class to get the distribution over the `condition` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2ec450",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "train_counts = Counter(drug_dataset[\"train\"][\"condition\"])\n",
    "\n",
    "print(f\"Conditions distribution:\\n\\t{train_counts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a30d58",
   "metadata": {},
   "source": [
    "We can see that, `birth control`, `depression`, `acne`, `anxiety`, and `pain` are the top 5 conidtions that occurs the most in our dataset. So let's now, filter out all the rows where is *condition* is not that and then, rename the `condition` column name to `labels` (because that is something that will be required by our model):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d44ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_conditions = ['birth control', 'depression', 'pain', 'anxiety', 'acne']\n",
    "\n",
    "drug_dataset = drug_dataset.filter(\n",
    "    lambda batch: [condition in allowed_conditions for condition in batch[\"condition\"]],\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "drug_dataset = drug_dataset.rename_column(\n",
    "    original_column_name=\"condition\",\n",
    "    new_column_name=\"labels\"\n",
    ")\n",
    "\n",
    "conditions_label = drug_dataset.unique(\"labels\")[\"train\"]\n",
    "print(f\"Now there are only {len(conditions_label)} conditions label, which are:\\n\\t{conditions_label}\")\n",
    "\n",
    "drug_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa567d41",
   "metadata": {},
   "source": [
    "### 5. Encoding the labels into ClassLabels\n",
    "Now, since this task is a *Multi-label classification* task, therefore we need to convert the text values in the `labels` columns, `birth control`, `depression`, `pain`, `anxiety` and `acne` into discreet numerical values i.e., `ClassLabels`, to represent them as **labels** for the model. Luckily, the `DatasetDict` object has `class_encode_column()` function to handle this task for us in-place:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cbd3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the labels to the right form\n",
    "drug_dataset = drug_dataset.class_encode_column(\"labels\")\n",
    "\n",
    "print(drug_dataset[\"train\"].features[\"labels\"])\n",
    "\n",
    "label_features = drug_dataset[\"train\"].features[\"labels\"]\n",
    "label_names = label_features.names\n",
    "\n",
    "for label in label_names:\n",
    "    print(f\"{label} -> {label_features.str2int(label)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00b6fc8",
   "metadata": {},
   "source": [
    "### 6. Splitting the Dataset\n",
    "Now, that we have `67844` *ClassLabels encoded* data in total, let's split the dataset into `train`, `validation`, `test` with a 70-20-10 percentage ratio, respectively. However, we need to follow a rulewhen splitting:\n",
    "<br />\n",
    "In Machine Learning, **stratification** refers to the practice of ensuring that the distribution of labels is consistent across the `train`, `validation`, and `test` datasets. This means that if the *training* dataset contains `60%` of label `x` and `40%` of label `y` (e.g., `6` rows of `x` and `4` rows of `y` out of `10` total), then the *validation* and *test* sets should also maintain the same proportions - `60% x` and `40% y`, respectively.\n",
    "\n",
    "\n",
    "For splitting the data we will use the in-built `train_test_split()` method, where we can also specify using the `stratify_by_column=\"labels\"` argument, to stratify the splits based on the `labels` column `ClassLabels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6905c0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "# Split off 70% train, 30% temporary (for validation + test)\n",
    "train_valtest = drug_dataset[\"train\"].train_test_split(\n",
    "    test_size=0.3,\n",
    "    seed=41,\n",
    "    stratify_by_column=\"labels\"\n",
    ")\n",
    "\n",
    "# Split 30% temporary into 20% validation and 10% test\n",
    "val_test = train_valtest[\"test\"].train_test_split(\n",
    "    test_size=1/3,  # 1/3 of 30% = 10%\n",
    "    seed=41,\n",
    "    stratify_by_column=\"labels\"\n",
    ")\n",
    "\n",
    "\n",
    "# Recombine into a final DatasetDict\n",
    "drug_dataset_final = datasets.DatasetDict(\n",
    "    {\n",
    "        \"train\" : train_valtest[\"train\"],\n",
    "        \"validation\" : val_test[\"train\"],\n",
    "        \"test\" : val_test[\"test\"]\n",
    "    }\n",
    ")\n",
    "drug_dataset_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc71309",
   "metadata": {},
   "source": [
    "As we have quite a lot of data, `65k+` in total. Let's just for the sake of making the training process faster, only take `10%` randomly shuffled sample of each split for the training and evaluating the model.\n",
    "> Note if you would like to train the model on the whole data, simply skip the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2b1527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to only take 10% of the data per split \n",
    "pct = 0.1\n",
    "\n",
    "drug_dataset_final[\"train\"] = drug_dataset_final[\"train\"].shuffle(seed=42).select(range(int(pct*len(drug_dataset_final[\"train\"]))))\n",
    "drug_dataset_final[\"validation\"] = drug_dataset_final[\"validation\"].shuffle(seed=42).select(range(int(pct*len(drug_dataset_final[\"validation\"]))))\n",
    "drug_dataset_final[\"test\"] = drug_dataset_final[\"test\"].shuffle(seed=42).select(range(int(pct*len(drug_dataset_final[\"test\"]))))\n",
    "\n",
    "drug_dataset_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73e251c",
   "metadata": {},
   "source": [
    "Let's look if the ClassLabels distribution is correct amongst the splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ceebb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_distributions(split, dataset):\n",
    "    print(f\"{split} ClassLabels distribution:\")\n",
    "\n",
    "    # get the distribution numbers\n",
    "    class_labels_counts = Counter(dataset[split][\"labels\"])\n",
    "\n",
    "    # distribution dictionary\n",
    "    dist = {}\n",
    "    # for every label \n",
    "    for label_id, count in class_labels_counts.items():\n",
    "        # get the label name using the label id\n",
    "        label_name = dataset[split].features[\"labels\"].int2str(label_id)\n",
    "        # compute the percentage\n",
    "        pct = round((count/len(drug_dataset_final[split]))*100, 2)\n",
    "\n",
    "        # add it to the distribution dict\n",
    "        dist[label_id] = (label_name, pct)\n",
    "    \n",
    "    # sort the distribution dict on label id and print the data\n",
    "    sorted_dist = dict(sorted(dist.items()))\n",
    "    for label_id, name_pct in sorted_dist.items():\n",
    "        print(f\"\\t id - {label_id} {name_pct[0]} : {name_pct[1]}% \")\n",
    "\n",
    "\n",
    "print_distributions(\"train\", drug_dataset_final)\n",
    "\n",
    "print_distributions(\"validation\", drug_dataset_final)\n",
    "\n",
    "print_distributions(\"test\", drug_dataset_final)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4800da",
   "metadata": {},
   "source": [
    "### 7. Initialise the Model and other config\n",
    "\n",
    "Now that we have our final dataset, let's now:\n",
    "+ gather the tokeniser and the model, \n",
    "+ tokenise the data and refine it all for once. \n",
    "\n",
    "> Note: When initialising the model, we also have to specify the `num_labels=5` arguments because we are training the model for a multi-class classification task and there are `5` labels in total:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff02b90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding, AutoModelForSequenceClassification\n",
    "from torch.utils.data import DataLoader\n",
    "from pprint import pprint\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokeniser = AutoTokenizer.from_pretrained(checkpoint)\n",
    "# initialise the model and also specify the number of labels\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=6)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokeniser)\n",
    "\n",
    "def tokenisation_function(data):\n",
    "    return tokeniser(data['review'], truncation=True)\n",
    "\n",
    "tokenised_datasets = drug_dataset_final.map(\n",
    "    tokenisation_function,\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "\n",
    "pprint(tokenised_datasets)\n",
    "\n",
    "\n",
    "tokenised_datasets = tokenised_datasets.remove_columns(\n",
    "    column_names=['patient_id', 'drugName', 'review', 'rating', 'date', 'usefulCount', 'review_length']\n",
    ")\n",
    "\n",
    "tokenised_datasets.set_format(\"torch\")\n",
    "\n",
    "pprint(tokenised_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d3ff5d",
   "metadata": {},
   "source": [
    "### 8. Set up the dataloader\n",
    "\n",
    "> Note: if you didn't take the 10% sample of the data at step 6 and in total still have `65k+` data. It could be worth decreasing the `train_batch_size` to below `16` if your GPU does not have a lot of memory, otherwise, the training will take time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cece146c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 16\n",
    "eval_batch_size = min(128, len(tokenised_datasets[\"validation\"]))\n",
    "test_batch_size = min(128, len(tokenised_datasets[\"test\"]))\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=tokenised_datasets[\"train\"],\n",
    "    batch_size=train_batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    dataset=tokenised_datasets[\"validation\"],\n",
    "    batch_size=eval_batch_size,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=tokenised_datasets[\"test\"],\n",
    "    batch_size=test_batch_size,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"So there are, {len(train_dataloader)} batches of size {train_batch_size} in the training dataset,\\n\\t{len(eval_dataloader)} batches of size {eval_batch_size} in the evaluation dataset, and\\n\\t {len(test_dataloader)} batches of size {test_batch_size} in the test dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c07159",
   "metadata": {},
   "source": [
    "### 9. Setup the *accelerator*, *optimisor* and *learning rate scheduler* object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fdb301",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "\n",
    "optimiser = AdamW(\n",
    "    params=model.parameters(),\n",
    "    lr=2e-5\n",
    ")\n",
    "\n",
    "accelerator = Accelerator()\n",
    "train_dl, eval_dl, test_dl, model, optimiser = accelerator.prepare(\n",
    "    train_dataloader,\n",
    "    eval_dataloader,\n",
    "    test_dataloader,\n",
    "    model,\n",
    "    optimiser\n",
    ")\n",
    "\n",
    "num_epochs = 5\n",
    "num_training_steps = num_epochs * len(train_dl)\n",
    "\n",
    "num_warmup_steps = .01 * num_training_steps\n",
    "\n",
    "lr_schedular = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimiser,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "print(f\"Total training steps {num_training_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6e5e1a",
   "metadata": {},
   "source": [
    "### Evaluation Metric Setup\n",
    "\n",
    "Now this time since there is no pre-evaluation metric present, therefore we have to define which metrics to use when evaluating our model.\n",
    "<br />\n",
    "For a *Classification* task,  the best metrics to evalute a model are **Accuracy**, **Precision**, **Recall** and **F1 Score**. The latter three metrics are detrived from a **Confusion Matrix**, which is basically a `N X N matrix`, where `N` is the *number of classes or categories* that are to be predicted. The values inside the *confusion matrix* represents one of these 4 values:\n",
    "+ **True Positives (TP)** : It is the case where we predicted Yes and the real output was also Yes.\n",
    "+ **True Negatives (TN)**: It is the case where we predicted No and the real output was also No.\n",
    "+ **False Positives (FP)**: It is the case where we predicted Yes but it was actually No.\n",
    "+ **False Negatives (FN)**: It is the case where we predicted No but it was actually Yes. \n",
    "\n",
    "For example, suppose there is a problem which is a binary classification with labels as `Yes` or `No`. So, here `N = 2`, therfore we will get a `2 X 2` *confusion matrix*. Now let's say we tested our model with 165 samples and the results using *confusion matrix* looks like this:\n",
    "\n",
    "|              |Predicted No|Predited Yes|\n",
    "|--------------|------------|------------|\n",
    "|**Actual No** |50|10|\n",
    "|**Actual Yes**|5|100|\n",
    "\n",
    "Therefore, out of the 165 predictions, `100` predictions were **TP** (bottom right), `50` were **TN** (top left), `10` were **FP** (top right), and `5` were *FN* (bottom left).\n",
    "\n",
    "\n",
    "Now, how these values are useful because we can use them to calculate **Precision**, **Recall** and **F1 Score**:\n",
    "+ **Precision**: It measures how many of the positive predictions made by the model are actually correct. It's useful when the cost of false positives is high such as in medical diagnoses where predicting a disease when it’s not present can have serious consequences. Therefore, *Precision* helps ensure that when the model predicts a positive outcome, it’s likely to be correct.\n",
    "$$\n",
    "\\text{Precision} = \\frac{TP}{TP+FP}\n",
    "$$\n",
    "+ **Recall**: *Recall* or *Sensitivity measures* how many of the actual positive cases were correctly identified by the model. It is important when missing a positive case (*false negative*) is more costly than false positives (like disease detection).\n",
    "$$\n",
    "\\text{Recall} = \\frac{TP}{TP+FN}\n",
    "$$\n",
    "+ **F1 Score**: The *F1 Score* is the *harmonic mean* of *precision* and *recall*. It is useful when we need a balance between *precision* and *recall*, as it combines both into a single number. A *high F1 score* means the model performs well on both metrics, i.e., the model is performing well. Its range is `[0,1]`:\n",
    "$$\n",
    "\\text{F1 Score}=2\\times\\frac{Precision+Recall}{Precision×Recall} \n",
    "$$\n",
    "Now, when you have multiple classes, you still often want a single precision/recall/F1 number—but how you combine per-class scores depends on whether you care more about rare classes, common classes, or every example equally. Here’s what each averaging strategy does:\n",
    "\n",
    "+ **Weighted**: Compute each class’s score, then average them but weight by how many true examples each class has - so common labels count more.\n",
    "\n",
    "+ **Micro**: Pool all true/false positives and negatives across every example, then compute one overall score - every prediction is equal (large classes dominate).\n",
    "\n",
    "+ **Macro**: Compute each class’s score and then take the simple average—every class counts the same, no matter how many examples it has.\n",
    "\n",
    "> NOTE: **Lower recall** and **higher precision** gives us **great accuracy** but then it misses a large number of instances and that's why **accuracy** alone is not a good metric when evaluating a model and using **Recall**, **Precision** and **F1 score** if possible is a good practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2fde33",
   "metadata": {},
   "source": [
    "Luckily, the `evaluate` lib provides `combine()` method, where you can specify which metrics to use for the evaluation, and also when calling the `compute()` we can pass the `average` argument to specify which averaging strategy to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7f3142",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from evaluate import combine\n",
    "import torch\n",
    "from livelossplot import PlotLosses\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def perform_evaluation():\n",
    "    \"\"\"\n",
    "    Perform evaluation on the validation set\n",
    "    \"\"\"\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    eval_epoch_loss = []\n",
    "\n",
    "    # initialising evaluation\n",
    "    eval_acc_metric = evaluate.load(\"accuracy\")\n",
    "    eval_f1_metric = evaluate.load(\"f1\")\n",
    "    eval_specific_metric = combine(\n",
    "        evaluations=[\n",
    "            \"precision\",\n",
    "            \"recall\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    for batch in eval_dl:\n",
    "        # Disable gradient computation for evaluation (saves memory and computation)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "            # Store loss inside no_grad for memory efficiency\n",
    "            eval_epoch_loss.append(outputs.loss.item())\n",
    "\n",
    "            # Get predictions for metrics (logits already created without gradients)\n",
    "            logits = outputs.logits\n",
    "            refs = batch[\"labels\"]\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            # Add batch to evaluation metric\n",
    "            eval_acc_metric.add_batch(\n",
    "                predictions=accelerator.gather(preds),\n",
    "                references=accelerator.gather(refs)\n",
    "            )\n",
    "            eval_f1_metric.add_batch(\n",
    "                predictions=accelerator.gather(preds),\n",
    "                references=accelerator.gather(refs)\n",
    "            )\n",
    "            eval_specific_metric.add_batch(\n",
    "                predictions=accelerator.gather(preds),\n",
    "                references=accelerator.gather(refs)\n",
    "            )\n",
    "    \n",
    "    eval_avg_loss = sum(eval_epoch_loss) / len(eval_epoch_loss)\n",
    "    eval_pred_stats = {}\n",
    "    eval_pred_stats.update(eval_acc_metric.compute())\n",
    "    eval_pred_stats.update(\n",
    "        eval_f1_metric.compute(\n",
    "            average=\"weighted\",\n",
    "            labels= list(range(len(label_names)))\n",
    "        )\n",
    "    )\n",
    "    eval_pred_stats.update(\n",
    "        eval_specific_metric.compute(\n",
    "            average=\"weighted\",\n",
    "            zero_division=0,\n",
    "            labels= list(range(len(label_names)))\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return eval_avg_loss, eval_pred_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a513ea4",
   "metadata": {},
   "source": [
    "For after we will have the model trained we would like to evaluate its performance on the test data because testing on untouched data gives a true measure of how our model will perform on new examples and prevents us from overfitting by tuning to the same data we used to train it. \n",
    "<br />\n",
    "So, let's write the evaluation function on the test data, and this time we can also ask for the *confusion_matrix* from the `evalute.compute()` function along with other metrics to further evalute the model on the test data:\n",
    "\n",
    "> Note: when evaluating the model on the test data we don't need to look at the loss value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bc7d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_evaluation():\n",
    "    model.eval()\n",
    "\n",
    "    test_acc_metric = evaluate.load(\"accuracy\")\n",
    "    test_f1_metric = evaluate.load(\"f1\")\n",
    "    test_specific_metric = combine(\n",
    "        evaluations=[\n",
    "            \"precision\",\n",
    "            \"recall\"\n",
    "        ]\n",
    "    )\n",
    "    test_cm_metric = evaluate.load(\"confusion_matrix\")\n",
    "\n",
    "    for batch in test_dl:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "            logits = outputs.logits\n",
    "            refs = batch[\"labels\"]\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            test_acc_metric.add_batch(\n",
    "                predictions=accelerator.gather(preds),\n",
    "                references=accelerator.gather(refs)\n",
    "            )\n",
    "            test_f1_metric.add_batch(\n",
    "                predictions=accelerator.gather(preds),\n",
    "                references=accelerator.gather(refs)\n",
    "            )\n",
    "            test_specific_metric.add_batch(\n",
    "                predictions=accelerator.gather(preds),\n",
    "                references=accelerator.gather(refs)\n",
    "            )\n",
    "            test_cm_metric.add_batch(\n",
    "                predictions=accelerator.gather(preds),\n",
    "                references=accelerator.gather(refs)\n",
    "            )\n",
    "    \n",
    "    test_pred_stats = {}\n",
    "    test_pred_stats.update(test_acc_metric.compute())\n",
    "    test_pred_stats.update(\n",
    "        test_f1_metric.compute(\n",
    "            average=\"weighted\",\n",
    "            labels= list(range(len(label_names)))\n",
    "        )\n",
    "    )\n",
    "    test_pred_stats.update(\n",
    "        test_specific_metric.compute(\n",
    "            average=\"weighted\",\n",
    "            labels= list(range(len(label_names))),\n",
    "            zero_division=0\n",
    "        )\n",
    "    )\n",
    "    test_pred_stats.update(\n",
    "        test_cm_metric.compute(\n",
    "            labels= list(range(len(label_names)))\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return test_pred_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1595a5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_cm(cm):\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
    "    plt.xticks(range(len(label_names)), label_names, rotation=45)\n",
    "    plt.yticks(range(len(label_names)), label_names)\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e438a56b",
   "metadata": {},
   "source": [
    "the main training function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b905c85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "def training_function():\n",
    "    # initialise the plotter for the learning curve\n",
    "    plotter = PlotLosses(mode='notebook')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # ensure model is in training mode\n",
    "        model.train()\n",
    "        train_epoch_loss = []\n",
    "\n",
    "        # metrics for training data\n",
    "        train_acc_metric = evaluate.load(\"accuracy\")\n",
    "        train_f1_metric = evaluate.load(\"f1\")\n",
    "        train_specific_metric = combine(\n",
    "            evaluations=[\n",
    "                \"precision\",\n",
    "                \"recall\"\n",
    "            ]   \n",
    "        )\n",
    "\n",
    "        for batch in train_dl:\n",
    "            # Forward Pass (keep gradient attached)\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            # Backward Pass (while gradients are still attached)\n",
    "            accelerator.backward(loss)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimiser.step()\n",
    "            lr_schedular.step()\n",
    "            optimiser.zero_grad()\n",
    "\n",
    "            # metric computation\n",
    "            with torch.no_grad():\n",
    "                # detach loss for metric computation \n",
    "                train_epoch_loss.append(loss.detach().item())\n",
    "\n",
    "                # detach logits for metric computation\n",
    "                logits = outputs.logits.detach()\n",
    "                # no need to detach labels (they don't have gradients)\n",
    "                refs = batch['labels']\n",
    "                preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "                # add batch to the train matric\n",
    "                train_acc_metric.add_batch(\n",
    "                    predictions=accelerator.gather(preds),\n",
    "                    references=accelerator.gather(refs)\n",
    "                )\n",
    "                train_f1_metric.add_batch(\n",
    "                    predictions=accelerator.gather(preds),\n",
    "                    references=accelerator.gather(refs)\n",
    "                )\n",
    "                train_specific_metric.add_batch(\n",
    "                    predictions=accelerator.gather(preds),\n",
    "                    references=accelerator.gather(refs)\n",
    "                )\n",
    "            \n",
    "            progress_bar.update(1)\n",
    "\n",
    "        # compute training metrics\n",
    "        tain_avg_loss = sum(train_epoch_loss)/len(train_epoch_loss)\n",
    "        train_pred_stats = {}\n",
    "        train_pred_stats.update(train_acc_metric.compute())\n",
    "        train_pred_stats.update(\n",
    "            train_f1_metric.compute(\n",
    "                average=\"weighted\",\n",
    "                labels= list(range(len(label_names)))\n",
    "            )\n",
    "        )\n",
    "        train_pred_stats.update(\n",
    "            train_specific_metric.compute(\n",
    "                average=\"weighted\",\n",
    "                labels= list(range(len(label_names))),\n",
    "                zero_division=0\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # evaluation phase\n",
    "        eval_avg_loss, eval_pred_stats = perform_evaluation()\n",
    "\n",
    "        plotter.update({\n",
    "            'loss': tain_avg_loss,\n",
    "            'val_loss': eval_avg_loss,\n",
    "            'acc': train_pred_stats['accuracy'],\n",
    "            'val_acc': eval_pred_stats['accuracy'],\n",
    "            'precision': train_pred_stats['precision'],\n",
    "            'val_precision': eval_pred_stats['precision'],\n",
    "            'recall': train_pred_stats['recall'],\n",
    "            'val_recall': eval_pred_stats['recall'],\n",
    "            'f1': train_pred_stats['f1'],\n",
    "            'val_f1': eval_pred_stats['f1'],\n",
    "        })\n",
    "        plotter.send() \n",
    "\n",
    "    # perform evaluation on the test dataset\n",
    "    test_pred_stats = test_evaluation()\n",
    "    cm = test_pred_stats.pop(\"confusion_matrix\")\n",
    "    pprint(f\"Test Dataset Evaluation Results:\\n{test_pred_stats}\")\n",
    "    # plot the confusion matrix on the test data\n",
    "    plot_cm(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc212c85",
   "metadata": {},
   "source": [
    "Let's launch the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21805635",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import notebook_launcher\n",
    "\n",
    "notebook_launcher(training_function, num_processes=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
