{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccfc270b",
   "metadata": {},
   "source": [
    "# Handling Local Data\n",
    "To load datasets that are stored either on your laptop or on a remote server, we can still use the `load_dataset()` function. This time, we just need to specify the type of loading script in the `load_dataset()` function, along with a `data_files=''` argument that specifies the path to one or more files.\n",
    "\n",
    "![\"load_dataset()\"](data/chapter_5/load_dataset.png \"load_dataset()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99872279",
   "metadata": {},
   "source": [
    "### Loading a local dataset\n",
    "\n",
    "| Data format | Loading script | Example |\n",
    "|-------------|----------------|---------|\n",
    "| CSV & TSV |`csv`|`load_dataset(\"csv\", data_files=\"my_file.csv\")`|\n",
    "| Text files |`text`|`load_dataset(\"text\", data_files=\"my_file.txt\")`|\n",
    "| JSON & JSON Lines |`json`|`load_dataset(\"json\", data_files=\"my_file.json\")`|\n",
    "| Pickled DataFrames |`pandas`|`load_dataset(\"pandas\", data_files=\"my_dataframe.pkl\")`|\n",
    "\n",
    "For this example, let's use the [SQuAD-it](https://github.com/crux82/squad-it/) dataset, which is a large-scale **json** dataset for question answering in Italian. It's hosted on GitHub, let's first download it in our `data/chapter_5` dir using `wget` and then decompress these compressed files `SQuAD_it-train.json.gz`, `SQuAD_it-test.json.gz` using `gzip`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef67e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd data/chapter_5 && wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-train.json.gz\n",
    "!cd data/chapter_5 && wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-test.json.gz\n",
    "\n",
    "!cd data/chapter_5 && gzip -dkv SQuAD_it-*.json.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfbe4aa",
   "metadata": {},
   "source": [
    "Now that we have our data in the `JSON` format, we can simply use the `load_dataset()` function, we just need to know if we’re dealing with **ordinary JSON** (*similar to a nested dictionary*) or **JSON Lines** (*line-separated JSON*). Like many question answering datasets, **SQuAD-it** uses the *nested format*, with all the text stored in a **data field**. This means we can load the dataset by specifying the `field='data'` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102036ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "squad_it_dataset = load_dataset(\"json\", data_files=\"data/chapter_5/SQuAD_it-train.json\", field=\"data\")\n",
    "\n",
    "squad_it_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c461d993",
   "metadata": {},
   "source": [
    "As we can see, by default, loading local files creates a `DatasetDict` object with only a **train** split. But, what we really want is to include both the **train** and **test** splits in a single `DatasetDict` object so we can apply `Dataset.map()` functions across both splits at once. To do this, we can provide a dictionary to the \n",
    "```python\n",
    "data_files={\"train\":\"path to the training data\", \"test\":\"path to the testing data\"}\n",
    "```\n",
    "argument that maps each split name to a file associated with that split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc636f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {\n",
    "    \"train\":\"data/chapter_5/SQuAD_it-train.json\",\n",
    "    \"test\":\"data/chapter_5/SQuAD_it-test.json\"\n",
    "}\n",
    "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")\n",
    "squad_it_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb86667",
   "metadata": {},
   "source": [
    "The loading scripts in Datasets actually support automatic decompression of the input files, so we could have skipped the use of gzip by pointing the `data_files` argument directly to the compressed files:\n",
    "```python\n",
    "data_files = {\n",
    "    \"train\": \"data/chapter_5/SQuAD_it-train.json.gz\", \n",
    "    \"test\": \"data/chapter_5/SQuAD_it-test.json.gz\"\n",
    "}\n",
    "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")\n",
    "```\n",
    "This can be useful if you don’t want to manually decompress many `GZIP` files. The automatic decompression also applies to other common formats like `ZIP` and `TAR`, so you just need to point `data_files` to the compressed files.\n",
    "\n",
    "> The `data_files` argument is also quite flexible and can be either *a single file path*, *a list of file paths*, or *a dictionary* that maps split names to file paths. You can also *glob files* that match a *specified pattern* according to the rules used by the `Unix shell` (e.g., you can glob all the `JSON` files in a directory as a single split by setting `data_files=\"*.json\"`). See the [Datasets documentation](https://huggingface.co/docs/datasets/loading#local-and-remote-files) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a5facc",
   "metadata": {},
   "source": [
    "### Loading a remote dataset\n",
    "\n",
    "Fortunately, loading *remote files* is just as simple as loading *local* ones!\n",
    "<br />\n",
    "Instead of providing a path to *local files*, we point the `data_files` argument to **one or more URLs** where the *remote files* are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf78445",
   "metadata": {},
   "outputs": [],
   "source": [
    "url =  \"https://github.com/crux82/squad-it/raw/master/\"\n",
    "\n",
    "data_files = {\n",
    "    \"train\": url + \"SQuAD_it-train.json.gz\",\n",
    "    \"test\": url + \"SQuAD_it-test.json.gz\",\n",
    "}\n",
    "\n",
    "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")\n",
    "squad_it_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7a6b66",
   "metadata": {},
   "source": [
    "# Data Manipulation\n",
    "\n",
    "The `DatasetDict` object comes with a lot of functionalities to manipulate the original dataset.\n",
    "<br />\n",
    "For this example, we’ll use the [Drug Review Dataset](https://archive.ics.uci.edu/ml/datasets/Drug+Review+Dataset+%28Drugs.com%29) that’s hosted on the [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php), which contains patient reviews on various drugs, along with the condition being treated and a 10-star rating of the patient’s satisfaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d60332",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd data/chapter_5/ && wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip\"\n",
    "!cd data/chapter_5/ && unzip drugsCom_raw.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bef2b6b",
   "metadata": {},
   "source": [
    "As we can see, this the data is in the `TSV` format which is a variant of `CSV` that uses tabs instead of commas as the separator. So, when loading these files using `load_dataset()`, we use the specify `csv` as the *loading script* and most importantly the `delimiter=\\t` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a62b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_files = {\n",
    "    \"train\" : \"data/chapter_5/drugsComTrain_raw.tsv\",\n",
    "    \"test\" : \"data/chapter_5/drugsComTest_raw.tsv\"\n",
    "}\n",
    "\n",
    "drug_dataset = load_dataset(\"csv\", data_files=data_files, delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901948e7",
   "metadata": {},
   "source": [
    "Now that we have the `DatasetDict` object, we can create a random sample to get a quick feel for the type of data you’re working with and to do so we simply have to chain the `Dataset.shuffle()` and `Dataset.select()` function to first randomly shuffle the data  (we can also pass the `seed` argument to later use the same shuffle) and select/see the first *n* data elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd508b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_sample = drug_dataset[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "\n",
    "drug_sample[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0004751",
   "metadata": {},
   "source": [
    "From above we can see before passing this data to the model or even for tokenisation we need to perform few pre-processing steps:\n",
    "  + The `Unnamed: 0` column needs to be renamed to `patient_id`.\n",
    "  + The `condition` column includes a mix of *uppercase* and *lowercase* labels.\n",
    "  + The `reviews` are of varying length and contain a mix of Python line separators `(\\r\\n)` as well as HTML character codes like `&\\#039;`.\n",
    "\n",
    "So, we can use the in-built functions like the, `rename_column()` - to rename the column name, `map()` and `filter()` - to map all the `condition` column values to lowercase, and also filter out the special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7b5c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "\n",
    "# rename the column name\n",
    "drug_dataset = drug_dataset.rename_column(\n",
    "    original_column_name=\"Unnamed: 0\",\n",
    "    new_column_name=\"patient_id\"\n",
    ")\n",
    "\n",
    "# map conditon column values to lowercase\n",
    "def lowercase_condition(data):\n",
    "    return {\"condition\": [row.lower() for row in data[\"condition\"]]}\n",
    "    # return {\"condition\": data[\"condition\"].lower()} # if not using batched=True in the map() function\n",
    "    \n",
    "\n",
    "# let's first remove all the rows with null values, otherwise the above\n",
    "# function will throw an error\n",
    "drug_dataset = drug_dataset.filter(lambda x: x[\"condition\"] is not None)\n",
    "\n",
    "# map lowercasse\n",
    "drug_dataset = drug_dataset.map(lowercase_condition, batched=True)\n",
    "\n",
    "\n",
    "# unescape all the HTML special characters in our corpus\n",
    "drug_dataset =  drug_dataset.map(\n",
    "    lambda x: {\"review\": [html.unescape(row) for row in x[\"review\"]]},\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "\n",
    "drug_dataset[\"train\"][:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a75b07",
   "metadata": {},
   "source": [
    ">In Python, `lambda` functions are small functions that you can define without explicitly naming them. They take the general form `lambda <arguments> : <expression>`,\n",
    "where `lambda` is one of Python’s special keywords, `<arguments>` is a list/set of *comma-separated values* that define the *inputs* to the function, and `<expression>` represents the operations you wish to execute. For example, we can define a simple lambda function that squares a number as follows: `lambda x : x * x`\n",
    "To apply this function to an input, we need to wrap it and the input in parentheses:\n",
    "`(lambda x: x * x)(3) -> 9`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d1d981",
   "metadata": {},
   "source": [
    "### From Datasets to DataFrames and back\n",
    "\n",
    "We can use the the `set_format()` function of the `DatasetDict` object to convert it into a different dataframe such as *Pandas*, *NumPy*, *PyTorch*, *TensorFlow*, and *JAX*. To convert it back to the `DatasetDict` object, we simply need to call the `reset_format()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a106953",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_dataset.set_format(\"pandas\")\n",
    "\n",
    "drug_dataset[\"train\"][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce5b9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_dataset.reset_format()\n",
    "\n",
    "drug_dataset[\"train\"][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cd2206",
   "metadata": {},
   "source": [
    "### Creating a validation set\n",
    "The `DatasetDict` object also provides a `Dataset.train_test_split()` function that is based on the famous functionality from `scikit-learn` which can be used to further split the data into a train-validation-test format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26ca1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80-20 percent train-validation split on the training dataset\n",
    "drug_dataset_clean = drug_dataset[\"train\"].train_test_split(train_size=0.8, seed=41)\n",
    "\n",
    "# name the 20% split data as the validation\n",
    "drug_dataset_clean[\"validation\"] = drug_dataset_clean.pop(\"test\")\n",
    "\n",
    "# Add the orignal test dataset\n",
    "drug_dataset_clean[\"test\"] = drug_dataset[\"test\"]\n",
    "\n",
    "drug_dataset_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5393b117",
   "metadata": {},
   "source": [
    "### Saving a dataset\n",
    "To save a dataset to disk:\n",
    "\n",
    "| Data format | Function |\n",
    "|-------------|----------|\n",
    "|*Arrow*|`Dataset.save_to_disk()`|\n",
    "|*CSV*|`Dataset.to_csv()`|\n",
    "|*JSON*|`Dataset.to_json()`|\n",
    "\n",
    "For example, let’s save our cleaned dataset in the Arrow format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fa6c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_dataset_clean.save_to_disk(\"data/chapter_5/drug-reviews\")\n",
    "\n",
    "!ls data/chapter_5/drug-reviews/*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dba53d",
   "metadata": {},
   "source": [
    "Once the dataset is saved, we can load it by using the load_from_disk() function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16a19ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "drug_dataset_reloaded = load_from_disk(\"data/chapter_5/drug-reviews\")\n",
    "drug_dataset_reloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7a153a",
   "metadata": {},
   "source": [
    "For the **CSV** and **JSON** formats, we have to store each split as a separate file. One way to do this is by iterating over the keys and values in the `DatasetDict` object. This saves each split in JSON Lines format, where each row in the dataset is stored as a single line of JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feec7411",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split, dataset in drug_dataset_clean.items():\n",
    "    dataset.to_json(f\"data/chapter_5/drug-reviews-{split}.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d5062f",
   "metadata": {},
   "source": [
    "And to load the data we can simply use the `load_dataset()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474c446e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {\n",
    "    \"train\": \"data/chapter_5/drug-reviews-train.jsonl\",\n",
    "    \"validation\": \"data/chapter_5/drug-reviews-validation.jsonl\",\n",
    "    \"test\": \"data/chapter_5/drug-reviews-test.jsonl\",\n",
    "}\n",
    "drug_dataset_reloaded = load_dataset(\"json\", data_files=data_files)\n",
    "\n",
    "drug_dataset_clean = drug_dataset_reloaded\n",
    "\n",
    "drug_dataset_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e41097",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "Let's train a classifier that can predict the patient condition based on the drug review.\n",
    "### 1. Download the data.\n",
    "We are re-downloaing the data because we want to clean it more deeply this time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8ea7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# download the data\n",
    "!cd data/chapter_5/ && curl -O \"https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip\"\n",
    "!cd data/chapter_5/ && unzip -o drugsCom_raw.zip\n",
    "\n",
    "\n",
    "# load the data\n",
    "data_files = {\n",
    "    \"train\" : \"data/chapter_5/drugsComTrain_raw.tsv\",\n",
    "    \"test\" : \"data/chapter_5/drugsComTest_raw.tsv\"\n",
    "}\n",
    "\n",
    "drug_dataset = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files=data_files,\n",
    "    delimiter='\\t'\n",
    ")\n",
    "\n",
    "drug_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3887f889",
   "metadata": {},
   "source": [
    "### 2. Merge the split togther"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11de2255",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "# pop the testing data out of the drug dataset\n",
    "testing_data = drug_dataset.pop(\"test\")\n",
    "# merge the splits\n",
    "drug_dataset[\"train\"] = concatenate_datasets([drug_dataset[\"train\"], testing_data])\n",
    "\n",
    "drug_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6535b045",
   "metadata": {},
   "source": [
    "### 3. Intial data filteration phase, where we are:\n",
    "\n",
    "+ Changing the column name from `Unnamed: 0` to `patient_id`\n",
    "+ Removing the rows that does not having anything in thier `condition` column.\n",
    "+ Setting all the values inside the `condition` column to *lowecase*.\n",
    "+ Converting the html characters in the `condition` column into readable format, i.e., `unescape`.\n",
    "+ Remove the rows with `review` column length less than a certain number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07db4547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "\n",
    "# rename the column\n",
    "drug_dataset = drug_dataset.rename_column(\n",
    "    original_column_name=\"Unnamed: 0\",\n",
    "    new_column_name=\"patient_id\"\n",
    ")\n",
    "\n",
    "\n",
    "# removing out the empty condition rows\n",
    "drug_dataset = drug_dataset.filter(\n",
    "    lambda batch: [condition is not None for condition in batch[\"condition\"]],\n",
    "    batched=True,\n",
    "    desc=\"Removing empty Condition rows\"\n",
    ")\n",
    "\n",
    "\n",
    "## lowercase function\n",
    "def lowercase_condition(data):\n",
    "    return {\"condition\": [row.lower() for row in data[\"condition\"]]}\n",
    "\n",
    "## map lowercase\n",
    "drug_dataset = drug_dataset.map(\n",
    "    lowercase_condition,\n",
    "    batched=True,\n",
    "    desc=\"Mapping Condition values to lowercase\"\n",
    ")\n",
    "\n",
    "\n",
    "# unescape all the special characters\n",
    "drug_dataset = drug_dataset.map(\n",
    "    lambda x: {\"review\": [html.unescape(review_row) for review_row in x[\"review\"]]},\n",
    "    batched=True,\n",
    "    desc=\"Mapping HTML Unescape over Condition values\"\n",
    ")\n",
    "\n",
    "drug_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cafbe9d",
   "metadata": {},
   "source": [
    "Now, whenever we are dealing with *customer reviews*, it is a good practice to check the *number of words* in each *review*. A *review* might be just a *single word* like *“Great!”* or a *full-blown essay with thousands of words*, and depending on the use case you’ll need to handle these extremes differently.\n",
    "<br />\n",
    "In our case, some *reviews* containing just a single word, which, although it may be okay for **sentiment analysis**, would not be informative when predicting a *condition*. So, to compute the number of words in each review, we’ll use a rough heuristic based on splitting each text by whitespace and use the `filter()` function to remove reviews that contain fewer than **30 words**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a42250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a new column with row's review corresponding length\n",
    "def compute_review_length(data):\n",
    "    return {\"review_length\": [len(row.split()) for row in data[\"review\"]]}\n",
    "\n",
    "# map the review_length column\n",
    "drug_dataset  = drug_dataset.map(\n",
    "    compute_review_length,\n",
    "    batched=True,\n",
    "    desc=\"Mapping review_length column\"\n",
    ")\n",
    "\n",
    "# filter out rows that has review_length length less than and qual to 30\n",
    "drug_dataset = drug_dataset.filter(\n",
    "    lambda batch: [review_length >= 30 for review_length in batch[\"review_length\"]],\n",
    "    batched=True,\n",
    "    desc=\"Removing rows with review length less than 30\"\n",
    ")\n",
    "\n",
    "drug_dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd98748",
   "metadata": {},
   "source": [
    "### 4. Setting up the `labels` column.\n",
    "\n",
    "Let's first see how macny unique *conditions* there are in the dataset, using the in-built `unique()` function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754a9ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {len(drug_dataset.unique(\"condition\")[\"train\"])} unique conditions in the dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f82939",
   "metadata": {},
   "source": [
    "As we can see, there are `853` unique conditions in the `condition` column. Let's have a look at thier distribution and only select the first 5 conditions that occurs the most as the *labels* for this *multi-class classification task* and remove all of the others.\n",
    "<br />\n",
    "We can use the `Counter()` method from the `collections` class to get the distribution over the `condition` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2ec450",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "train_counts = Counter(drug_dataset[\"train\"][\"condition\"])\n",
    "\n",
    "print(f\"Conditions distribution:\\n\\t{train_counts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a30d58",
   "metadata": {},
   "source": [
    "We can see that, `birth control`, `depression`, `acne`, `anxiety`, and `pain` are the top 5 conidtions that occurs the most in our dataset. So let's now, filter out all the rows where is *condition* is not that and then, rename the `condition` column name to `labels` (because that is something that will be required by our model):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d44ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_conditions = ['birth control', 'depression', 'pain', 'anxiety', 'acne']\n",
    "\n",
    "drug_dataset = drug_dataset.filter(\n",
    "    lambda batch: [condition in allowed_conditions for condition in batch[\"condition\"]],\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "drug_dataset = drug_dataset.rename_column(\n",
    "    original_column_name=\"condition\",\n",
    "    new_column_name=\"labels\"\n",
    ")\n",
    "\n",
    "conditions_label = drug_dataset.unique(\"labels\")[\"train\"]\n",
    "print(f\"Now there are only {len(conditions_label)} conditions label, which are:\\n\\t{conditions_label}\")\n",
    "\n",
    "drug_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa567d41",
   "metadata": {},
   "source": [
    "### 5. Encoding the labels into ClassLabels\n",
    "Now, since this task is a *Multi-label classification* task, therefore we need to convert the text values in the `labels` columns, `birth control`, `depression`, `pain`, `anxiety` and `acne` into discreet numerical values i.e., `ClassLabels`, to represent them as **labels** for the model. Luckily, the `DatasetDict` object has `class_encode_column()` function to handle this task for us in-place:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cbd3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the labels to the right form\n",
    "drug_dataset = drug_dataset.class_encode_column(\"labels\")\n",
    "\n",
    "print(drug_dataset[\"train\"].features[\"labels\"])\n",
    "\n",
    "label_features = drug_dataset[\"train\"].features[\"labels\"]\n",
    "label_names = label_features.names\n",
    "\n",
    "for label in label_names:\n",
    "    print(f\"{label} -> {label_features.str2int(label)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00b6fc8",
   "metadata": {},
   "source": [
    "### 6. Splitting the Dataset\n",
    "Now, that we have `67844` *ClassLabels encoded* data in total, let's split the dataset into `train`, `validation`, `test` with a 70-20-10 percentage ratio, respectively. However, we need to follow a rulewhen splitting:\n",
    "<br />\n",
    "In Machine Learning, **stratification** refers to the practice of ensuring that the distribution of labels is consistent across the `train`, `validation`, and `test` datasets. This means that if the *training* dataset contains `60%` of label `x` and `40%` of label `y` (e.g., `6` rows of `x` and `4` rows of `y` out of `10` total), then the *validation* and *test* sets should also maintain the same proportions - `60% x` and `40% y`, respectively.\n",
    "\n",
    "\n",
    "For splitting the data we will use the in-built `train_test_split()` method, where we can also specify using the `stratify_by_column=\"labels\"` argument, to stratify the splits based on the `labels` column `ClassLabels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6905c0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "# Split off 70% train, 30% temporary (for validation + test)\n",
    "train_valtest = drug_dataset[\"train\"].train_test_split(\n",
    "    test_size=0.3,\n",
    "    seed=41,\n",
    "    stratify_by_column=\"labels\"\n",
    ")\n",
    "\n",
    "# Split 30% temporary into 20% validation and 10% test\n",
    "val_test = train_valtest[\"test\"].train_test_split(\n",
    "    test_size=1/3,  # 1/3 of 30% = 10%\n",
    "    seed=41,\n",
    "    stratify_by_column=\"labels\"\n",
    ")\n",
    "\n",
    "\n",
    "# Recombine into a final DatasetDict\n",
    "drug_dataset_final = datasets.DatasetDict(\n",
    "    {\n",
    "        \"train\" : train_valtest[\"train\"],\n",
    "        \"validation\" : val_test[\"train\"],\n",
    "        \"test\" : val_test[\"test\"]\n",
    "    }\n",
    ")\n",
    "drug_dataset_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc71309",
   "metadata": {},
   "source": [
    "As we have quite a lot of data, `65k+` in total. Let's just for the sake of making the training process faster, only take `10%` randomly shuffled sample of each split for the training and evaluating the model.\n",
    "> Note if you would like to train the model on the whole data, simply skip the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2b1527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to only take 10% of the data per split \n",
    "pct = 0.1\n",
    "\n",
    "drug_dataset_final[\"train\"] = drug_dataset_final[\"train\"].shuffle(seed=42).select(range(int(pct*len(drug_dataset_final[\"train\"]))))\n",
    "drug_dataset_final[\"validation\"] = drug_dataset_final[\"validation\"].shuffle(seed=42).select(range(int(pct*len(drug_dataset_final[\"validation\"]))))\n",
    "drug_dataset_final[\"test\"] = drug_dataset_final[\"test\"].shuffle(seed=42).select(range(int(pct*len(drug_dataset_final[\"test\"]))))\n",
    "\n",
    "drug_dataset_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73e251c",
   "metadata": {},
   "source": [
    "Let's look if the ClassLabels distribution is correct amongst the splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ceebb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_distributions(split, dataset):\n",
    "    print(f\"{split} ClassLabels distribution:\")\n",
    "\n",
    "    # get the distribution numbers\n",
    "    class_labels_counts = Counter(dataset[split][\"labels\"])\n",
    "\n",
    "    # distribution dictionary\n",
    "    dist = {}\n",
    "    # for every label \n",
    "    for label_id, count in class_labels_counts.items():\n",
    "        # get the label name using the label id\n",
    "        label_name = dataset[split].features[\"labels\"].int2str(label_id)\n",
    "        # compute the percentage\n",
    "        pct = round((count/len(drug_dataset_final[split]))*100, 2)\n",
    "\n",
    "        # add it to the distribution dict\n",
    "        dist[label_id] = (label_name, pct)\n",
    "    \n",
    "    # sort the distribution dict on label id and print the data\n",
    "    sorted_dist = dict(sorted(dist.items()))\n",
    "    for label_id, name_pct in sorted_dist.items():\n",
    "        print(f\"\\t id - {label_id} {name_pct[0]} : {name_pct[1]}% \")\n",
    "\n",
    "\n",
    "print_distributions(\"train\", drug_dataset_final)\n",
    "\n",
    "print_distributions(\"validation\", drug_dataset_final)\n",
    "\n",
    "print_distributions(\"test\", drug_dataset_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4800da",
   "metadata": {},
   "source": [
    "### 7. Initialise the Model and other config\n",
    "\n",
    "Now that we have our final dataset, let's now:\n",
    "+ gather the tokeniser and the model, \n",
    "+ tokenise the data and refine it all for once. \n",
    "\n",
    "> Note: When initialising the model, we also have to specify the `num_labels=5` arguments because we are training the model for a multi-class classification task and there are `5` labels in total:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff02b90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding, AutoModelForSequenceClassification\n",
    "from torch.utils.data import DataLoader\n",
    "from pprint import pprint\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokeniser = AutoTokenizer.from_pretrained(checkpoint)\n",
    "# initialise the model and also specify the number of labels\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=6)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokeniser)\n",
    "\n",
    "def tokenisation_function(data):\n",
    "    return tokeniser(data['review'], truncation=True)\n",
    "\n",
    "tokenised_datasets = drug_dataset_final.map(\n",
    "    tokenisation_function,\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "\n",
    "pprint(tokenised_datasets)\n",
    "\n",
    "\n",
    "tokenised_datasets = tokenised_datasets.remove_columns(\n",
    "    column_names=['patient_id', 'drugName', 'review', 'rating', 'date', 'usefulCount', 'review_length']\n",
    ")\n",
    "\n",
    "tokenised_datasets.set_format(\"torch\")\n",
    "\n",
    "pprint(tokenised_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d3ff5d",
   "metadata": {},
   "source": [
    "### 8. Set up the dataloader\n",
    "\n",
    "> Note: if you didn't take the 10% sample of the data at `step 6` and in total there are still `65k+` data. It could be worth setting the `train_batch_size` <= `16`, if your GPU does not have a lot of memory, otherwise, the training will take time; also similarly for `eval_batch_size` and `test_batch_size` we will set it to `64`, otherwise the evaluation stage will take a lot of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cece146c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 16\n",
    "eval_batch_size = min(64, len(tokenised_datasets[\"validation\"]))\n",
    "test_batch_size = min(64, len(tokenised_datasets[\"test\"]))\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=tokenised_datasets[\"train\"],\n",
    "    batch_size=train_batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    dataset=tokenised_datasets[\"validation\"],\n",
    "    batch_size=eval_batch_size,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=tokenised_datasets[\"test\"],\n",
    "    batch_size=test_batch_size,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"So there are,\\n\\t{len(train_dataloader)} batches of size {train_batch_size} in the training dataset,\\n\\t{len(eval_dataloader)} batches of size {eval_batch_size} in the evaluation dataset, and\\n\\t {len(test_dataloader)} batches of size {test_batch_size} in the test dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c07159",
   "metadata": {},
   "source": [
    "### 9. Setup the *accelerator*, *optimisor* and *learning rate scheduler* object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fdb301",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "\n",
    "# optimiser\n",
    "optimiser = AdamW(\n",
    "    params=model.parameters(),\n",
    "    lr=2e-5\n",
    ")\n",
    "\n",
    "# accelerator\n",
    "accelerator = Accelerator()\n",
    "# preparing accelerator objects\n",
    "train_dl, eval_dl, test_dl, model, optimiser = accelerator.prepare(\n",
    "    train_dataloader,\n",
    "    eval_dataloader,\n",
    "    test_dataloader,\n",
    "    model,\n",
    "    optimiser\n",
    ")\n",
    "\n",
    "num_epochs = 5\n",
    "num_training_steps = num_epochs * len(train_dl)\n",
    "# 10% warmup\n",
    "num_warmup_steps = int(.1 * num_training_steps)\n",
    "\n",
    "lr_schedular = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimiser,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "print(f\"Total training steps {num_training_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6e5e1a",
   "metadata": {},
   "source": [
    "### 10. Evaluation Metric Setup\n",
    "\n",
    "Now this time since there is no pre-evaluation metric present, therefore we have to define which metrics to use when evaluating our model.\n",
    "<br />\n",
    "For a *Classification* task,  the best metrics to evalute a model are **Accuracy**, **Precision**, **Recall** and **F1 Score**. The latter three metrics are detrived from a **Confusion Matrix**, which is basically a `N X N matrix`, where `N` is the *number of classes or categories* that are to be predicted. The values inside the *confusion matrix* represents one of these 4 values:\n",
    "+ **True Positives (TP)** : It is the case where we predicted Yes and the real output was also Yes.\n",
    "+ **True Negatives (TN)**: It is the case where we predicted No and the real output was also No.\n",
    "+ **False Positives (FP)**: It is the case where we predicted Yes but it was actually No.\n",
    "+ **False Negatives (FN)**: It is the case where we predicted No but it was actually Yes. \n",
    "\n",
    "For example, suppose there is a problem which is a binary classification with labels as `Yes` or `No`. So, here `N = 2`, therfore we will get a `2 X 2` *confusion matrix*. Now let's say we tested our model with 165 samples and the results using *confusion matrix* looks like this:\n",
    "\n",
    "|              |Predicted No|Predited Yes|\n",
    "|--------------|------------|------------|\n",
    "|**Actual No** |50|10|\n",
    "|**Actual Yes**|5|100|\n",
    "\n",
    "Therefore, out of the 165 predictions, `100` predictions were **TP** (bottom right), `50` were **TN** (top left), `10` were **FP** (top right), and `5` were *FN* (bottom left).\n",
    "\n",
    "\n",
    "Now, how these values are useful because we can use them to calculate **Precision**, **Recall** and **F1 Score**:\n",
    "+ **Precision**: It measures how many of the positive predictions made by the model are actually correct. It's useful when the cost of false positives is high such as in medical diagnoses where predicting a disease when it’s not present can have serious consequences. Therefore, *Precision* helps ensure that when the model predicts a positive outcome, it’s likely to be correct.\n",
    "$$\n",
    "\\text{Precision} = \\frac{TP}{TP+FP}\n",
    "$$\n",
    "+ **Recall**: *Recall* or *Sensitivity measures* how many of the actual positive cases were correctly identified by the model. It is important when missing a positive case (*false negative*) is more costly than false positives (like disease detection).\n",
    "$$\n",
    "\\text{Recall} = \\frac{TP}{TP+FN}\n",
    "$$\n",
    "+ **F1 Score**: The *F1 Score* is the *harmonic mean* of *precision* and *recall*. It is useful when we need a balance between *precision* and *recall*, as it combines both into a single number. A *high F1 score* means the model performs well on both metrics, i.e., the model is performing well. Its range is `[0,1]`:\n",
    "$$\n",
    "\\text{F1 Score}=2\\times\\frac{Precision+Recall}{Precision×Recall} \n",
    "$$\n",
    "Now, when you have multiple classes, you still often want a single precision/recall/F1 number—but how you combine per-class scores depends on whether you care more about rare classes, common classes, or every example equally and there you have to use a *averaging strategy*. Here’s what each averaging strategy does:\n",
    "\n",
    "+ **Weighted**: Compute each class’s score, then average them but weight by how many true examples each class has - so common labels count more.\n",
    "\n",
    "+ **Micro**: Pool all true/false positives and negatives across every example, then compute one overall score - every prediction is equal (large classes dominate).\n",
    "\n",
    "+ **Macro**: Compute each class’s score and then take the simple average—every class counts the same, no matter how many examples it has.\n",
    "\n",
    "> NOTE: **Lower recall** and **higher precision** gives us **great accuracy** but then it misses a large number of instances and that's why **accuracy** alone is not a good metric when evaluating a model and using **Recall**, **Precision** and **F1 score** if possible is a good practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2fde33",
   "metadata": {},
   "source": [
    "Luckily, the `evaluate` lib provides `combine()` method, where you can specify which metrics to use for the evaluation, and also when calling the `compute()` we can pass the `average` argument to specify which averaging strategy to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7f3142",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import torch\n",
    "\n",
    "def perform_evaluation():\n",
    "    \"\"\"\n",
    "    Perform evaluation on the validation set\n",
    "    \"\"\"\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    eval_epoch_loss = []\n",
    "\n",
    "    # initialising evaluation metrics\n",
    "    ## accuracy\n",
    "    eval_acc_metric = evaluate.load(\"accuracy\") \n",
    "    ## f1 score\n",
    "    eval_f1_metric = evaluate.load(\"f1\")\n",
    "    ## precision & recall\n",
    "    eval_specific_metric = evaluate.combine(\n",
    "        evaluations=[\n",
    "            \"precision\",\n",
    "            \"recall\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # for every validation batch\n",
    "    for batch in eval_dl:\n",
    "        # Disable gradient computation for evaluation (saves memory and computation)\n",
    "        with torch.no_grad():\n",
    "            # pass the input to the model\n",
    "            outputs = model(**batch)\n",
    "            # Store loss inside no_grad for memory efficiency\n",
    "            eval_epoch_loss.append(outputs.loss.item())\n",
    "\n",
    "            # Get predictions for metrics (logits already created without gradients)\n",
    "            logits = outputs.logits\n",
    "            refs = batch[\"labels\"]\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            # Add preds and refs to evaluation metrics\n",
    "            ## accuracy\n",
    "            eval_acc_metric.add_batch(\n",
    "                predictions=accelerator.gather(preds),\n",
    "                references=accelerator.gather(refs)\n",
    "            )\n",
    "            ## f1 score\n",
    "            eval_f1_metric.add_batch(\n",
    "                predictions=accelerator.gather(preds),\n",
    "                references=accelerator.gather(refs)\n",
    "            )\n",
    "            ## precision & recall\n",
    "            eval_specific_metric.add_batch(\n",
    "                predictions=accelerator.gather(preds),\n",
    "                references=accelerator.gather(refs)\n",
    "            )\n",
    "    \n",
    "    # compute the average loss\n",
    "    eval_avg_loss = sum(eval_epoch_loss) / len(eval_epoch_loss)\n",
    "\n",
    "    # dict to store the metrics stats\n",
    "    eval_pred_stats = {}\n",
    "    # compute accuracy and add it to the dict\n",
    "    eval_pred_stats.update(eval_acc_metric.compute())\n",
    "    # compute the f1 score, with 'weighted' as the averaging strategy\n",
    "    # and update the dict with the metric\n",
    "    eval_pred_stats.update(\n",
    "        eval_f1_metric.compute(\n",
    "            average=\"weighted\",\n",
    "            labels= list(range(len(label_names))) # ClassLabels\n",
    "        )\n",
    "    )\n",
    "    # compute precision and recall, with 'weighted' as the averaging strategy\n",
    "    # and update the dict with the metric. \n",
    "    eval_pred_stats.update(\n",
    "        eval_specific_metric.compute(\n",
    "            average=\"weighted\",\n",
    "            zero_division=0, # when there is a zero in the denominator, replace the result with 0\n",
    "            labels= list(range(len(label_names))) # ClassLabels\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return eval_avg_loss, eval_pred_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a513ea4",
   "metadata": {},
   "source": [
    "We would also like to evaluate our model performance on the test data after it is totally trained because testing on untouched data gives a true measure of how our model will perform on new examples and prevents us from overfitting by tuning to the same data we used to train it. \n",
    "<br />\n",
    "So, let's write the evaluation function on the test data, and this time we can also ask for the *confusion_matrix* from the `evalute.compute()` function along with other metrics to further evalute the model on the test data.\n",
    "\n",
    "> Note: when evaluating the model on the test data we don't need to look at the loss value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bc7d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_evaluation():\n",
    "    \"\"\"\n",
    "    Perform evaluation on the test set\n",
    "    \"\"\"\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # initialising evaluation metrics\n",
    "    ## accuracy\n",
    "    test_acc_metric = evaluate.load(\"accuracy\")\n",
    "    ## f1 score\n",
    "    test_f1_metric = evaluate.load(\"f1\")\n",
    "    ## precision & recall\n",
    "    test_specific_metric = evaluate.combine(\n",
    "        evaluations=[\n",
    "            \"precision\",\n",
    "            \"recall\"\n",
    "        ]\n",
    "    )\n",
    "    ## confusion metrix\n",
    "    test_cm_metric = evaluate.load(\"confusion_matrix\")\n",
    "\n",
    "    # for every test batch\n",
    "    for batch in test_dl:\n",
    "        # Disable gradient computation for evaluation (saves memory and computation)\n",
    "        with torch.no_grad():\n",
    "            # pass the input to the model\n",
    "            outputs = model(**batch)\n",
    "\n",
    "            # Get predictions for metrics (logits already created without gradients)\n",
    "            logits = outputs.logits\n",
    "            refs = batch[\"labels\"]\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            # Add preds and refs to evaluation metrics\n",
    "            ## accuracy\n",
    "            test_acc_metric.add_batch(\n",
    "                predictions=accelerator.gather(preds),\n",
    "                references=accelerator.gather(refs)\n",
    "            )\n",
    "            ## f1 score\n",
    "            test_f1_metric.add_batch(\n",
    "                predictions=accelerator.gather(preds),\n",
    "                references=accelerator.gather(refs)\n",
    "            )\n",
    "            ## precision & recall\n",
    "            test_specific_metric.add_batch(\n",
    "                predictions=accelerator.gather(preds),\n",
    "                references=accelerator.gather(refs)\n",
    "            )\n",
    "            ## confusion metrix\n",
    "            test_cm_metric.add_batch(\n",
    "                predictions=accelerator.gather(preds),\n",
    "                references=accelerator.gather(refs)\n",
    "            )\n",
    "    \n",
    "    # dict to store the metrics stats\n",
    "    test_pred_stats = {}\n",
    "    # compute accuracy and add it to the dict\n",
    "    test_pred_stats.update(test_acc_metric.compute())\n",
    "    # compute the f1 score, with 'weighted' as the averaging strategy\n",
    "    # and update the dict with the metric\n",
    "    test_pred_stats.update(\n",
    "        test_f1_metric.compute(\n",
    "            average=\"weighted\",\n",
    "            labels= list(range(len(label_names))) # ClassLabels\n",
    "        )\n",
    "    )\n",
    "    # compute precision and recall, with 'weighted' as the averaging strategy\n",
    "    # and update the dict with the metric. \n",
    "    test_pred_stats.update(\n",
    "        test_specific_metric.compute(\n",
    "            average=\"weighted\",\n",
    "            labels= list(range(len(label_names))), # ClassLabels\n",
    "            zero_division=0  # when there is a zero in the denominator, replace the result with 0\n",
    "        )\n",
    "    )\n",
    "    # compute the confusion matrix \n",
    "    test_pred_stats.update(\n",
    "        test_cm_metric.compute(\n",
    "            labels= list(range(len(label_names)))  # ClassLabels\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return test_pred_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d8c265",
   "metadata": {},
   "source": [
    "let's also write a plot function so that we can visualise the confusion matrix and other metrics  from the test evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1595a5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(confusion_matrix, label_names):\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix, display_labels=label_names)\n",
    "    disp.plot(cmap=plt.cm.Blues, values_format='d')  # Use '.2f' for float\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_metrics(metrics_dict):\n",
    "    names = list(metrics_dict.keys())\n",
    "    values = list(metrics_dict.values())\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.barh(names, values, color='skyblue')\n",
    "    plt.xlabel(\"Score\")\n",
    "    plt.title(\"Evaluation Metrics\")\n",
    "    plt.xlim(0, 1)  # If all values are between 0 and 1\n",
    "    plt.grid(True, axis='x', linestyle='--', alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e438a56b",
   "metadata": {},
   "source": [
    "### 11. Training\n",
    "\n",
    "Let's write the training function first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b905c85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from livelossplot import PlotLosses\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# training progress bar\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "def training_function():\n",
    "    # initialise the plotter for the learning curve\n",
    "    plotter = PlotLosses(mode='notebook')\n",
    "\n",
    "    # for every epoch\n",
    "    for epoch in range(num_epochs):\n",
    "        # ensure model is in training mode\n",
    "        model.train()\n",
    "\n",
    "        # store loss per batch \n",
    "        train_epoch_loss = []\n",
    "\n",
    "        # metrics for training data\n",
    "        ## accuracy\n",
    "        train_acc_metric = evaluate.load(\"accuracy\")\n",
    "        ## f1 score\n",
    "        train_f1_metric = evaluate.load(\"f1\")\n",
    "        ## precision & recall\n",
    "        train_specific_metric = evaluate.combine(\n",
    "            evaluations=[\n",
    "                \"precision\",\n",
    "                \"recall\"\n",
    "            ]   \n",
    "        )\n",
    "\n",
    "        # for every bacth in the training set\n",
    "        for batch in train_dl:\n",
    "            # Forward Pass (keep gradient attached)\n",
    "            outputs = model(**batch)\n",
    "            ## get the loss\n",
    "            loss = outputs.loss\n",
    "\n",
    "            # Backward Pass (while gradients are still attached)\n",
    "            ## compute gradients\n",
    "            accelerator.backward(loss)\n",
    "            ## gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            ## Nudges the weights (\"knobs\") in the right direction based on the gradient\n",
    "            optimiser.step()\n",
    "            ## Update the learning-rate scheduler\n",
    "            lr_schedular.step()\n",
    "            ## Reset gradients to zero so they don’t accumulate\n",
    "            ## into the next batch.\n",
    "            optimiser.zero_grad()\n",
    "\n",
    "\n",
    "            # metric computation\n",
    "            with torch.no_grad():\n",
    "                # detach loss for metric computation \n",
    "                train_epoch_loss.append(loss.detach().item())\n",
    "\n",
    "                # detach logits for metric computation\n",
    "                logits = outputs.logits.detach()\n",
    "                # no need to detach labels (they don't have gradients)\n",
    "                refs = batch['labels']\n",
    "                preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "                # add preds and refs to the train matrics\n",
    "                ## accuracy\n",
    "                train_acc_metric.add_batch(\n",
    "                    predictions=accelerator.gather(preds),\n",
    "                    references=accelerator.gather(refs)\n",
    "                )\n",
    "                ## f1 score\n",
    "                train_f1_metric.add_batch(\n",
    "                    predictions=accelerator.gather(preds),\n",
    "                    references=accelerator.gather(refs)\n",
    "                )\n",
    "                ## precision and recall\n",
    "                train_specific_metric.add_batch(\n",
    "                    predictions=accelerator.gather(preds),\n",
    "                    references=accelerator.gather(refs)\n",
    "                )\n",
    "            \n",
    "            # update the progress bar by 1 step\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        # training average loss\n",
    "        tain_avg_loss = sum(train_epoch_loss)/len(train_epoch_loss)\n",
    "\n",
    "        ## dict to store the metrics stats\n",
    "        train_pred_stats = {}\n",
    "        # compute accuracy and add it to the dict\n",
    "        train_pred_stats.update(train_acc_metric.compute())\n",
    "        # compute the f1 score, with 'weighted' as the averaging strategy\n",
    "        # and update the dict with the metric\n",
    "        train_pred_stats.update(\n",
    "            train_f1_metric.compute(\n",
    "                average=\"weighted\",\n",
    "                labels= list(range(len(label_names))) # ClassLabels\n",
    "            )\n",
    "        )\n",
    "        # compute precision and recall, with 'weighted' as the averaging strategy\n",
    "        # and update the dict with the metric. \n",
    "        train_pred_stats.update(\n",
    "            train_specific_metric.compute(\n",
    "                average=\"weighted\",\n",
    "                labels= list(range(len(label_names))), # ClassLabels\n",
    "                zero_division=0 # when there is a zero in the denominator, replace the result with 0\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "        # evaluation phase\n",
    "        eval_avg_loss, eval_pred_stats = perform_evaluation()\n",
    "\n",
    "        # update the learning curve\n",
    "        plotter.update({\n",
    "            'loss': tain_avg_loss,\n",
    "            'val_loss': eval_avg_loss,\n",
    "            'acc': train_pred_stats['accuracy'],\n",
    "            'val_acc': eval_pred_stats['accuracy'],\n",
    "            'precision': train_pred_stats['precision'],\n",
    "            'val_precision': eval_pred_stats['precision'],\n",
    "            'recall': train_pred_stats['recall'],\n",
    "            'val_recall': eval_pred_stats['recall'],\n",
    "            'f1': train_pred_stats['f1'],\n",
    "            'val_f1': eval_pred_stats['f1'],\n",
    "        })\n",
    "        plotter.send() \n",
    "\n",
    "\n",
    "    print(\"\\n\\n\\n################# Test dataset Evaluation:\\n\\n\")\n",
    "    # After the model is totally trained, perform evaluation on the test dataset\n",
    "    test_pred_stats = test_evaluation()\n",
    "\n",
    "    confusion_matrix = test_pred_stats.pop(\"confusion_matrix\")\n",
    "    # plot the metrics\n",
    "    plot_confusion_matrix(confusion_matrix, label_names)\n",
    "    plot_metrics(test_pred_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc212c85",
   "metadata": {},
   "source": [
    "Finally, let's launch the training loop with `num_processes=1`, as my machine has only 1 dedicated gpu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21805635",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import notebook_launcher\n",
    "\n",
    "# launch the accelrator based training funcrion with one gpu\n",
    "notebook_launcher(training_function, num_processes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25dca31",
   "metadata": {},
   "source": [
    "### 12. Performance Analysis\n",
    "\n",
    "As we can see, in the initial training phase, it was observed that validation metrics (accuracy, F1, precision, recall) were unexpectedly higher than training metrics during the first epoch. This behavior, though uncommon, can occur due to factors such as *dropout* being applied only during training, the use of a *pretrained model* that already performs well on validation data, or inconsistencies in metric aggregation (e.g., batch-wise vs full set). As training progressed, the model quickly improved on the training set, with metrics surpassing validation scores by the second epoch. However, validation performance *plateaued* and the validation *loss* began to rise slightly after epoch 2, suggesting early signs of *overfitting*. This indicates that employing **early stopping** (around epoch 2–3) and **stronger regularization** or **data augmentation** strategies may help maintain generalization.\n",
    "\n",
    "\n",
    "On the held-out test set, the model demonstrated strong and consistent performance across all key metrics (accuracy, precision, recall, and F1 score), all approaching or above 0.93. The confusion matrix further supports this, showing high classification accuracy across categories like birth control, depression, and anxiety, with only minor misclassifications—most notably, some confusion between acne and birth control, and between depression and anxiety. Overall, the model generalizes well to unseen data and handles class separation effectively, confirming the effectiveness of the training approach despite the early metric inversion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918fa3f1",
   "metadata": {},
   "source": [
    "# Managing Big Data\n",
    "Nowadays, it's common for datasets used to train models from scratch to range from multiple gigabytes to several terabytes. In such cases, even loading the data can be challenging - especially when hardware is limited, such as having restricted RAM or GPU memory.\n",
    "<br />\n",
    "Fortunately, the Hugging Face datasets library is designed to handle these challenges:\n",
    "\n",
    "- It addresses memory management issues by treating datasets as memory-mapped files, enabling efficient access without loading the entire dataset into RAM.\n",
    "\n",
    "- It also offers a streaming feature that allows you to access data on-the-fly. This is especially useful when you can’t store a large dataset locally - data is downloaded and processed one sample at a time, without requiring the full dataset to be downloaded first.\n",
    "\n",
    "You don’t need to do anything special to benefit from memory-mapping - it works automatically in all the examples we’ve seen so far.\n",
    "\n",
    "So in this section, we’ll focus on how the streaming feature works in practice.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11d7ac9",
   "metadata": {},
   "source": [
    "### Streaming dataset\n",
    "\n",
    "To enable dataset streaming you just need to pass the `streaming=True` argument to the `load_dataset()` function.\n",
    "<br />\n",
    "For example, let’s in *streaming* mode load the [*HuggingFace FineWeb*](https://huggingface.co/datasets/HuggingFaceFW/fineweb) dataset, it is a 18.5T tokens (originally 15T tokens) of cleaned and deduplicated english web data from CommonCrawl. Its total size is 108 TB.\n",
    "\n",
    "> Note: we are using this dataset only for example purposes. It is mainly used to train LLM models and the data processing pipeline is optimized for LLM performance and ran on the  [datatrove](https://github.com/huggingface/datatrove/) library (a large scale data processing library) and not on `datasets`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6de5440",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "fineweb_dataset = load_dataset(\n",
    "    \"HuggingFaceFW/fineweb\",\n",
    "    streaming=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699dde7b",
   "metadata": {},
   "source": [
    "Now, when we have `streaming=True` in the `load_dataset()` function instead of the usual `DatasetDict` object it returns an `IterableDataset` object. As the name suggests, to access the elements of an `IterableDataset` we need to iterate over it using `iter()` enclosed inside a `next()`(to get the next present value in the iteration):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf71383",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(fineweb_dataset['train']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06586af7",
   "metadata": {},
   "source": [
    "Now to process the data inside the `IterableDataset` object, for example, during *pre-processing* or *tokenisation*, we can the `IterableDataset.map()`. The process is exactly the same as the one we used to tokenize our `DatasetDict` dataset prevously, with the only difference being that outputs are returned one by one, but we can also pass `batched=True` here, and it will process the examples batch by batch; the default batch size is 1,000 and can be specified with the `batch_size` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd24831",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from pprint import pprint \n",
    "\n",
    "checkpoint = \"distilbert-base-uncased\"\n",
    "tokeniser = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "tokenised_datasets = fineweb_dataset.map(\n",
    "    lambda data: tokeniser(data[\"text\"], truncation=True),\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "pprint(next(iter(tokenised_datasets['train'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d37de8d",
   "metadata": {},
   "source": [
    "We can also shuffle a streamed dataset using `IterableDataset.shuffle()`, but unlike `Dataset.shuffle()` this only shuffles the elements in a predefined `buffer_size`:\n",
    "\n",
    "> Note: In this example, we selected a random example from the first `10,000` examples in the buffer. Once an example is accessed, its spot in the buffer is filled with the next example in the corpus (i.e., the `10,001`st example in the case above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c26a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_dataset = fineweb_dataset.shuffle(buffer_size=10_000, seed=42)\n",
    "next(iter(shuffled_dataset[\"train\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a47307",
   "metadata": {},
   "source": [
    "To select the first `n` examples we can call the `IterableDataset.take()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8a0634",
   "metadata": {},
   "outputs": [],
   "source": [
    "fineweb_dataset_head = fineweb_dataset[\"train\"].take(5)\n",
    "list(fineweb_dataset_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb796c3",
   "metadata": {},
   "source": [
    "And similary, we can use the `IterableDataset.skip()` function to skip n examples and combined both `take()` and `skip()` to even create splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb43b248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip the first 1,000 examples and include the rest in the training set\n",
    "train_split_dataset = shuffled_dataset[\"train\"].skip(1000)\n",
    "# Take the first 1,000 examples for the validation set\n",
    "validation_split_dataset =  shuffled_dataset[\"train\"].take(1000)\n",
    "\n",
    "pprint(train_split_dataset)\n",
    "\n",
    "pprint(validation_split_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60de838",
   "metadata": {},
   "source": [
    "Lastly, we can also combine multiple datasets together to create a single corpus using the `interleave_datasets()` function. It converts a list of IterableDataset objects into a single IterableDataset, where the elements of the new dataset are obtained by alternating among the source examples.\n",
    "Let's combine the above dataset with [**FineWeb2**](https://huggingface.co/datasets/HuggingFaceFW/fineweb-2) dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966a9e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "fineweb2_dataset = load_dataset(\n",
    "    \"HuggingFaceFW/fineweb-2\",\n",
    "    \"aai_Latn\",\n",
    "    streaming=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed737868",
   "metadata": {},
   "source": [
    "let’s now combine the  datasets with the interleave_datasets() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ebb00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "from datasets import interleave_datasets\n",
    "\n",
    "fineweb_train_combined_dataset = interleave_datasets([fineweb_dataset[\"train\"], fineweb2_dataset[\"train\"]])\n",
    "list(islice(fineweb_train_combined_dataset, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66a1229",
   "metadata": {},
   "source": [
    "Here we’ve used the `islice()` function from Python’s `itertools` module to select the first two examples from the combined dataset, and we can see that they match the first examples from each of the two source datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc5a5ff",
   "metadata": {},
   "source": [
    "# Semantic search with FAISS\n",
    "Let's use the [*lewtun/github-issues*](https://huggingface.co/datasets/lewtun/github-issues) dataset, a corpus of GitHub issues associated with [**HuggingFace Datasets**](https://github.com/huggingface/datasets/issues) branch, where each issues contains a title, a description, and a set of labels that characterize the issue, and use this information to build a search engine that can help us find answers to our most pressing questions about the library!\n",
    "\n",
    "<img src=data/chapter_5/datasets-issues-single.png width=\"800\"/> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a72a51",
   "metadata": {},
   "source": [
    "### Using embeddings for semantic search\n",
    "\n",
    "As we saw earlier, Transformer-based language models first turn each *token* into an *integer ID*, and then look up a *fixed-size input embedding vector* for that ID. After the tokens pass through all the *self-attention layers*, we get a set of **contextual embeddings*—one vector per token that now knows about the whole sentence.\n",
    "<br />\n",
    "By *pooling* these *contextual token embeddings*—for example, by taking the mean, grabbing the special `[CLS]` vector, or another strategy - we collapse them into a single vector that represents an entire sentence, paragraph, or even a whole document.\n",
    "\n",
    "> Example: The sentence “Bug fixed.” is tokenised as `[CLS] bug fixed [SEP]`. Each token is turned into an ID (e.g., `101`, `12572`, `2196`, `102`) and looked up in BERT’s embedding table, yielding four separate `768-dimensional vectors` — *one per token*. These are still “raw” (they only identify the token itself). The vectors then pass through BERT’s 12 *self-attention layers*, becoming *contextual* — now each vector encodes information about the whole sentence. When we need a single vector for the entire issue, we choose a **pooling strategy**. Mean pooling averages all contextual vectors, but an even simpler option is `[CLS]` *pooling*: we keep only the vector at the first position. During BERT’s pre-training the model is explicitly told that this `[CLS]` position will be used for downstream classification tasks, so the network learns to funnel **sentence-level information** into that spot. Grabbing it is fast (no averaging) and often works well because it was trained to act as a ready-made summary.\n",
    "\n",
    "<br />\n",
    "Once we have these pooled embeddings, we can compare documents by measuring how similar their vectors are—for example, with a dot-product (cosine) score. Documents whose vectors are most alike are considered the most semantically similar.\n",
    "\n",
    "![\"semantic search\"](data/chapter_5/semantic_search.png \"semantic_search\")\n",
    "\n",
    "Let's build a semantic search engine that uses these embeddings. Unlike traditional keyword search, semantic search looks at meaning rather than exact word matches, so it can retrieve relevant documents even when they don’t share the same keywords as the query.\n",
    "\n",
    "### Loading and preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dec70a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "issues_dataset = load_dataset(\"lewtun/github-issues\", split=\"train\")\n",
    "\n",
    "issues_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759b5c66",
   "metadata": {},
   "source": [
    "Normally, when we fetch the issues from the Github API it also returns all the *pull_request* along with the *issues*. So, let's now filter out the rows that corresponds to *pull_request*.\n",
    "<br />\n",
    "Luckily, this dataset comes with an additional column `is_pull_request`, to specify if a particular row contains *pull request* data. While we’re at it, let’s also filter out rows with no comments, since these provide no answers to user queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a166e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "issues_dataset = issues_dataset.filter(\n",
    "    lambda x: (x[\"is_pull_request\"] == False and len(x[\"comments\"]) > 0)\n",
    ")\n",
    "\n",
    "print(issues_dataset)\n",
    "\n",
    "issues_dataset.num_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727cd249",
   "metadata": {},
   "source": [
    "We can see that there are a lot of column in out dataset (28), most of which we don’t need to build our search engine. From a search perspective, the most informative columns are `title`, `body`, and `comments`, while `html_url` provides us with a link back to the source issue. Let’s use the `remove_columns()` function to drop the rest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21375a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns =  issues_dataset.column_names\n",
    "columns_to_keep = [\"title\", \"body\", \"html_url\", \"comments\"]\n",
    "columns_to_rm = set(columns_to_keep).symmetric_difference(columns)\n",
    "\n",
    "issues_dataset = issues_dataset.remove_columns(column_names=columns_to_rm)\n",
    "\n",
    "issues_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980a3fd0",
   "metadata": {},
   "source": [
    "To create our embeddings we’ll augment each comment with the issue’s title and body, since these fields often include useful contextual information. Because our comments column is currently a list of comments for each issue, we need to \"**explode**” the column so that each row consists of an (html_url, title, body, comment) tuple. In Pandas we can do this with the `DataFrame.explode()` function, which creates a new row for each element in a list-like column, while replicating all the other column values. To see this in action, let’s first switch to the Pandas DataFrame format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ed7385",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "issues_dataset.set_format(\"pandas\")\n",
    "issues_df = issues_dataset[:]\n",
    "\n",
    "issues_df[\"comments\"][0].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af29e2b",
   "metadata": {},
   "source": [
    "We can see from above that the issue at index 0 have two comments. Now, when we explode `issues_df` based on the `comments` column, we expect to get one row for each of these comments. Let’s check if that’s the case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7557e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_df = issues_df.explode(\"comments\", ignore_index=True)\n",
    "comment_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153faa8d",
   "metadata": {},
   "source": [
    "Great, we can see the rows have been replicated, with the comments column containing the individual comments! Now that we’re finished with `Pandas`, we can quickly switch back to a `Dataset` by loading the DataFrame in memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914c8faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "comments_dataset = Dataset.from_pandas(comment_df)\n",
    "\n",
    "comments_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1dd84c",
   "metadata": {},
   "source": [
    "Let's now change the `comments` column name to `comment` and also add a new column called `comment_length` and using it filter out row with comment length less than 15:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ccfe4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_dataset = comments_dataset.rename_column(\n",
    "    original_column_name=\"comments\",\n",
    "    new_column_name=\"comment\"\n",
    ")\n",
    "\n",
    "\n",
    "comments_dataset = comments_dataset.map(\n",
    "    lambda batch: \n",
    "    {\n",
    "        \"comment_length\" : [\n",
    "            len(comment.split()) for comment in batch[\"comment\"]\n",
    "        ]\n",
    "    },\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "comments_dataset = comments_dataset.filter(\n",
    "    lambda batch : [\n",
    "        comment_length > 15 for comment_length in batch[\"comment_length\"]\n",
    "    ],\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "comments_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53071967",
   "metadata": {},
   "source": [
    "Having cleaned up our dataset a bit, let’s concatenate the issue title, description, and comments together in a new text column. As usual, we’ll write a simple function that we can pass to Dataset.map():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fc1201",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_text(batch):\n",
    "\n",
    "    text = [\n",
    "        f\"{title} \\n {body} \\n {comment}\"\n",
    "        for title, body, comment in zip(batch[\"title\"], batch[\"body\"], batch[\"comment\"])\n",
    "    ]\n",
    "    return {\"text\" : text}\n",
    "\n",
    "\n",
    "comments_dataset = comments_dataset.map(\n",
    "    concatenate_text,\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "comments_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a396efe8",
   "metadata": {},
   "source": [
    "### Creating text embeddings\n",
    "\n",
    "Since we know to get the embedding representation of a input all we need is the last hidden state output of a model.. Fortunately, there’s a library called `sentence-transformers` that contains all the right models which are dedicated for creating embeddings. As described in the `sentence-transformers` library’s [documentation](https://www.sbert.net/examples/applications/semantic-search/README.html#symmetric-vs-asymmetric-semantic-search), our use case is an example of **asymmetric semantic search** because *we have a short query whose answer we’d like to find in a longer document, like a an issue comment*.\n",
    "<br /> \n",
    "The handy `sentence-transformers` library’s documentation [model overview table](https://www.sbert.net/docs/sentence_transformer/pretrained_models.html#model-overview) indicates that the `all-mpnet-base-v2` checkpoint has the best performance for *semantic search*, so we’ll use that for our application. We’ll also load the tokenizer using the same checkpoint:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea00245",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model_ckpt = 'sentence-transformers/all-mpnet-base-v2'\n",
    "\n",
    "tokeniser = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModel.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d44a3e",
   "metadata": {},
   "source": [
    "To speed up the embedding process, it helps to place the model and inputs on a GPU device, so let’s do that now:\n",
    "> Note: No need to run the cell below if your device does not have a GPU or if there is a TPU specify that first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328177a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5f4841",
   "metadata": {},
   "source": [
    "Now, we need one vector for each GitHub issue.As we mentioned earlier, the easiest way is `[CLS]` *pooling*.\n",
    "<br />\n",
    "One popular approach is to perform `[CLS]` *pooling* on our *model’s outputs*, where we simply collect the *last hidden state* for the special `[CLS]` *token*. The following function does the trick for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f26705",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cls_pooling(model_output):\n",
    "    # returns the [CLS] vector which is at index 0\n",
    "    # for every input passed to the model\n",
    "    return model_output.last_hidden_state[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f845ed39",
   "metadata": {},
   "source": [
    "Next, we’ll create a helper function that will tokenize a list of documents, place the tensors on the GPU, feed them to the model, and finally apply CLS pooling to the outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0306756",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(text_list):\n",
    "    tokenised_input = tokeniser(\n",
    "        text_list,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    tokenised_input = {\n",
    "        k : v.to(device) for k, v in tokenised_input.items()\n",
    "    }\n",
    "\n",
    "    model_output = model(**tokenised_input)\n",
    "\n",
    "    return cls_pooling(model_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb44e3ea",
   "metadata": {},
   "source": [
    "We can test the function works by feeding it the first text entry in our corpus and inspecting the output shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fff587",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = get_embeddings(comments_dataset[\"text\"][0])\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1db83e",
   "metadata": {},
   "source": [
    "Great, we’ve converted the first entry in our corpus into a 768-dimensional vector!\n",
    "<br />\n",
    "We can use `Dataset.map()` to apply our `get_embeddings()` function to each row in our corpus, so let’s create a new embeddings column as follows:\n",
    "\n",
    "> Note: We detach the embedding to remove it from the autograd graph, move it to the CPU because .numpy() only works on CPU tensors, and finally convert it to a NumPy array so the Datasets library can hand it off to FAISS (Facebook AI Similarity Search)) for indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1599188",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dataset = comments_dataset.map(\n",
    "    lambda batch : {\n",
    "        \"embedding\" : [\n",
    "            get_embeddings(text).detach().cpu().numpy()[0]  # 1-D array for each text\n",
    "            for text in batch[\"text\"]\n",
    "        ]\n",
    "    },\n",
    "    batched = True\n",
    ")\n",
    "\n",
    "embeddings_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a572a35a",
   "metadata": {},
   "source": [
    "### Using FAISS for efficient similarity search\n",
    "\n",
    "Now that we have a dataset of *embeddings*, we need some way to search over them. To do this, we’ll use a special data structure in `Datasets` called a **FAISS index**. [FAISS](https://faiss.ai/) (short for Facebook AI Similarity Search) is a library that provides efficient algorithms to quickly search and cluster embedding vectors.\n",
    "<br />\n",
    "The basic idea behind *FAISS* is to create a special data structure called an **index** that allows one to find which *embeddings are similar to an input embedding*. Creating a *FAISS index* in `Datasets` is simple — we use the `Dataset.add_faiss_index()` function and specify which column of our dataset we’d like to index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be476946",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dataset.add_faiss_index(column=\"embedding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6346fe77",
   "metadata": {},
   "source": [
    "So first we need to convert the question query into embedding vector, i.e., just like with the embedded dataset, we need to have a 768-dimensional vector representing the query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b86cf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_query = \"How can I load a dataset offline?\"\n",
    "\n",
    "question_query_embedding = get_embeddings(question_query).detach().cpu().numpy()\n",
    "question_query_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba475d2",
   "metadata": {},
   "source": [
    "Now that we have the question query in its embedding form, we can compare it against the whole corpus to find the most similar embeddings doing a nearest neighbor lookup with the `Dataset.get_nearest_examples()` function, with the main parameter being `k` which specify how many nearest neighbor to lookup.\n",
    "The `Dataset.get_nearest_examples()` function returns a tuple of *scores* that rank the overlap between the query and the document, and a corresponding set of *samples*. So the total of the returned tuple will be `k`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d50181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare 6 nearest neighbors\n",
    "k = 6\n",
    "\n",
    "scores, samples = embeddings_dataset.get_nearest_examples(\n",
    "    index_name=\"embedding\",\n",
    "    query=question_query_embedding,\n",
    "    k=k\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6e061b",
   "metadata": {},
   "source": [
    "Let’s collect these in a `pandas.DataFrame` so we can easily sort them and visualise them using its `head()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aede01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# convert the sample dict to pd\n",
    "samples_df = pd.DataFrame.from_dict(samples)\n",
    "# add the corresponding scores\n",
    "samples_df[\"score\"] = scores\n",
    "# sort based on the scores\n",
    "samples_df.sort_values(\"score\", ascending=False, inplace=True)\n",
    "\n",
    "# visualise in a table form\n",
    "samples_df.head(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc49850",
   "metadata": {},
   "source": [
    "# The End!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
