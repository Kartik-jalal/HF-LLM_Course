{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccfc270b",
   "metadata": {},
   "source": [
    "# Handling Local Data\n",
    "To load datasets that are stored either on your laptop or on a remote server, we can still use the `load_dataset()` function. This time, we just need to specify the type of loading script in the `load_dataset()` function, along with a `data_files=''` argument that specifies the path to one or more files.\n",
    "\n",
    "![\"load_dataset()\"](data/chapter_5/load_dataset.png \"load_dataset()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99872279",
   "metadata": {},
   "source": [
    "### Loading a local dataset\n",
    "\n",
    "| Data format | Loading script | Example |\n",
    "|-------------|----------------|---------|\n",
    "| CSV & TSV |`csv`|`load_dataset(\"csv\", data_files=\"my_file.csv\")`|\n",
    "| Text files |`text`|`load_dataset(\"text\", data_files=\"my_file.txt\")`|\n",
    "| JSON & JSON Lines |`json`|`load_dataset(\"json\", data_files=\"my_file.json\")`|\n",
    "| Pickled DataFrames |`pandas`|`load_dataset(\"pandas\", data_files=\"my_dataframe.pkl\")`|\n",
    "\n",
    "For this example, let's use the [SQuAD-it](https://github.com/crux82/squad-it/) dataset, which is a large-scale **json** dataset for question answering in Italian. It's hosted on GitHub, let's first download it in our `data/chapter_5` dir using `wget` and then decompress these compressed files `SQuAD_it-train.json.gz`, `SQuAD_it-test.json.gz` using `gzip`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef67e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd data/chapter_5 && wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-train.json.gz\n",
    "!cd data/chapter_5 && wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-test.json.gz\n",
    "\n",
    "!cd data/chapter_5 && gzip -dkv SQuAD_it-*.json.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfbe4aa",
   "metadata": {},
   "source": [
    "Now that we have our data in the `JSON` format, we can simply use the `load_dataset()` function, we just need to know if we’re dealing with **ordinary JSON** (*similar to a nested dictionary*) or **JSON Lines** (*line-separated JSON*). Like many question answering datasets, **SQuAD-it** uses the *nested format*, with all the text stored in a **data field**. This means we can load the dataset by specifying the `field='data'` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102036ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "squad_it_dataset = load_dataset(\"json\", data_files=\"data/chapter_5/SQuAD_it-train.json\", field=\"data\")\n",
    "\n",
    "squad_it_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c461d993",
   "metadata": {},
   "source": [
    "As we can see, by default, loading local files creates a `DatasetDict` object with only a **train** split. But, what we really want is to include both the **train** and **test** splits in a single `DatasetDict` object so we can apply `Dataset.map()` functions across both splits at once. To do this, we can provide a dictionary to the \n",
    "```python\n",
    "data_files={\"train\":\"path to the training data\", \"test\":\"path to the testing data\"}\n",
    "```\n",
    "argument that maps each split name to a file associated with that split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc636f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {\n",
    "    \"train\":\"data/chapter_5/SQuAD_it-train.json\",\n",
    "    \"test\":\"data/chapter_5/SQuAD_it-test.json\"\n",
    "}\n",
    "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")\n",
    "squad_it_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb86667",
   "metadata": {},
   "source": [
    "The loading scripts in Datasets actually support automatic decompression of the input files, so we could have skipped the use of gzip by pointing the `data_files` argument directly to the compressed files:\n",
    "```python\n",
    "data_files = {\n",
    "    \"train\": \"data/chapter_5/SQuAD_it-train.json.gz\", \n",
    "    \"test\": \"data/chapter_5/SQuAD_it-test.json.gz\"\n",
    "}\n",
    "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")\n",
    "```\n",
    "This can be useful if you don’t want to manually decompress many `GZIP` files. The automatic decompression also applies to other common formats like `ZIP` and `TAR`, so you just need to point `data_files` to the compressed files.\n",
    "\n",
    "> The `data_files` argument is also quite flexible and can be either *a single file path*, *a list of file paths*, or *a dictionary* that maps split names to file paths. You can also *glob files* that match a *specified pattern* according to the rules used by the `Unix shell` (e.g., you can glob all the `JSON` files in a directory as a single split by setting `data_files=\"*.json\"`). See the [Datasets documentation](https://huggingface.co/docs/datasets/loading#local-and-remote-files) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a5facc",
   "metadata": {},
   "source": [
    "### Loading a remote dataset\n",
    "\n",
    "Fortunately, loading *remote files* is just as simple as loading *local* ones!\n",
    "<br />\n",
    "Instead of providing a path to *local files*, we point the `data_files` argument to **one or more URLs** where the *remote files* are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf78445",
   "metadata": {},
   "outputs": [],
   "source": [
    "url =  \"https://github.com/crux82/squad-it/raw/master/\"\n",
    "\n",
    "data_files = {\n",
    "    \"train\": url + \"SQuAD_it-train.json.gz\",\n",
    "    \"test\": url + \"SQuAD_it-test.json.gz\",\n",
    "}\n",
    "\n",
    "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")\n",
    "squad_it_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7a6b66",
   "metadata": {},
   "source": [
    "# Data Manipulation\n",
    "\n",
    "The `DatasetDict` object comes with a lot of functionalities to manipulate the original dataset.\n",
    "<br />\n",
    "For this example, we’ll use the [Drug Review Dataset](https://archive.ics.uci.edu/ml/datasets/Drug+Review+Dataset+%28Drugs.com%29) that’s hosted on the [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php), which contains patient reviews on various drugs, along with the condition being treated and a 10-star rating of the patient’s satisfaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d60332",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd data/chapter_5/ && wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip\"\n",
    "!cd data/chapter_5/ && unzip drugsCom_raw.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bef2b6b",
   "metadata": {},
   "source": [
    "As we can see, this the data is in the `TSV` format which is a variant of `CSV` that uses tabs instead of commas as the separator. So, when loading these files using `load_dataset()`, we use the specify `csv` as the *loading script* and most importantly the `delimiter=\\t` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a62b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_files = {\n",
    "    \"train\" : \"data/chapter_5/drugsComTrain_raw.tsv\",\n",
    "    \"test\" : \"data/chapter_5/drugsComTest_raw.tsv\"\n",
    "}\n",
    "\n",
    "drug_dataset = load_dataset(\"csv\", data_files=data_files, delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901948e7",
   "metadata": {},
   "source": [
    "Now that we have the `DatasetDict` object, we can create a random sample to get a quick feel for the type of data you’re working with and to do so we simply have to chain the `Dataset.shuffle()` and `Dataset.select()` function to first randomly shuffle the data  (we can also pass the `seed` argument to later use the same shuffle) and select/see the first *n* data elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd508b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_sample = drug_dataset[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "\n",
    "drug_sample[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0004751",
   "metadata": {},
   "source": [
    "From above we can see before passing this data to the model or even for tokenisation we need to perform few pre-processing steps:\n",
    "  + The `Unnamed: 0` column needs to be renamed to `patient_id`.\n",
    "  + The `condition` column includes a mix of *uppercase* and *lowercase* labels.\n",
    "  + The `reviews` are of varying length and contain a mix of Python line separators `(\\r\\n)` as well as HTML character codes like `&\\#039;`.\n",
    "\n",
    "So, we can use the in-built functions like the, `rename_column()` - to rename the column name, `map()` and `filter()` - to map all the `condition` column values to lowercase, and also filter out the special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7b5c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "\n",
    "# rename the column name\n",
    "drug_dataset = drug_dataset.rename_column(\n",
    "    original_column_name=\"Unnamed: 0\",\n",
    "    new_column_name=\"patient_id\"\n",
    ")\n",
    "\n",
    "# map conditon column values to lowercase\n",
    "def lowercase_condition(data):\n",
    "    return {\"condition\": [row.lower() for row in data[\"condition\"]]}\n",
    "    # return {\"condition\": data[\"condition\"].lower()} # if not using batched=True in the map() function\n",
    "    \n",
    "\n",
    "# let's first remove all the rows with null values, otherwise the above\n",
    "# function will throw an error\n",
    "drug_dataset = drug_dataset.filter(lambda x: x[\"condition\"] is not None)\n",
    "\n",
    "# map lowercasse\n",
    "drug_dataset = drug_dataset.map(lowercase_condition, batched=True)\n",
    "\n",
    "\n",
    "# unescape all the HTML special characters in our corpus\n",
    "drug_dataset =  drug_dataset.map(\n",
    "    lambda x: {\"review\": [html.unescape(row) for row in x[\"review\"]]},\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "\n",
    "drug_dataset[\"train\"][:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a75b07",
   "metadata": {},
   "source": [
    ">In Python, `lambda` functions are small functions that you can define without explicitly naming them. They take the general form `lambda <arguments> : <expression>`,\n",
    "where `lambda` is one of Python’s special keywords, `<arguments>` is a list/set of *comma-separated values* that define the *inputs* to the function, and `<expression>` represents the operations you wish to execute. For example, we can define a simple lambda function that squares a number as follows: `lambda x : x * x`\n",
    "To apply this function to an input, we need to wrap it and the input in parentheses:\n",
    "`(lambda x: x * x)(3) -> 9`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d1d981",
   "metadata": {},
   "source": [
    "### From Datasets to DataFrames and back\n",
    "\n",
    "We can use the the `set_format()` function of the `DatasetDict` object to convert it into a different dataframe such as *Pandas*, *NumPy*, *PyTorch*, *TensorFlow*, and *JAX*. To convert it back to the `DatasetDict` object, we simply need to call the `reset_format()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a106953",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_dataset.set_format(\"pandas\")\n",
    "\n",
    "drug_dataset[\"train\"][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce5b9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_dataset.reset_format()\n",
    "\n",
    "drug_dataset[\"train\"][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cd2206",
   "metadata": {},
   "source": [
    "### Creating a validation set\n",
    "The `DatasetDict` object also provides a `Dataset.train_test_split()` function that is based on the famous functionality from `scikit-learn` which can be used to further split the data into a train-validation-test format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26ca1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80-20 percent train-validation split on the training dataset\n",
    "drug_dataset_clean = drug_dataset[\"train\"].train_test_split(train_size=0.8, seed=41)\n",
    "\n",
    "# name the 20% split data as the validation\n",
    "drug_dataset_clean[\"validation\"] = drug_dataset_clean.pop(\"test\")\n",
    "\n",
    "# Add the orignal test dataset\n",
    "drug_dataset_clean[\"test\"] = drug_dataset[\"test\"]\n",
    "\n",
    "drug_dataset_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5393b117",
   "metadata": {},
   "source": [
    "### Saving a dataset\n",
    "To save a dataset to disk:\n",
    "\n",
    "| Data format | Function |\n",
    "|-------------|----------|\n",
    "|*Arrow*|`Dataset.save_to_disk()`|\n",
    "|*CSV*|`Dataset.to_csv()`|\n",
    "|*JSON*|`Dataset.to_json()`|\n",
    "\n",
    "For example, let’s save our cleaned dataset in the Arrow format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fa6c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_dataset_clean.save_to_disk(\"data/chapter_5/drug-reviews\")\n",
    "\n",
    "!ls data/chapter_5/drug-reviews/*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dba53d",
   "metadata": {},
   "source": [
    "Once the dataset is saved, we can load it by using the load_from_disk() function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16a19ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "drug_dataset_reloaded = load_from_disk(\"data/chapter_5/drug-reviews\")\n",
    "drug_dataset_reloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7a153a",
   "metadata": {},
   "source": [
    "For the **CSV** and **JSON** formats, we have to store each split as a separate file. One way to do this is by iterating over the keys and values in the `DatasetDict` object. This saves each split in JSON Lines format, where each row in the dataset is stored as a single line of JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feec7411",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split, dataset in drug_dataset_clean.items():\n",
    "    dataset.to_json(f\"data/chapter_5/drug-reviews-{split}.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d5062f",
   "metadata": {},
   "source": [
    "And to load the data we can simply use the `load_dataset()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474c446e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {\n",
    "    \"train\": \"data/chapter_5/drug-reviews-train.jsonl\",\n",
    "    \"validation\": \"data/chapter_5/drug-reviews-validation.jsonl\",\n",
    "    \"test\": \"data/chapter_5/drug-reviews-test.jsonl\",\n",
    "}\n",
    "drug_dataset_reloaded = load_dataset(\"json\", data_files=data_files)\n",
    "\n",
    "drug_dataset_clean = drug_dataset_reloaded\n",
    "\n",
    "drug_dataset_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e41097",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "Let's train a classifier that can predict the patient condition based on the drug review.\n",
    "\n",
    "1. Downloading and preparing the data (no need to run the cell below, if `drug_dataset_reloaded` object is still there or simply run the above cell if the right files are there):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8ea7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import html\n",
    "\n",
    "# download the data\n",
    "!cd data/chapter_5/ && curl -O \"https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip\"\n",
    "!cd data/chapter_5/ && unzip -o drugsCom_raw.zip\n",
    "\n",
    "\n",
    "# load the data\n",
    "data_files = {\n",
    "    \"train\" : \"data/chapter_5/drugsComTrain_raw.tsv\",\n",
    "    \"test\" : \"data/chapter_5/drugsComTest_raw.tsv\"\n",
    "}\n",
    "\n",
    "drug_dataset = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files=data_files,\n",
    "    delimiter='\\t'\n",
    ")\n",
    "\n",
    "\n",
    "# filter out the data\n",
    "## rename the column\n",
    "drug_dataset = drug_dataset.rename_column(\n",
    "    original_column_name=\"Unnamed: 0\",\n",
    "    new_column_name=\"patient_id\"\n",
    ")\n",
    "\n",
    "## map condition column values to lowercase\n",
    "def lowercase_condition(data):\n",
    "    return {\"condition\": [row.lower() for row in data[\"condition\"]]}\n",
    "\n",
    "## filter out the null rows\n",
    "drug_dataset = drug_dataset.filter(lambda x: x[\"condition\"] is not None)\n",
    "\n",
    "## map lowercase\n",
    "drug_dataset = drug_dataset.map(lowercase_condition, batched=True)\n",
    "\n",
    "\n",
    "# unescape all the special characters\n",
    "drug_dataset = drug_dataset.map(\n",
    "    lambda x: {\"review\": [html.unescape(review_row) for review_row in x[\"review\"]]},\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "drug_dataset = drug_dataset.map(\n",
    "    lambda x: {\"condition\": [html.unescape(condition_row) for condition_row in x[\"condition\"]]},\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "\n",
    "# Creating validation set\n",
    "drug_dataset_clean = drug_dataset[\"train\"].train_test_split(\n",
    "    train_size=0.8,\n",
    "    seed=41\n",
    ")\n",
    "\n",
    "drug_dataset_clean[\"validation\"] = drug_dataset_clean.pop(\"test\")\n",
    "\n",
    "\n",
    "drug_dataset_clean[\"test\"] = drug_dataset[\"test\"]\n",
    "\n",
    "drug_dataset_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc71309",
   "metadata": {},
   "source": [
    "As we can have quite a lot of data `200k+` in total. Let's just for the sake of making the training process faster, only take `5%` randomly shuffled sample of each split for the training and evaluating the model.\n",
    "> Note if you would like to train the model on the whole data, simply skip the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2b1527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to only take 5% of the data per split \n",
    "pct = 0.05\n",
    "\n",
    "drug_dataset_clean[\"train\"] = drug_dataset_clean[\"train\"].shuffle(seed=42).select(range(int(pct*len(drug_dataset_clean[\"train\"]))))\n",
    "drug_dataset_clean[\"validation\"] = drug_dataset_clean[\"validation\"].shuffle(seed=42).select(range(int(pct*len(drug_dataset_clean[\"validation\"]))))\n",
    "drug_dataset_clean[\"test\"] = drug_dataset_clean[\"test\"].shuffle(seed=42).select(range(int(pct*len(drug_dataset_clean[\"test\"]))))\n",
    "\n",
    "drug_dataset_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd98748",
   "metadata": {},
   "source": [
    "Since there are many values in the `condition` column, let's first have a look at all of them and only select the first 5 conditions that occurs the most as the *labels* for this *classification task* and mark all of the *others* as `other`. We can use `Counter()` method from the `collections` class to get the distribution over the `condition` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2ec450",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# simple case – one label per example\n",
    "train_counts = Counter(drug_dataset_clean[\"train\"][\"condition\"])\n",
    "print(f\"Train dataset condition distribution:\\n\\t{train_counts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a30d58",
   "metadata": {},
   "source": [
    "We can see that, `birth control`, `depression`, `pain`, `anxiety` and `acne`, are the top 5 conidtions that occurs the most in our dataset. \n",
    "In Machine Learning, there is a concept called **stratification**, which states that a model will only perform well when the data distribution between the **train**, **validation** and **test** follows the same pattern, i.e., stratified. So always make sure the distribution stratified when splitting the data or choosing `labels`. Let's also make sure that is the case here, meaning for the `validation` and `test` dataset we would also like to see that  `birth control`, `depression`, `pain`, `anxiety` and `acne` are the top 5 conidtions.\n",
    "\n",
    "> Note: Normally, you first choose the labels, refine the data and then only split the data into train-validation-test split using *stratification* over the refined labels. Here we did the otherway around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ceebb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_counts = Counter(drug_dataset_clean[\"validation\"][\"condition\"])\n",
    "print(f\"Validation dataset condition distribution:\\n\\t{validation_counts}\")\n",
    "\n",
    "test_counts = Counter(drug_dataset_clean[\"test\"][\"condition\"])\n",
    "print(f\"Test dataset condition distribution:\\n\\t{test_counts}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdac516",
   "metadata": {},
   "source": [
    "Now that we know the data will be well stratified when we will only take the first 5 `condition` value for the classification task. Let's create a new columns `labels` where we transfer these values only if the row's `condition` column value corresponds to one of these 5 conditions, otherwise we will put `other` there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d44ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_conditions = ['birth control', 'depression', 'pain', 'anxiety', 'acne']\n",
    "\n",
    "drug_dataset_clean = drug_dataset_clean.map(\n",
    "    lambda x: {\"labels\": [c if c in allowed_conditions else 'other' for c in x['condition']]},\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "drug_dataset_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89aae48",
   "metadata": {},
   "source": [
    "Now let's use the in-built `unique()` method to see how many values are there in the `labels` columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8387492e",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_dataset_clean[\"train\"].unique('labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e3cc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label_counts = Counter(drug_dataset_clean[\"train\"][\"labels\"])\n",
    "print(f\"train dataset labels distribution:\\n\\t{train_label_counts}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d120492",
   "metadata": {},
   "source": [
    "Now, since this task is a *Multi-label classification* task, therefore we need to convert the text values in the `labels` columns, `other`, `birth control`, `depression`, `pain`, `anxiety` and `acne` into discreet numerical values to represent them as **labels** for the model. Luckily, the `DatasetDict` object has `class_encode_column()` function to handle this task for us in-place:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713ecdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the labels to the right form\n",
    "drug_dataset_clean = drug_dataset_clean.class_encode_column(\"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81538623",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(drug_dataset_clean[\"train\"].features[\"labels\"])\n",
    "\n",
    "label_names = drug_dataset_clean[\"train\"].features[\"labels\"].names\n",
    "\n",
    "print(drug_dataset_clean[\"train\"].unique(\"labels\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22be13da",
   "metadata": {},
   "source": [
    "Now, whenever we are dealing with customer reviews, it is a good practice to check the number of words in each review. A review might be just a single word like “Great!” or a full-blown essay with thousands of words, and depending on the use case you’ll need to handle these extremes differently. In our case, some reviews containing just a single word, which, although it may be okay for **sentiment analysis**, would not be informative when preddicting the condition.\n",
    "<br />\n",
    "So, to compute the number of words in each review, we’ll use a rough heuristic based on splitting each text by whitespace and use the `filter()` function to remove reviews that contain fewer than **30 words**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99edf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before review filtering:\")\n",
    "print(drug_dataset_clean)\n",
    "\n",
    "# returns a new column with row's review corresponding length\n",
    "def compute_review_length(data):\n",
    "    return {\"review_length\": [len(row.split()) for row in data[\"review\"]]}\n",
    "\n",
    "# map the review_length column\n",
    "drug_dataset_clean  = drug_dataset_clean.map(\n",
    "    compute_review_length,\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "# filter out rows that has review_length length less than and qual to 30\n",
    "drug_dataset_clean = drug_dataset_clean.filter(\n",
    "    lambda x: x[\"review_length\"] > 30\n",
    ")\n",
    "\n",
    "print(\"After review filtering:\")\n",
    "drug_dataset_clean "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4800da",
   "metadata": {},
   "source": [
    "Now that we have cleaned our data, let's first gather the tokeniser and the model, tokenise the data, and refine it all for once. When initialising the model, we also have to specify the `num_labels=6` arguments because we are training the model for a multi-class classification task and there are `6` labels in total:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff02b90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding, AutoModelForSequenceClassification\n",
    "from torch.utils.data import DataLoader\n",
    "from pprint import pprint\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokeniser = AutoTokenizer.from_pretrained(checkpoint)\n",
    "# initialise the model and also specify the number of labels\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=6)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokeniser)\n",
    "\n",
    "def tokenisation_function(data):\n",
    "    return tokeniser(data['review'], truncation=True)\n",
    "\n",
    "tokenised_datasets = drug_dataset_clean.map(\n",
    "    tokenisation_function,\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "\n",
    "pprint(tokenised_datasets)\n",
    "\n",
    "\n",
    "tokenised_datasets = tokenised_datasets.remove_columns(\n",
    "    column_names=['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length']\n",
    ")\n",
    "\n",
    "tokenised_datasets.set_format(\"torch\")\n",
    "\n",
    "pprint(tokenised_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d3ff5d",
   "metadata": {},
   "source": [
    "seperate and divide the data into batched using dataloader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cece146c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 32\n",
    "eval_batch_size = min(128, len(tokenised_datasets[\"validation\"]))\n",
    "test_batch_size = min(128, len(tokenised_datasets[\"test\"]))\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=tokenised_datasets[\"train\"],\n",
    "    batch_size=train_batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    dataset=tokenised_datasets[\"validation\"],\n",
    "    batch_size=eval_batch_size,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=tokenised_datasets[\"test\"],\n",
    "    batch_size=test_batch_size,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"So there are, {len(train_dataloader)} batches of size {train_batch_size} in the training dataset,\\n\\t{len(eval_dataloader)} batches of size {eval_batch_size} in the evaluation dataset, and\\n\\t {len(test_dataloader)} batches of size {test_batch_size} in the test dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c07159",
   "metadata": {},
   "source": [
    "initialise the *accelerator*, *optimisor* and *learning rate scheduler* object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fdb301",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "\n",
    "optimiser = AdamW(\n",
    "    params=model.parameters(),\n",
    "    lr=2e-5\n",
    ")\n",
    "\n",
    "accelerator = Accelerator()\n",
    "train_dl, eval_dl, test_dl, model, optimiser = accelerator.prepare(\n",
    "    train_dataloader,\n",
    "    eval_dataloader,\n",
    "    test_dataloader,\n",
    "    model,\n",
    "    optimiser\n",
    ")\n",
    "\n",
    "num_epochs = 5\n",
    "num_training_steps = num_epochs * len(train_dl)\n",
    "\n",
    "num_warmup_steps = .01 * num_training_steps\n",
    "\n",
    "lr_schedular = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimiser,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "print(f\"Total training steps {num_training_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6e5e1a",
   "metadata": {},
   "source": [
    "Now this time since there is no pre-evaluation metric present, therefore we have to define which metrics to use when evaluating our model.\n",
    "<br />\n",
    "For a *Classification* task,  the best metrics to evalute a model are **Accuracy**, **Precision**, **Recall** and **F1 Score**. The latter three metrics are detrived from a **Confusion Matrix**, which is basically a `N X N matrix`, where `N` is the *number of classes or categories* that are to be predicted. The values inside the *confusion matrix* represents one of these 4 values:\n",
    "+ **True Positives (TP)** : It is the case where we predicted Yes and the real output was also Yes.\n",
    "+ **True Negatives (TN)**: It is the case where we predicted No and the real output was also No.\n",
    "+ **False Positives (FP)**: It is the case where we predicted Yes but it was actually No.\n",
    "+ **False Negatives (FN)**: It is the case where we predicted No but it was actually Yes. \n",
    "\n",
    "For example, suppose there is a problem which is a binary classification with labels as `Yes` or `No`. So, here `N = 2`, therfore we will get a `2 X 2` *confusion matrix*. Now let's say we tested our model with 165 samples and the results using *confusion matrix* looks like this:\n",
    "\n",
    "|              |Predicted No|Predited Yes|\n",
    "|--------------|------------|------------|\n",
    "|**Actual No** |50|10|\n",
    "|**Actual Yes**|5|100|\n",
    "\n",
    "Therefore, out of the 165 predictions, `100` predictions were **TP** (bottom right), `50` were **TN** (top left), `10` were **FP** (top right), and `5` were *FN* (bottom left).\n",
    "\n",
    "\n",
    "Now, how these values are useful because we can use them to calculate **Precision**, **Recall** and **F1 Score**:\n",
    "+ **Precision**: It measures how many of the positive predictions made by the model are actually correct. It's useful when the cost of false positives is high such as in medical diagnoses where predicting a disease when it’s not present can have serious consequences. Therefore, *Precision* helps ensure that when the model predicts a positive outcome, it’s likely to be correct.\n",
    "$$\n",
    "\\text{Precision} = \\frac{TP}{TP+FP}\n",
    "$$\n",
    "+ **Recall**: *Recall* or *Sensitivity measures* how many of the actual positive cases were correctly identified by the model. It is important when missing a positive case (*false negative*) is more costly than false positives (like disease detection).\n",
    "$$\n",
    "\\text{Recall} = \\frac{TP}{TP+FN}\n",
    "$$\n",
    "+ **F1 Score**: The *F1 Score* is the *harmonic mean* of *precision* and *recall*. It is useful when we need a balance between *precision* and *recall*, as it combines both into a single number. A *high F1 score* means the model performs well on both metrics, i.e., the model is performing well. Its range is `[0,1]`:\n",
    "$$\n",
    "\\text{F1 Score}=2\\times\\frac{Precision+Recall}{Precision×Recall} \n",
    "$$\n",
    "Now, when you have multiple classes, you still often want a single precision/recall/F1 number—but how you combine per-class scores depends on whether you care more about rare classes, common classes, or every example equally. Here’s what each averaging strategy does:\n",
    "\n",
    "+ **Weighted**: Compute each class’s score, then average them but weight by how many true examples each class has - so common labels count more.\n",
    "\n",
    "+ **Micro**: Pool all true/false positives and negatives across every example, then compute one overall score - every prediction is equal (large classes dominate).\n",
    "\n",
    "+ **Macro**: Compute each class’s score and then take the simple average—every class counts the same, no matter how many examples it has.\n",
    "\n",
    "> NOTE: **Lower recall** and **higher precision** gives us **great accuracy** but then it misses a large number of instances and that's why **accuracy** alone is not a good metric when evaluating a model and using **Recall**, **Precision** and **F1 score** if possible is a good practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2fde33",
   "metadata": {},
   "source": [
    "Luckily, the `evaluate` lib provides `combine()` method, where you can specify which metrics to use for the evaluation, and also when calling the `compute()` we can pass the `average` argument to specify which averaging strategy to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7f3142",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import combine\n",
    "import torch\n",
    "from livelossplot import PlotLosses\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def perform_evaluation():\n",
    "    \"\"\"\n",
    "    Perform evaluation on the validation set\n",
    "    \"\"\"\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    eval_epoch_loss = []\n",
    "\n",
    "    # initialising evaluation\n",
    "    eval_metric = combine(\n",
    "        evaluations=[\n",
    "            \"accuracy\",\n",
    "            \"precision\",\n",
    "            \"recall\",\n",
    "            \"f1\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    for batch in eval_dl:\n",
    "        # Disable gradient computation for evaluation (saves memory and computation)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "            # Store loss inside no_grad for memory efficiency\n",
    "            eval_epoch_loss.append(outputs.loss.item())\n",
    "\n",
    "            # Get predictions for metrics (logits already created without gradients)\n",
    "            logits = outputs.logits\n",
    "            refs = batch[\"labels\"]\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            # Add batch to evaluation metric\n",
    "            eval_metric.add_batch(\n",
    "                predictions=accelerator.gather(preds),\n",
    "                references=accelerator.gather(refs)\n",
    "            )\n",
    "    \n",
    "    eval_avg_loss = sum(eval_epoch_loss) / len(eval_epoch_loss)\n",
    "    eval_pred_stats = eval_metric.compute(\n",
    "        kwargs={\n",
    "            \"average\":\"weighted\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return eval_avg_loss, eval_pred_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a513ea4",
   "metadata": {},
   "source": [
    "For after we will have the model trained we would like to evaluate its performance on the test data because testing on untouched data gives a true measure of how our model will perform on new examples and prevents us from overfitting by tuning to the same data we used to train it. \n",
    "<br />\n",
    "So, let's write the evaluation function on the test data, and this time we can also ask for the *confusion_matrix* from the `evalute.compute()` function along with other metrics to further evalute the model on the test data:\n",
    "\n",
    "> Note: when evaluating the model on the test data we don't need to look at the loss value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bc7d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_evaluation():\n",
    "    model.eval()\n",
    "\n",
    "    test_metric = combine(\n",
    "        evaluations=[\n",
    "            \"accuracy\",\n",
    "            \"precision\",\n",
    "            \"recall\",\n",
    "            \"f1\",\n",
    "            \"confusion_metrix\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    for batch in test_dl:\n",
    "        with torch.no_grsd():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "            logits = outputs.logits\n",
    "            refs = batch[\"labels\"]\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            test_metric.add_batch(\n",
    "                predictions=accelerator.gather(preds),\n",
    "                references=accelerator.gather(refs)\n",
    "            )\n",
    "    \n",
    "    return test_metric.compute(\n",
    "        kwargs={\n",
    "            \"average\":\"weighted\",\n",
    "            \"labels\": list(range(len(label_names)))\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1595a5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_cm(cm):\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
    "    plt.xticks(range(len(label_names)), label_names, rotation=45)\n",
    "    plt.yticks(range(len(label_names)), label_names)\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e438a56b",
   "metadata": {},
   "source": [
    "the main training function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b905c85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "def training_function():\n",
    "    # initialise the plotter for the learning curve\n",
    "    plotter = PlotLosses(mode='notebook')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # ensure model is in training mode\n",
    "        model.train()\n",
    "        train_epoch_loss = []\n",
    "\n",
    "        # metrics for training data\n",
    "        train_metric = combine(\n",
    "            evaluations=[\n",
    "                \"accuracy\",\n",
    "                \"precision\",\n",
    "                \"recall\",\n",
    "                \"f1\"\n",
    "            ]   \n",
    "        )\n",
    "\n",
    "        for batch in train_dl:\n",
    "            # Forward Pass (keep gradient attached)\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            # Backward Pass (while gradients are still attached)\n",
    "            accelerator.backward(loss)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimiser.step()\n",
    "            lr_schedular.step()\n",
    "            optimiser.zero_grad()\n",
    "\n",
    "            # metric computation\n",
    "            with torch.no_grad():\n",
    "                # detach loss for metric computation \n",
    "                train_epoch_loss.append(loss.detach().item())\n",
    "\n",
    "                # detach logits for metric computation\n",
    "                logits = outputs.logits.detach()\n",
    "                # no need to detach labels (they don't have gradients)\n",
    "                refs = batch['labels']\n",
    "                preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "                # add batch to the train matric\n",
    "                train_metric.add_batch(\n",
    "                    predictions=accelerator.gather(preds),\n",
    "                    references=accelerator.gather(refs)\n",
    "                )\n",
    "            \n",
    "            progress_bar.update(1)\n",
    "\n",
    "        # compute training metrics\n",
    "        tain_avg_loss = sum(train_epoch_loss)/len(train_epoch_loss)\n",
    "        train_pred_stats = train_metric.compute(\n",
    "            kwargs={\n",
    "                \"average\":\"weighted\"\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # evaluation phase\n",
    "        eval_avg_loss, eval_pred_stats = perform_evaluation()\n",
    "\n",
    "        plotter.update({\n",
    "            'loss': tain_avg_loss,\n",
    "            'val_loss': eval_avg_loss,\n",
    "            'acc': train_pred_stats['accuracy'],\n",
    "            'val_acc': eval_pred_stats['accuracy'],\n",
    "            'precision': train_pred_stats['precision'],\n",
    "            'val_precision': eval_pred_stats['precision'],\n",
    "            'recall': train_pred_stats['recall'],\n",
    "            'val_recall': eval_pred_stats['recall'],\n",
    "            'f1': train_pred_stats['f1'],\n",
    "            'val_f1': eval_pred_stats['f1'],\n",
    "        })\n",
    "        plotter.send() \n",
    "\n",
    "    # perform evaluation on the test dataset\n",
    "    test_pred_stats = test_evaluation()\n",
    "    plotter.update({\n",
    "        'test_acc': test_pred_stats['accuracy'],\n",
    "        'test_precision': test_pred_stats['precision'],\n",
    "        'test_recall': test_pred_stats['recall'],\n",
    "        'test_f1': test_pred_stats['f1'],\n",
    "    })\n",
    "    plotter.send()\n",
    "    # plot the confusion matrix on the test data\n",
    "    plot_cm(test_pred_stats[\"confusion_matrix\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc212c85",
   "metadata": {},
   "source": [
    "Let's launch the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21805635",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import notebook_launcher\n",
    "\n",
    "notebook_launcher(training_function, num_processes=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
