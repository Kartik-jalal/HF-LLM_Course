{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccfc270b",
   "metadata": {},
   "source": [
    "# Handling Local Data\n",
    "To load datasets that are stored either on your laptop or on a remote server, we can still use the `load_dataset()` function. This time, we just need to specify the type of loading script in the `load_dataset()` function, along with a `data_files=''` argument that specifies the path to one or more files.\n",
    "\n",
    "![\"load_dataset()\"](data/chapter_5/load_dataset.png \"load_dataset()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99872279",
   "metadata": {},
   "source": [
    "### Loading a local dataset\n",
    "\n",
    "| Data format | Loading script | Example |\n",
    "|-------------|----------------|---------|\n",
    "| CSV & TSV |`csv`|`load_dataset(\"csv\", data_files=\"my_file.csv\")`|\n",
    "| Text files |`text`|`load_dataset(\"text\", data_files=\"my_file.txt\")`|\n",
    "| JSON & JSON Lines |`json`|`load_dataset(\"json\", data_files=\"my_file.json\")`|\n",
    "| Pickled DataFrames |`pandas`|`load_dataset(\"pandas\", data_files=\"my_dataframe.pkl\")`|\n",
    "\n",
    "For this example, let's use the [SQuAD-it](https://github.com/crux82/squad-it/) dataset, which is a large-scale **json** dataset for question answering in Italian. It's hosted on GitHub, let's first download it in our `data/chapter_5` dir using `wget` and then decompress these compressed files `SQuAD_it-train.json.gz`, `SQuAD_it-test.json.gz` using `gzip`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef67e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd data/chapter_5 && wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-train.json.gz\n",
    "!cd data/chapter_5 && wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-test.json.gz\n",
    "\n",
    "!cd data/chapter_5 && gzip -dkv SQuAD_it-*.json.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfbe4aa",
   "metadata": {},
   "source": [
    "Now that we have our data in the `JSON` format, we can simply use the `load_dataset()` function, we just need to know if we’re dealing with **ordinary JSON** (*similar to a nested dictionary*) or **JSON Lines** (*line-separated JSON*). Like many question answering datasets, **SQuAD-it** uses the *nested format*, with all the text stored in a **data field**. This means we can load the dataset by specifying the `field='data'` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102036ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "squad_it_dataset = load_dataset(\"json\", data_files=\"data/chapter_5/SQuAD_it-train.json\", field=\"data\")\n",
    "\n",
    "squad_it_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c461d993",
   "metadata": {},
   "source": [
    "As we can see, by default, loading local files creates a `DatasetDict` object with only a **train** split. But, what we really want is to include both the **train** and **test** splits in a single `DatasetDict` object so we can apply `Dataset.map()` functions across both splits at once. To do this, we can provide a dictionary to the \n",
    "```python\n",
    "data_files={\"train\":\"path to the training data\", \"test\":\"path to the testing data\"}\n",
    "```\n",
    "argument that maps each split name to a file associated with that split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc636f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {\n",
    "    \"train\":\"data/chapter_5/SQuAD_it-train.json\",\n",
    "    \"test\":\"data/chapter_5/SQuAD_it-test.json\"\n",
    "}\n",
    "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")\n",
    "squad_it_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb86667",
   "metadata": {},
   "source": [
    "The loading scripts in Datasets actually support automatic decompression of the input files, so we could have skipped the use of gzip by pointing the `data_files` argument directly to the compressed files:\n",
    "```python\n",
    "data_files = {\n",
    "    \"train\": \"data/chapter_5/SQuAD_it-train.json.gz\", \n",
    "    \"test\": \"data/chapter_5/SQuAD_it-test.json.gz\"\n",
    "}\n",
    "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")\n",
    "```\n",
    "This can be useful if you don’t want to manually decompress many `GZIP` files. The automatic decompression also applies to other common formats like `ZIP` and `TAR`, so you just need to point `data_files` to the compressed files.\n",
    "\n",
    "> The `data_files` argument is also quite flexible and can be either *a single file path*, *a list of file paths*, or *a dictionary* that maps split names to file paths. You can also *glob files* that match a *specified pattern* according to the rules used by the `Unix shell` (e.g., you can glob all the `JSON` files in a directory as a single split by setting `data_files=\"*.json\"`). See the [Datasets documentation](https://huggingface.co/docs/datasets/loading#local-and-remote-files) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a5facc",
   "metadata": {},
   "source": [
    "### Loading a remote dataset\n",
    "\n",
    "Fortunately, loading *remote files* is just as simple as loading *local* ones!\n",
    "<br />\n",
    "Instead of providing a path to *local files*, we point the `data_files` argument to **one or more URLs** where the *remote files* are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf78445",
   "metadata": {},
   "outputs": [],
   "source": [
    "url =  \"https://github.com/crux82/squad-it/raw/master/\"\n",
    "\n",
    "data_files = {\n",
    "    \"train\": url + \"SQuAD_it-train.json.gz\",\n",
    "    \"test\": url + \"SQuAD_it-test.json.gz\",\n",
    "}\n",
    "\n",
    "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")\n",
    "squad_it_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7a6b66",
   "metadata": {},
   "source": [
    "# Data Manipulation\n",
    "\n",
    "The `DatasetDict` object comes with a lot of functionalities to manipulate the original dataset.\n",
    "<br />\n",
    "For this example, we’ll use the [Drug Review Dataset](https://archive.ics.uci.edu/ml/datasets/Drug+Review+Dataset+%28Drugs.com%29) that’s hosted on the [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php), which contains patient reviews on various drugs, along with the condition being treated and a 10-star rating of the patient’s satisfaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d60332",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd data/chapter_5/ && wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip\"\n",
    "!cd data/chapter_5/ && unzip drugsCom_raw.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bef2b6b",
   "metadata": {},
   "source": [
    "As we can see, this the data is in the `TSV` format which is a variant of `CSV` that uses tabs instead of commas as the separator. So, when loading these files using `load_dataset()`, we use the specify `csv` as the *loading script* and most importantly the `delimiter=\\t` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a62b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_files = {\n",
    "    \"train\" : \"data/chapter_5/drugsComTrain_raw.tsv\",\n",
    "    \"test\" : \"data/chapter_5/drugsComTest_raw.tsv\"\n",
    "}\n",
    "\n",
    "drug_dataset = load_dataset(\"csv\", data_files=data_files, delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901948e7",
   "metadata": {},
   "source": [
    "Now that we have the `DatasetDict` object, we can create a random sample to get a quick feel for the type of data you’re working with and to do so we simply have to chain the `Dataset.shuffle()` and `Dataset.select()` function to first randomly shuffle the data  (we can also pass the `seed` argument to later use the same shuffle) and select/see the first *n* data elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd508b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_sample = drug_dataset[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "\n",
    "drug_sample[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0004751",
   "metadata": {},
   "source": [
    "From above we can see before passing this data to the model or even for tokenisation we need to perform few pre-processing steps:\n",
    "  + The `Unnamed: 0` column needs to be renamed to `patient_id`.\n",
    "  + The `condition` column includes a mix of *uppercase* and *lowercase* labels.\n",
    "  + The `reviews` are of varying length and contain a mix of Python line separators `(\\r\\n)` as well as HTML character codes like `&\\#039;`.\n",
    "\n",
    "So, we can use the in-built functions like the, `rename_column()` - to rename the column name, `map()` and `filter()` - to map all the `condition` column values to lowercase, and also filter out the special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7b5c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "\n",
    "# rename the column name\n",
    "drug_dataset = drug_dataset.rename_column(\n",
    "    original_column_name=\"Unnamed: 0\",\n",
    "    new_column_name=\"patient_id\"\n",
    ")\n",
    "\n",
    "# map conditon column values to lowercase\n",
    "def lowercase_condition(data):\n",
    "    return {\"condition\": [row.lower() for row in data[\"condition\"]]}\n",
    "    # return {\"condition\": data[\"condition\"].lower()} # if not using batched=True in the map() function\n",
    "    \n",
    "\n",
    "# let's first remove all the rows with null values, otherwise the above\n",
    "# function will throw an error\n",
    "drug_dataset = drug_dataset.filter(lambda x: x[\"condition\"] is not None)\n",
    "\n",
    "# map lowercasse\n",
    "drug_dataset = drug_dataset.map(lowercase_condition, batched=True)\n",
    "\n",
    "\n",
    "# unescape all the HTML special characters in our corpus\n",
    "drug_dataset =  drug_dataset.map(\n",
    "    lambda x: {\"review\": [html.unescape(row) for row in x[\"review\"]]},\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "\n",
    "drug_dataset[\"train\"][:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a75b07",
   "metadata": {},
   "source": [
    ">In Python, `lambda` functions are small functions that you can define without explicitly naming them. They take the general form `lambda <arguments> : <expression>`,\n",
    "where `lambda` is one of Python’s special keywords, `<arguments>` is a list/set of *comma-separated values* that define the *inputs* to the function, and `<expression>` represents the operations you wish to execute. For example, we can define a simple lambda function that squares a number as follows: `lambda x : x * x`\n",
    "To apply this function to an input, we need to wrap it and the input in parentheses:\n",
    "`(lambda x: x * x)(3) -> 9`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d1d981",
   "metadata": {},
   "source": [
    "### From Datasets to DataFrames and back\n",
    "\n",
    "We can use the the `set_format()` function of the `DatasetDict` object to convert it into a different dataframe such as *Pandas*, *NumPy*, *PyTorch*, *TensorFlow*, and *JAX*. To convert it back to the `DatasetDict` object, we simply need to call the `reset_format()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a106953",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_dataset.set_format(\"pandas\")\n",
    "\n",
    "drug_dataset[\"train\"][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce5b9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_dataset.reset_format()\n",
    "\n",
    "drug_dataset[\"train\"][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cd2206",
   "metadata": {},
   "source": [
    "### Creating a validation set\n",
    "The `DatasetDict` object also provides a `Dataset.train_test_split()` function that is based on the famous functionality from `scikit-learn` which can be used to further split the data into a train-validation-test format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26ca1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80-20 percent train-validation split on the training dataset\n",
    "drug_dataset_clean = drug_dataset[\"train\"].train_test_split(train_size=0.8, seed=41)\n",
    "\n",
    "# name the 20% split data as the validation\n",
    "drug_dataset_clean[\"validation\"] = drug_dataset_clean.pop(\"test\")\n",
    "\n",
    "# Add the orignal test dataset\n",
    "drug_dataset_clean[\"test\"] = drug_dataset[\"test\"]\n",
    "\n",
    "drug_dataset_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5393b117",
   "metadata": {},
   "source": [
    "### Saving a dataset\n",
    "To save a dataset to disk:\n",
    "\n",
    "| Data format | Function |\n",
    "|-------------|----------|\n",
    "|*Arrow*|`Dataset.save_to_disk()`|\n",
    "|*CSV*|`Dataset.to_csv()`|\n",
    "|*JSON*|`Dataset.to_json()`|\n",
    "\n",
    "For example, let’s save our cleaned dataset in the Arrow format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fa6c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_dataset_clean.save_to_disk(\"data/chapter_5/drug-reviews\")\n",
    "\n",
    "!ls data/chapter_5/drug-reviews/*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dba53d",
   "metadata": {},
   "source": [
    "Once the dataset is saved, we can load it by using the load_from_disk() function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16a19ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "drug_dataset_reloaded = load_from_disk(\"data/chapter_5/drug-reviews\")\n",
    "drug_dataset_reloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7a153a",
   "metadata": {},
   "source": [
    "For the **CSV** and **JSON** formats, we have to store each split as a separate file. One way to do this is by iterating over the keys and values in the `DatasetDict` object. This saves each split in JSON Lines format, where each row in the dataset is stored as a single line of JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feec7411",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split, dataset in drug_dataset_clean.items():\n",
    "    dataset.to_json(f\"data/chapter_5/drug-reviews-{split}.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d5062f",
   "metadata": {},
   "source": [
    "And to load the data we can simply use the `load_dataset()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474c446e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {\n",
    "    \"train\": \"data/chapter_5/drug-reviews-train.jsonl\",\n",
    "    \"validation\": \"data/chapter_5/drug-reviews-validation.jsonl\",\n",
    "    \"test\": \"data/chapter_5/drug-reviews-test.jsonl\",\n",
    "}\n",
    "drug_dataset_reloaded = load_dataset(\"json\", data_files=data_files)\n",
    "\n",
    "drug_dataset_clean = drug_dataset_reloaded\n",
    "\n",
    "drug_dataset_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e41097",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "Let's train a classifier that can predict the patient condition based on the drug review.\n",
    "### 1. Download the data.\n",
    "We are re-downloaing the data because we want to clean it more deeply this time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8ea7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# download the data\n",
    "!cd data/chapter_5/ && curl -O \"https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip\"\n",
    "!cd data/chapter_5/ && unzip -o drugsCom_raw.zip\n",
    "\n",
    "\n",
    "# load the data\n",
    "data_files = {\n",
    "    \"train\" : \"data/chapter_5/drugsComTrain_raw.tsv\",\n",
    "    \"test\" : \"data/chapter_5/drugsComTest_raw.tsv\"\n",
    "}\n",
    "\n",
    "drug_dataset = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files=data_files,\n",
    "    delimiter='\\t'\n",
    ")\n",
    "\n",
    "drug_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3887f889",
   "metadata": {},
   "source": [
    "### 2. Merge the split togther"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11de2255",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "# pop the testing data out of the drug dataset\n",
    "testing_data = drug_dataset.pop(\"test\")\n",
    "# merge the splits\n",
    "drug_dataset[\"train\"] = concatenate_datasets([drug_dataset[\"train\"], testing_data])\n",
    "\n",
    "drug_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6535b045",
   "metadata": {},
   "source": [
    "### 3. Intial data filteration phase, where we are:\n",
    "\n",
    "+ Changing the column name from `Unnamed: 0` to `patient_id`\n",
    "+ Removing the rows that does not having anything in thier `condition` column.\n",
    "+ Setting all the values inside the `condition` column to *lowecase*.\n",
    "+ Converting the html characters in the `condition` column into readable format, i.e., `unescape`.\n",
    "+ Remove the rows with `review` column length less than a certain number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07db4547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "\n",
    "# rename the column\n",
    "drug_dataset = drug_dataset.rename_column(\n",
    "    original_column_name=\"Unnamed: 0\",\n",
    "    new_column_name=\"patient_id\"\n",
    ")\n",
    "\n",
    "\n",
    "# removing out the empty condition rows\n",
    "drug_dataset = drug_dataset.filter(\n",
    "    lambda batch: [condition is not None for condition in batch[\"condition\"]],\n",
    "    batched=True,\n",
    "    desc=\"Removing empty Condition rows\"\n",
    ")\n",
    "\n",
    "\n",
    "## lowercase function\n",
    "def lowercase_condition(data):\n",
    "    return {\"condition\": [row.lower() for row in data[\"condition\"]]}\n",
    "\n",
    "## map lowercase\n",
    "drug_dataset = drug_dataset.map(\n",
    "    lowercase_condition,\n",
    "    batched=True,\n",
    "    desc=\"Mapping Condition values to lowercase\"\n",
    ")\n",
    "\n",
    "\n",
    "# unescape all the special characters\n",
    "drug_dataset = drug_dataset.map(\n",
    "    lambda x: {\"review\": [html.unescape(review_row) for review_row in x[\"review\"]]},\n",
    "    batched=True,\n",
    "    desc=\"Mapping HTML Unescape over Condition values\"\n",
    ")\n",
    "\n",
    "drug_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cafbe9d",
   "metadata": {},
   "source": [
    "Now, whenever we are dealing with *customer reviews*, it is a good practice to check the *number of words* in each *review*. A *review* might be just a *single word* like *“Great!”* or a *full-blown essay with thousands of words*, and depending on the use case you’ll need to handle these extremes differently.\n",
    "<br />\n",
    "In our case, some *reviews* containing just a single word, which, although it may be okay for **sentiment analysis**, would not be informative when predicting a *condition*. So, to compute the number of words in each review, we’ll use a rough heuristic based on splitting each text by whitespace and use the `filter()` function to remove reviews that contain fewer than **30 words**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a42250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a new column with row's review corresponding length\n",
    "def compute_review_length(data):\n",
    "    return {\"review_length\": [len(row.split()) for row in data[\"review\"]]}\n",
    "\n",
    "# map the review_length column\n",
    "drug_dataset  = drug_dataset.map(\n",
    "    compute_review_length,\n",
    "    batched=True,\n",
    "    desc=\"Mapping review_length column\"\n",
    ")\n",
    "\n",
    "# filter out rows that has review_length length less than and qual to 30\n",
    "drug_dataset = drug_dataset.filter(\n",
    "    lambda batch: [review_length >= 30 for review_length in batch[\"review_length\"]],\n",
    "    batched=True,\n",
    "    desc=\"Removing rows with review length less than 30\"\n",
    ")\n",
    "\n",
    "drug_dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd98748",
   "metadata": {},
   "source": [
    "### 4. Setting up the `labels` column.\n",
    "\n",
    "Let's first see how macny unique *conditions* there are in the dataset, using the in-built `unique()` function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754a9ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {len(drug_dataset.unique(\"condition\")[\"train\"])} unique conditions in the dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f82939",
   "metadata": {},
   "source": [
    "As we can see, there are `853` unique conditions in the `condition` column. Let's have a look at thier distribution and only select the first 5 conditions that occurs the most as the *labels* for this *multi-class classification task* and remove all of the others.\n",
    "<br />\n",
    "We can use the `Counter()` method from the `collections` class to get the distribution over the `condition` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2ec450",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "train_counts = Counter(drug_dataset[\"train\"][\"condition\"])\n",
    "\n",
    "print(f\"Conditions distribution:\\n\\t{train_counts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a30d58",
   "metadata": {},
   "source": [
    "We can see that, `birth control`, `depression`, `acne`, `anxiety`, and `pain` are the top 5 conidtions that occurs the most in our dataset. So let's now, filter out all the rows where is *condition* is not that and then, rename the `condition` column name to `labels` (because that is something that will be required by our model):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d44ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_conditions = ['birth control', 'depression', 'pain', 'anxiety', 'acne']\n",
    "\n",
    "drug_dataset = drug_dataset.filter(\n",
    "    lambda batch: [condition in allowed_conditions for condition in batch[\"condition\"]],\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "drug_dataset = drug_dataset.rename_column(\n",
    "    original_column_name=\"condition\",\n",
    "    new_column_name=\"labels\"\n",
    ")\n",
    "\n",
    "conditions_label = drug_dataset.unique(\"labels\")[\"train\"]\n",
    "print(f\"Now there are only {len(conditions_label)} conditions label, which are:\\n\\t{conditions_label}\")\n",
    "\n",
    "drug_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa567d41",
   "metadata": {},
   "source": [
    "### 5. Encoding the labels into ClassLabels\n",
    "Now, since this task is a *Multi-label classification* task, therefore we need to convert the text values in the `labels` columns, `birth control`, `depression`, `pain`, `anxiety` and `acne` into discreet numerical values i.e., `ClassLabels`, to represent them as **labels** for the model. Luckily, the `DatasetDict` object has `class_encode_column()` function to handle this task for us in-place:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cbd3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the labels to the right form\n",
    "drug_dataset = drug_dataset.class_encode_column(\"labels\")\n",
    "\n",
    "print(drug_dataset[\"train\"].features[\"labels\"])\n",
    "\n",
    "label_features = drug_dataset[\"train\"].features[\"labels\"]\n",
    "label_names = label_features.names\n",
    "\n",
    "for label in label_names:\n",
    "    print(f\"{label} -> {label_features.str2int(label)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00b6fc8",
   "metadata": {},
   "source": [
    "### 6. Splitting the Dataset\n",
    "Now, that we have `67844` *ClassLabels encoded* data in total, let's split the dataset into `train`, `validation`, `test` with a 70-20-10 percentage ratio, respectively. However, we need to follow a rulewhen splitting:\n",
    "<br />\n",
    "In Machine Learning, **stratification** refers to the practice of ensuring that the distribution of labels is consistent across the `train`, `validation`, and `test` datasets. This means that if the *training* dataset contains `60%` of label `x` and `40%` of label `y` (e.g., `6` rows of `x` and `4` rows of `y` out of `10` total), then the *validation* and *test* sets should also maintain the same proportions - `60% x` and `40% y`, respectively.\n",
    "\n",
    "\n",
    "For splitting the data we will use the in-built `train_test_split()` method, where we can also specify using the `stratify_by_column=\"labels\"` argument, to stratify the splits based on the `labels` column `ClassLabels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6905c0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "# Split off 70% train, 30% temporary (for validation + test)\n",
    "train_valtest = drug_dataset[\"train\"].train_test_split(\n",
    "    test_size=0.3,\n",
    "    seed=41,\n",
    "    stratify_by_column=\"labels\"\n",
    ")\n",
    "\n",
    "# Split 30% temporary into 20% validation and 10% test\n",
    "val_test = train_valtest[\"test\"].train_test_split(\n",
    "    test_size=1/3,  # 1/3 of 30% = 10%\n",
    "    seed=41,\n",
    "    stratify_by_column=\"labels\"\n",
    ")\n",
    "\n",
    "\n",
    "# Recombine into a final DatasetDict\n",
    "drug_dataset_final = datasets.DatasetDict(\n",
    "    {\n",
    "        \"train\" : train_valtest[\"train\"],\n",
    "        \"validation\" : val_test[\"train\"],\n",
    "        \"test\" : val_test[\"test\"]\n",
    "    }\n",
    ")\n",
    "drug_dataset_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc71309",
   "metadata": {},
   "source": [
    "As we have quite a lot of data, `65k+` in total. Let's just for the sake of making the training process faster, only take `10%` randomly shuffled sample of each split for the training and evaluating the model.\n",
    "> Note if you would like to train the model on the whole data, simply skip the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2b1527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to only take 10% of the data per split \n",
    "pct = 0.1\n",
    "\n",
    "drug_dataset_final[\"train\"] = drug_dataset_final[\"train\"].shuffle(seed=42).select(range(int(pct*len(drug_dataset_final[\"train\"]))))\n",
    "drug_dataset_final[\"validation\"] = drug_dataset_final[\"validation\"].shuffle(seed=42).select(range(int(pct*len(drug_dataset_final[\"validation\"]))))\n",
    "drug_dataset_final[\"test\"] = drug_dataset_final[\"test\"].shuffle(seed=42).select(range(int(pct*len(drug_dataset_final[\"test\"]))))\n",
    "\n",
    "drug_dataset_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73e251c",
   "metadata": {},
   "source": [
    "Let's look if the ClassLabels distribution is correct amongst the splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ceebb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_distributions(split, dataset):\n",
    "    print(f\"{split} ClassLabels distribution:\")\n",
    "\n",
    "    # get the distribution numbers\n",
    "    class_labels_counts = Counter(dataset[split][\"labels\"])\n",
    "\n",
    "    # distribution dictionary\n",
    "    dist = {}\n",
    "    # for every label \n",
    "    for label_id, count in class_labels_counts.items():\n",
    "        # get the label name using the label id\n",
    "        label_name = dataset[split].features[\"labels\"].int2str(label_id)\n",
    "        # compute the percentage\n",
    "        pct = round((count/len(drug_dataset_final[split]))*100, 2)\n",
    "\n",
    "        # add it to the distribution dict\n",
    "        dist[label_id] = (label_name, pct)\n",
    "    \n",
    "    # sort the distribution dict on label id and print the data\n",
    "    sorted_dist = dict(sorted(dist.items()))\n",
    "    for label_id, name_pct in sorted_dist.items():\n",
    "        print(f\"\\t id - {label_id} {name_pct[0]} : {name_pct[1]}% \")\n",
    "\n",
    "\n",
    "print_distributions(\"train\", drug_dataset_final)\n",
    "\n",
    "print_distributions(\"validation\", drug_dataset_final)\n",
    "\n",
    "print_distributions(\"test\", drug_dataset_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4800da",
   "metadata": {},
   "source": [
    "### 7. Initialise the Model and other config\n",
    "\n",
    "Now that we have our final dataset, let's now:\n",
    "+ gather the tokeniser and the model, \n",
    "+ tokenise the data and refine it all for once. \n",
    "\n",
    "> Note: When initialising the model, we also have to specify the `num_labels=5` arguments because we are training the model for a multi-class classification task and there are `5` labels in total:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff02b90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding, AutoModelForSequenceClassification\n",
    "from torch.utils.data import DataLoader\n",
    "from pprint import pprint\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokeniser = AutoTokenizer.from_pretrained(checkpoint)\n",
    "# initialise the model and also specify the number of labels\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=6)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokeniser)\n",
    "\n",
    "def tokenisation_function(data):\n",
    "    return tokeniser(data['review'], truncation=True)\n",
    "\n",
    "tokenised_datasets = drug_dataset_final.map(\n",
    "    tokenisation_function,\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "\n",
    "pprint(tokenised_datasets)\n",
    "\n",
    "\n",
    "tokenised_datasets = tokenised_datasets.remove_columns(\n",
    "    column_names=['patient_id', 'drugName', 'review', 'rating', 'date', 'usefulCount', 'review_length']\n",
    ")\n",
    "\n",
    "tokenised_datasets.set_format(\"torch\")\n",
    "\n",
    "pprint(tokenised_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d3ff5d",
   "metadata": {},
   "source": [
    "### 8. Set up the dataloader\n",
    "\n",
    "> Note: if you didn't take the 10% sample of the data at `step 6` and in total there are still `65k+` data. It could be worth setting the `train_batch_size` <= `16`, if your GPU does not have a lot of memory, otherwise, the training will take time; also similarly for `eval_batch_size` and `test_batch_size` we will set it to `64`, otherwise the evaluation stage will take a lot of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cece146c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 16\n",
    "eval_batch_size = min(64, len(tokenised_datasets[\"validation\"]))\n",
    "test_batch_size = min(64, len(tokenised_datasets[\"test\"]))\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=tokenised_datasets[\"train\"],\n",
    "    batch_size=train_batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    dataset=tokenised_datasets[\"validation\"],\n",
    "    batch_size=eval_batch_size,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=tokenised_datasets[\"test\"],\n",
    "    batch_size=test_batch_size,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"So there are,\\n\\t{len(train_dataloader)} batches of size {train_batch_size} in the training dataset,\\n\\t{len(eval_dataloader)} batches of size {eval_batch_size} in the evaluation dataset, and\\n\\t {len(test_dataloader)} batches of size {test_batch_size} in the test dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c07159",
   "metadata": {},
   "source": [
    "### 9. Setup the *accelerator*, *optimisor* and *learning rate scheduler* object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fdb301",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "\n",
    "# optimiser\n",
    "optimiser = AdamW(\n",
    "    params=model.parameters(),\n",
    "    lr=2e-5\n",
    ")\n",
    "\n",
    "# accelerator\n",
    "accelerator = Accelerator()\n",
    "# preparing accelerator objects\n",
    "train_dl, eval_dl, test_dl, model, optimiser = accelerator.prepare(\n",
    "    train_dataloader,\n",
    "    eval_dataloader,\n",
    "    test_dataloader,\n",
    "    model,\n",
    "    optimiser\n",
    ")\n",
    "\n",
    "num_epochs = 5\n",
    "num_training_steps = num_epochs * len(train_dl)\n",
    "# 10% warmup\n",
    "num_warmup_steps = int(.1 * num_training_steps)\n",
    "\n",
    "lr_schedular = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimiser,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "print(f\"Total training steps {num_training_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6e5e1a",
   "metadata": {},
   "source": [
    "### 10. Evaluation Metric Setup\n",
    "\n",
    "Now this time since there is no pre-evaluation metric present, therefore we have to define which metrics to use when evaluating our model.\n",
    "<br />\n",
    "For a *Classification* task,  the best metrics to evalute a model are **Accuracy**, **Precision**, **Recall** and **F1 Score**. The latter three metrics are detrived from a **Confusion Matrix**, which is basically a `N X N matrix`, where `N` is the *number of classes or categories* that are to be predicted. The values inside the *confusion matrix* represents one of these 4 values:\n",
    "+ **True Positives (TP)** : It is the case where we predicted Yes and the real output was also Yes.\n",
    "+ **True Negatives (TN)**: It is the case where we predicted No and the real output was also No.\n",
    "+ **False Positives (FP)**: It is the case where we predicted Yes but it was actually No.\n",
    "+ **False Negatives (FN)**: It is the case where we predicted No but it was actually Yes. \n",
    "\n",
    "For example, suppose there is a problem which is a binary classification with labels as `Yes` or `No`. So, here `N = 2`, therfore we will get a `2 X 2` *confusion matrix*. Now let's say we tested our model with 165 samples and the results using *confusion matrix* looks like this:\n",
    "\n",
    "|              |Predicted No|Predited Yes|\n",
    "|--------------|------------|------------|\n",
    "|**Actual No** |50|10|\n",
    "|**Actual Yes**|5|100|\n",
    "\n",
    "Therefore, out of the 165 predictions, `100` predictions were **TP** (bottom right), `50` were **TN** (top left), `10` were **FP** (top right), and `5` were *FN* (bottom left).\n",
    "\n",
    "\n",
    "Now, how these values are useful because we can use them to calculate **Precision**, **Recall** and **F1 Score**:\n",
    "+ **Precision**: It measures how many of the positive predictions made by the model are actually correct. It's useful when the cost of false positives is high such as in medical diagnoses where predicting a disease when it’s not present can have serious consequences. Therefore, *Precision* helps ensure that when the model predicts a positive outcome, it’s likely to be correct.\n",
    "$$\n",
    "\\text{Precision} = \\frac{TP}{TP+FP}\n",
    "$$\n",
    "+ **Recall**: *Recall* or *Sensitivity measures* how many of the actual positive cases were correctly identified by the model. It is important when missing a positive case (*false negative*) is more costly than false positives (like disease detection).\n",
    "$$\n",
    "\\text{Recall} = \\frac{TP}{TP+FN}\n",
    "$$\n",
    "+ **F1 Score**: The *F1 Score* is the *harmonic mean* of *precision* and *recall*. It is useful when we need a balance between *precision* and *recall*, as it combines both into a single number. A *high F1 score* means the model performs well on both metrics, i.e., the model is performing well. Its range is `[0,1]`:\n",
    "$$\n",
    "\\text{F1 Score}=2\\times\\frac{Precision+Recall}{Precision×Recall} \n",
    "$$\n",
    "Now, when you have multiple classes, you still often want a single precision/recall/F1 number—but how you combine per-class scores depends on whether you care more about rare classes, common classes, or every example equally and there you have to use a *averaging strategy*. Here’s what each averaging strategy does:\n",
    "\n",
    "+ **Weighted**: Compute each class’s score, then average them but weight by how many true examples each class has - so common labels count more.\n",
    "\n",
    "+ **Micro**: Pool all true/false positives and negatives across every example, then compute one overall score - every prediction is equal (large classes dominate).\n",
    "\n",
    "+ **Macro**: Compute each class’s score and then take the simple average—every class counts the same, no matter how many examples it has.\n",
    "\n",
    "> NOTE: **Lower recall** and **higher precision** gives us **great accuracy** but then it misses a large number of instances and that's why **accuracy** alone is not a good metric when evaluating a model and using **Recall**, **Precision** and **F1 score** if possible is a good practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2fde33",
   "metadata": {},
   "source": [
    "Luckily, the `evaluate` lib provides `combine()` method, where you can specify which metrics to use for the evaluation, and also when calling the `compute()` we can pass the `average` argument to specify which averaging strategy to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7f3142",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import torch\n",
    "\n",
    "def perform_evaluation():\n",
    "    \"\"\"\n",
    "    Perform evaluation on the validation set\n",
    "    \"\"\"\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    eval_epoch_loss = []\n",
    "\n",
    "    # initialising evaluation metrics\n",
    "    ## accuracy\n",
    "    eval_acc_metric = evaluate.load(\"accuracy\") \n",
    "    ## f1 score\n",
    "    eval_f1_metric = evaluate.load(\"f1\")\n",
    "    ## precision & recall\n",
    "    eval_specific_metric = evaluate.combine(\n",
    "        evaluations=[\n",
    "            \"precision\",\n",
    "            \"recall\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # for every validation batch\n",
    "    for batch in eval_dl:\n",
    "        # Disable gradient computation for evaluation (saves memory and computation)\n",
    "        with torch.no_grad():\n",
    "            # pass the input to the model\n",
    "            outputs = model(**batch)\n",
    "            # Store loss inside no_grad for memory efficiency\n",
    "            eval_epoch_loss.append(outputs.loss.item())\n",
    "\n",
    "            # Get predictions for metrics (logits already created without gradients)\n",
    "            logits = outputs.logits\n",
    "            refs = batch[\"labels\"]\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            # Add preds and refs to evaluation metrics\n",
    "            ## accuracy\n",
    "            eval_acc_metric.add_batch(\n",
    "                predictions=accelerator.gather(preds),\n",
    "                references=accelerator.gather(refs)\n",
    "            )\n",
    "            ## f1 score\n",
    "            eval_f1_metric.add_batch(\n",
    "                predictions=accelerator.gather(preds),\n",
    "                references=accelerator.gather(refs)\n",
    "            )\n",
    "            ## precision & recall\n",
    "            eval_specific_metric.add_batch(\n",
    "                predictions=accelerator.gather(preds),\n",
    "                references=accelerator.gather(refs)\n",
    "            )\n",
    "    \n",
    "    # compute the average loss\n",
    "    eval_avg_loss = sum(eval_epoch_loss) / len(eval_epoch_loss)\n",
    "\n",
    "    # dict to store the metrics stats\n",
    "    eval_pred_stats = {}\n",
    "    # compute accuracy and add it to the dict\n",
    "    eval_pred_stats.update(eval_acc_metric.compute())\n",
    "    # compute the f1 score, with 'weighted' as the averaging strategy\n",
    "    # and update the dict with the metric\n",
    "    eval_pred_stats.update(\n",
    "        eval_f1_metric.compute(\n",
    "            average=\"weighted\",\n",
    "            labels= list(range(len(label_names))) # ClassLabels\n",
    "        )\n",
    "    )\n",
    "    # compute precision and recall, with 'weighted' as the averaging strategy\n",
    "    # and update the dict with the metric. \n",
    "    eval_pred_stats.update(\n",
    "        eval_specific_metric.compute(\n",
    "            average=\"weighted\",\n",
    "            zero_division=0, # when there is a zero in the denominator, replace the result with 0\n",
    "            labels= list(range(len(label_names))) # ClassLabels\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return eval_avg_loss, eval_pred_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a513ea4",
   "metadata": {},
   "source": [
    "We would also like to evaluate our model performance on the test data after it is totally trained because testing on untouched data gives a true measure of how our model will perform on new examples and prevents us from overfitting by tuning to the same data we used to train it. \n",
    "<br />\n",
    "So, let's write the evaluation function on the test data, and this time we can also ask for the *confusion_matrix* from the `evalute.compute()` function along with other metrics to further evalute the model on the test data.\n",
    "\n",
    "> Note: when evaluating the model on the test data we don't need to look at the loss value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bc7d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_evaluation():\n",
    "    \"\"\"\n",
    "    Perform evaluation on the test set\n",
    "    \"\"\"\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # initialising evaluation metrics\n",
    "    ## accuracy\n",
    "    test_acc_metric = evaluate.load(\"accuracy\")\n",
    "    ## f1 score\n",
    "    test_f1_metric = evaluate.load(\"f1\")\n",
    "    ## precision & recall\n",
    "    test_specific_metric = evaluate.combine(\n",
    "        evaluations=[\n",
    "            \"precision\",\n",
    "            \"recall\"\n",
    "        ]\n",
    "    )\n",
    "    ## confusion metrix\n",
    "    test_cm_metric = evaluate.load(\"confusion_matrix\")\n",
    "\n",
    "    # for every test batch\n",
    "    for batch in test_dl:\n",
    "        # Disable gradient computation for evaluation (saves memory and computation)\n",
    "        with torch.no_grad():\n",
    "            # pass the input to the model\n",
    "            outputs = model(**batch)\n",
    "\n",
    "            # Get predictions for metrics (logits already created without gradients)\n",
    "            logits = outputs.logits\n",
    "            refs = batch[\"labels\"]\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            # Add preds and refs to evaluation metrics\n",
    "            ## accuracy\n",
    "            test_acc_metric.add_batch(\n",
    "                predictions=accelerator.gather(preds),\n",
    "                references=accelerator.gather(refs)\n",
    "            )\n",
    "            ## f1 score\n",
    "            test_f1_metric.add_batch(\n",
    "                predictions=accelerator.gather(preds),\n",
    "                references=accelerator.gather(refs)\n",
    "            )\n",
    "            ## precision & recall\n",
    "            test_specific_metric.add_batch(\n",
    "                predictions=accelerator.gather(preds),\n",
    "                references=accelerator.gather(refs)\n",
    "            )\n",
    "            ## confusion metrix\n",
    "            test_cm_metric.add_batch(\n",
    "                predictions=accelerator.gather(preds),\n",
    "                references=accelerator.gather(refs)\n",
    "            )\n",
    "    \n",
    "    # dict to store the metrics stats\n",
    "    test_pred_stats = {}\n",
    "    # compute accuracy and add it to the dict\n",
    "    test_pred_stats.update(test_acc_metric.compute())\n",
    "    # compute the f1 score, with 'weighted' as the averaging strategy\n",
    "    # and update the dict with the metric\n",
    "    test_pred_stats.update(\n",
    "        test_f1_metric.compute(\n",
    "            average=\"weighted\",\n",
    "            labels= list(range(len(label_names))) # ClassLabels\n",
    "        )\n",
    "    )\n",
    "    # compute precision and recall, with 'weighted' as the averaging strategy\n",
    "    # and update the dict with the metric. \n",
    "    test_pred_stats.update(\n",
    "        test_specific_metric.compute(\n",
    "            average=\"weighted\",\n",
    "            labels= list(range(len(label_names))), # ClassLabels\n",
    "            zero_division=0  # when there is a zero in the denominator, replace the result with 0\n",
    "        )\n",
    "    )\n",
    "    # compute the confusion matrix \n",
    "    test_pred_stats.update(\n",
    "        test_cm_metric.compute(\n",
    "            labels= list(range(len(label_names)))  # ClassLabels\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return test_pred_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d8c265",
   "metadata": {},
   "source": [
    "let's also write a plot function so that we can visualise the confusion matrix and other metrics  from the test evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1595a5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(confusion_matrix, label_names):\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix, display_labels=label_names)\n",
    "    disp.plot(cmap=plt.cm.Blues, values_format='d')  # Use '.2f' for float\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_metrics(metrics_dict):\n",
    "    names = list(metrics_dict.keys())\n",
    "    values = list(metrics_dict.values())\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.barh(names, values, color='skyblue')\n",
    "    plt.xlabel(\"Score\")\n",
    "    plt.title(\"Evaluation Metrics\")\n",
    "    plt.xlim(0, 1)  # If all values are between 0 and 1\n",
    "    plt.grid(True, axis='x', linestyle='--', alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e438a56b",
   "metadata": {},
   "source": [
    "### 11. Training\n",
    "\n",
    "Let's write the training function first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b905c85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from livelossplot import PlotLosses\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# training progress bar\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "def training_function():\n",
    "    # initialise the plotter for the learning curve\n",
    "    plotter = PlotLosses(mode='notebook')\n",
    "\n",
    "    # for every epoch\n",
    "    for epoch in range(num_epochs):\n",
    "        # ensure model is in training mode\n",
    "        model.train()\n",
    "\n",
    "        # store loss per batch \n",
    "        train_epoch_loss = []\n",
    "\n",
    "        # metrics for training data\n",
    "        ## accuracy\n",
    "        train_acc_metric = evaluate.load(\"accuracy\")\n",
    "        ## f1 score\n",
    "        train_f1_metric = evaluate.load(\"f1\")\n",
    "        ## precision & recall\n",
    "        train_specific_metric = evaluate.combine(\n",
    "            evaluations=[\n",
    "                \"precision\",\n",
    "                \"recall\"\n",
    "            ]   \n",
    "        )\n",
    "\n",
    "        # for every bacth in the training set\n",
    "        for batch in train_dl:\n",
    "            # Forward Pass (keep gradient attached)\n",
    "            outputs = model(**batch)\n",
    "            ## get the loss\n",
    "            loss = outputs.loss\n",
    "\n",
    "            # Backward Pass (while gradients are still attached)\n",
    "            ## compute gradients\n",
    "            accelerator.backward(loss)\n",
    "            ## gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            ## Nudges the weights (\"knobs\") in the right direction based on the gradient\n",
    "            optimiser.step()\n",
    "            ## Update the learning-rate scheduler\n",
    "            lr_schedular.step()\n",
    "            ## Reset gradients to zero so they don’t accumulate\n",
    "            ## into the next batch.\n",
    "            optimiser.zero_grad()\n",
    "\n",
    "\n",
    "            # metric computation\n",
    "            with torch.no_grad():\n",
    "                # detach loss for metric computation \n",
    "                train_epoch_loss.append(loss.detach().item())\n",
    "\n",
    "                # detach logits for metric computation\n",
    "                logits = outputs.logits.detach()\n",
    "                # no need to detach labels (they don't have gradients)\n",
    "                refs = batch['labels']\n",
    "                preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "                # add preds and refs to the train matrics\n",
    "                ## accuracy\n",
    "                train_acc_metric.add_batch(\n",
    "                    predictions=accelerator.gather(preds),\n",
    "                    references=accelerator.gather(refs)\n",
    "                )\n",
    "                ## f1 score\n",
    "                train_f1_metric.add_batch(\n",
    "                    predictions=accelerator.gather(preds),\n",
    "                    references=accelerator.gather(refs)\n",
    "                )\n",
    "                ## precision and recall\n",
    "                train_specific_metric.add_batch(\n",
    "                    predictions=accelerator.gather(preds),\n",
    "                    references=accelerator.gather(refs)\n",
    "                )\n",
    "            \n",
    "            # update the progress bar by 1 step\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        # training average loss\n",
    "        tain_avg_loss = sum(train_epoch_loss)/len(train_epoch_loss)\n",
    "\n",
    "        ## dict to store the metrics stats\n",
    "        train_pred_stats = {}\n",
    "        # compute accuracy and add it to the dict\n",
    "        train_pred_stats.update(train_acc_metric.compute())\n",
    "        # compute the f1 score, with 'weighted' as the averaging strategy\n",
    "        # and update the dict with the metric\n",
    "        train_pred_stats.update(\n",
    "            train_f1_metric.compute(\n",
    "                average=\"weighted\",\n",
    "                labels= list(range(len(label_names))) # ClassLabels\n",
    "            )\n",
    "        )\n",
    "        # compute precision and recall, with 'weighted' as the averaging strategy\n",
    "        # and update the dict with the metric. \n",
    "        train_pred_stats.update(\n",
    "            train_specific_metric.compute(\n",
    "                average=\"weighted\",\n",
    "                labels= list(range(len(label_names))), # ClassLabels\n",
    "                zero_division=0 # when there is a zero in the denominator, replace the result with 0\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "        # evaluation phase\n",
    "        eval_avg_loss, eval_pred_stats = perform_evaluation()\n",
    "\n",
    "        # update the learning curve\n",
    "        plotter.update({\n",
    "            'loss': tain_avg_loss,\n",
    "            'val_loss': eval_avg_loss,\n",
    "            'acc': train_pred_stats['accuracy'],\n",
    "            'val_acc': eval_pred_stats['accuracy'],\n",
    "            'precision': train_pred_stats['precision'],\n",
    "            'val_precision': eval_pred_stats['precision'],\n",
    "            'recall': train_pred_stats['recall'],\n",
    "            'val_recall': eval_pred_stats['recall'],\n",
    "            'f1': train_pred_stats['f1'],\n",
    "            'val_f1': eval_pred_stats['f1'],\n",
    "        })\n",
    "        plotter.send() \n",
    "\n",
    "\n",
    "    print(\"\\n\\n\\n################# Test dataset Evaluation:\\n\\n\")\n",
    "    # After the model is totally trained, perform evaluation on the test dataset\n",
    "    test_pred_stats = test_evaluation()\n",
    "\n",
    "    confusion_matrix = test_pred_stats.pop(\"confusion_matrix\")\n",
    "    # plot the metrics\n",
    "    plot_confusion_matrix(confusion_matrix, label_names)\n",
    "    plot_metrics(test_pred_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc212c85",
   "metadata": {},
   "source": [
    "Finally, let's launch the training loop with `num_processes=1`, as my machine has only 1 dedicated gpu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21805635",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import notebook_launcher\n",
    "\n",
    "# launch the accelrator based training funcrion with one gpu\n",
    "notebook_launcher(training_function, num_processes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25dca31",
   "metadata": {},
   "source": [
    "### 12. Performance Analysis\n",
    "\n",
    "As we can see, in the initial training phase, it was observed that validation metrics (accuracy, F1, precision, recall) were unexpectedly higher than training metrics during the first epoch. This behavior, though uncommon, can occur due to factors such as *dropout* being applied only during training, the use of a *pretrained model* that already performs well on validation data, or inconsistencies in metric aggregation (e.g., batch-wise vs full set). As training progressed, the model quickly improved on the training set, with metrics surpassing validation scores by the second epoch. However, validation performance *plateaued* and the validation *loss* began to rise slightly after epoch 2, suggesting early signs of *overfitting*. This indicates that employing **early stopping** (around epoch 2–3) and **stronger regularization** or **data augmentation** strategies may help maintain generalization.\n",
    "\n",
    "\n",
    "On the held-out test set, the model demonstrated strong and consistent performance across all key metrics (accuracy, precision, recall, and F1 score), all approaching or above 0.93. The confusion matrix further supports this, showing high classification accuracy across categories like birth control, depression, and anxiety, with only minor misclassifications—most notably, some confusion between acne and birth control, and between depression and anxiety. Overall, the model generalizes well to unseen data and handles class separation effectively, confirming the effectiveness of the training approach despite the early metric inversion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918fa3f1",
   "metadata": {},
   "source": [
    "# Managing Big Data\n",
    "Nowadays, it's common for datasets used to train models from scratch to range from multiple gigabytes to several terabytes. In such cases, even loading the data can be challenging - especially when hardware is limited, such as having restricted RAM or GPU memory.\n",
    "<br />\n",
    "Fortunately, the Hugging Face datasets library is designed to handle these challenges:\n",
    "\n",
    "- It addresses memory management issues by treating datasets as memory-mapped files, enabling efficient access without loading the entire dataset into RAM.\n",
    "\n",
    "- It also offers a streaming feature that allows you to access data on-the-fly. This is especially useful when you can’t store a large dataset locally - data is downloaded and processed one sample at a time, without requiring the full dataset to be downloaded first.\n",
    "\n",
    "You don’t need to do anything special to benefit from memory-mapping - it works automatically in all the examples we’ve seen so far.\n",
    "\n",
    "So in this section, we’ll focus on how the streaming feature works in practice.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11d7ac9",
   "metadata": {},
   "source": [
    "### Streaming dataset\n",
    "\n",
    "To enable dataset streaming you just need to pass the `streaming=True` argument to the `load_dataset()` function.\n",
    "<br />\n",
    "For example, let’s in *streaming* mode load the [*HuggingFace FineWeb*](https://huggingface.co/datasets/HuggingFaceFW/fineweb) dataset, it is a 18.5T tokens (originally 15T tokens) of cleaned and deduplicated english web data from CommonCrawl. Its total size is 108 TB.\n",
    "\n",
    "> Note: we are using this dataset only for example purposes. It is mainly used to train LLM models and the data processing pipeline is optimized for LLM performance and ran on the  [datatrove](https://github.com/huggingface/datatrove/) library (a large scale data processing library) and not on `datasets`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6de5440",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "fineweb_dataset = load_dataset(\n",
    "    \"HuggingFaceFW/fineweb\",\n",
    "    streaming=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699dde7b",
   "metadata": {},
   "source": [
    "Now, when we have `streaming=True` in the `load_dataset()` function instead of the usual `DatasetDict` object it returns an `IterableDataset` object. As the name suggests, to access the elements of an `IterableDataset` we need to iterate over it using `iter()` enclosed inside a `next()`(to get the next present value in the iteration):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf71383",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(fineweb_dataset['train']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06586af7",
   "metadata": {},
   "source": [
    "Now to process the data inside the `IterableDataset` object, for example, during *pre-processing* or *tokenisation*, we can the `IterableDataset.map()`. The process is exactly the same as the one we used to tokenize our `DatasetDict` dataset prevously, with the only difference being that outputs are returned one by one, but we can also pass `batched=True` here, and it will process the examples batch by batch; the default batch size is 1,000 and can be specified with the `batch_size` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd24831",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from pprint import pprint \n",
    "\n",
    "checkpoint = \"distilbert-base-uncased\"\n",
    "tokeniser = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "tokenised_datasets = fineweb_dataset.map(\n",
    "    lambda data: tokeniser(data[\"text\"], truncation=True),\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "pprint(next(iter(tokenised_datasets['train'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d37de8d",
   "metadata": {},
   "source": [
    "We can also shuffle a streamed dataset using `IterableDataset.shuffle()`, but unlike `Dataset.shuffle()` this only shuffles the elements in a predefined `buffer_size`:\n",
    "\n",
    "> Note: In this example, we selected a random example from the first `10,000` examples in the buffer. Once an example is accessed, its spot in the buffer is filled with the next example in the corpus (i.e., the `10,001`st example in the case above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c26a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_dataset = fineweb_dataset.shuffle(buffer_size=10_000, seed=42)\n",
    "next(iter(shuffled_dataset[\"train\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a47307",
   "metadata": {},
   "source": [
    "To select the first `n` examples we can call the `IterableDataset.take()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8a0634",
   "metadata": {},
   "outputs": [],
   "source": [
    "fineweb_dataset_head = fineweb_dataset[\"train\"].take(5)\n",
    "list(fineweb_dataset_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb796c3",
   "metadata": {},
   "source": [
    "And similary, we can use the `IterableDataset.skip()` function to skip n examples and combined both `take()` and `skip()` to even create splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb43b248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip the first 1,000 examples and include the rest in the training set\n",
    "train_split_dataset = shuffled_dataset[\"train\"].skip(1000)\n",
    "# Take the first 1,000 examples for the validation set\n",
    "validation_split_dataset =  shuffled_dataset[\"train\"].take(1000)\n",
    "\n",
    "pprint(train_split_dataset)\n",
    "\n",
    "pprint(validation_split_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60de838",
   "metadata": {},
   "source": [
    "Lastly, we can also combine multiple datasets together to create a single corpus using the `interleave_datasets()` function. It converts a list of IterableDataset objects into a single IterableDataset, where the elements of the new dataset are obtained by alternating among the source examples.\n",
    "Let's combine the above dataset with [**FineWeb2**](https://huggingface.co/datasets/HuggingFaceFW/fineweb-2) dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966a9e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "fineweb2_dataset = load_dataset(\n",
    "    \"HuggingFaceFW/fineweb-2\",\n",
    "    \"aai_Latn\",\n",
    "    streaming=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed737868",
   "metadata": {},
   "source": [
    "let’s now combine the  datasets with the interleave_datasets() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ebb00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "from datasets import interleave_datasets\n",
    "\n",
    "fineweb_train_combined_dataset = interleave_datasets([fineweb_dataset[\"train\"], fineweb2_dataset[\"train\"]])\n",
    "list(islice(fineweb_train_combined_dataset, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66a1229",
   "metadata": {},
   "source": [
    "Here we’ve used the `islice()` function from Python’s `itertools` module to select the first two examples from the combined dataset, and we can see that they match the first examples from each of the two source datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc5a5ff",
   "metadata": {},
   "source": [
    "# Creating your own dataset\n",
    "Let's create a corpus of [GitHub issues](https://github.com/features/issues/), which are commonly used to track bugs or features in GitHub repositories. This corpus could be used for various purposes, including:\n",
    "- Exploring how long it takes to close open issues or pull requests\n",
    "- Training a multilabel classifier that can tag issues with metadata based on the issue’s description (e.g., “bug,” “enhancement,” or “question”)\n",
    "- Creating a semantic search engine to find which issues match a user’s query.\n",
    "\n",
    "For this example, let's use the GitHub issues associated with [**HuggingFace Datasets**](https://github.com/huggingface/datasets/issues) branch. Each issues contains a title, a description, and a set of labels that characterize the issue.\n",
    "\n",
    "!['HuggingFace Datasets'](data/chapter_5/datasets-issues-single.png 'HuggingFace Datasets')\n",
    "\n",
    "We will use the `requests` python lib to make a HTTP GET request to pull the issues fro the [**Issues enpoint**](https://docs.github.com/en/rest/reference/issues#list-repository-issues) provided by [**GitHub REST API**](https://docs.github.com/en/rest). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4e366c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "page = 1\n",
    "# Number of issues to return per page\n",
    "per_page = 1\n",
    "url = f\"https://api.github.com/repos/huggingface/datasets/issues?page={page}&per_page={n_issue}\"\n",
    "response = requests.get(url)\n",
    "\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afcda36",
   "metadata": {},
   "source": [
    "In the above code we requested the first issue on the first page, and also checked the `status_code` property of the `response` object to see if the request was successful or not. A status code of 200 means the request was successful (you can find a list of possible HTTP status codes [here](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)). \n",
    "<br />\n",
    "Now let's access the main *payload*  in *JSON* format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fe57b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b653e2",
   "metadata": {},
   "source": [
    "Out of all the included info, the most inportant in our case are `title`, `body` and `number` because they describe the issue.\n",
    "<br />\n",
    "Now, when we make an unauthenticated request to Github REST API (like the way we did above), it will only allows 60 requests per hour. Although we can increase the `per_page` query parameter to reduce the number of requests we make, we will still hit the rate limit on any repository that has more than a few thousand issues. \n",
    "<br />\n",
    "So instead, let's create a [personal access token](https://docs.github.com/en/github/authenticating-to-github/creating-a-personal-access-token) so that you can boost the rate limit to 5,000 requests per hour.\n",
    "<br />\n",
    "Once we have created the *personal access token*, we just need to pass it to the `github_fine_grained_personal_access_tokens=` variable in the `.env` file and save it. Now, we can use the `dotenv` python lib to load the `github_personal_access_token` value.\n",
    "> Note: It's recommend to store any secret token in a .env file and use the python-dotenv library to load it automatically for you as an environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a827340b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "github_personal_access_token = os.getenv(\"github_fine_grained_personal_access_tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03abd8e7",
   "metadata": {},
   "source": [
    "now let's gather the issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbd8599",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def fetch_issues(\n",
    "        git_repo_owner,\n",
    "        repo_name,\n",
    "        num_issues=10000,\n",
    "        rate_limit=5000,\n",
    "        output_path=Path(\".\")\n",
    "):\n",
    "    # if the path not exist\n",
    "    if not output_path.is_dir():\n",
    "        # create the right folder\n",
    "        output_path.mkdir(exist_ok=True)\n",
    "\n",
    "    all_issues = []\n",
    "\n",
    "    # Number of issues to return per page\n",
    "    per_page = 100\n",
    "\n",
    "    num_pages = math.ceil(num_issues / per_page)\n",
    "    base_url = \"https://api.github.com/repos\"\n",
    "\n",
    "    progress_bar = tqdm(range(num_pages), desc=\"Downloading Page\")\n",
    "    for page in range(num_pages):\n",
    "        # Query with state=all to get both open and closed issues\n",
    "        query = f\"issues?page={page}&per_page={per_page}&state=all\"\n",
    "        page_issues = requests.get(\n",
    "            f\"{base_url}/{git_repo_owner}/{repo_name}/{query}\",\n",
    "            # pass the personal access token as the Authorization header\n",
    "            headers={\"Authorization\": f\"token {github_personal_access_token}\"}\n",
    "        )\n",
    "\n",
    "        all_issues.extend(page_issues.json())\n",
    "\n",
    "        \n",
    "\n",
    "        if len(all_issues) >= rate_limit or page_issues.status_code != 200:\n",
    "            progress_bar.close()\n",
    "            break\n",
    "        \n",
    "        progress_bar.update(1)\n",
    "\n",
    "    issues = pd.DataFrame.from_records(all_issues)\n",
    "    issues.to_json(\n",
    "        f\"{output_path}/{repo_name}-issues.jsonl\",\n",
    "        orient=\"records\",\n",
    "        lines=True\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Downloaded all the issues for {repo_name}! Dataset stored at {output_path}/{repo_name}-issues.jsonl\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2a0547",
   "metadata": {},
   "source": [
    "In the above function, once we have hit the rate limit or if the request we have was not successful, we simply stop and store all the issues we have gathered so far. Now, let's call the function and download all the issues and then we can use the `load_dataset()` function `datasets` lib to load the data:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
