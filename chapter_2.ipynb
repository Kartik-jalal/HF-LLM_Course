{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "220f39b7",
   "metadata": {},
   "source": [
    "# Replicating The pipeline() Function Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a590859",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\n",
    "    [\n",
    "        \"It is really hot here in Italy and going outside means being drenched in your own sweat and it does not feel good.\",\n",
    "        \"The weather in my hometown during summer is really nice!\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d3d1e4",
   "metadata": {},
   "source": [
    "When we run the above cell, we obtain\n",
    "```\n",
    "[{'label': 'NEGATIVE', 'score': 0.9997081160545349},\n",
    " {'label': 'POSITIVE', 'score': 0.9998784065246582}]\n",
    "```\n",
    "Now under the hood, the pipeline function is conducting all the tasks neccessary to use the specified model (in this case it is the default model, which is *distilbert-base-uncased-finetuned-sst-2-english*) for the prediction. The steps are, *preprocessing (tokenisation)*, *then passing that input to the model*, and finally, *postprocessing*:\n",
    "<br />\n",
    "<br />\n",
    "![Pipeline Steps](data/chapter_2/pipeline_steps.png \"The Three Steps!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8bd522",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b994e298",
   "metadata": {},
   "source": [
    "### Preprocessing with a tokenizer\n",
    "\n",
    "Every model have their own way of splitting the text input into tokens, mapping them to interger and additing any additional input value that is needed for the model to process. Therefore, when fine_tuning/inference a pretrained model one needs to make sure the tokenisation methods fellows the same rule and it can be done by using the `from_pretrained()` method by the `AutoTokenizer` class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d199cdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdccf8d",
   "metadata": {},
   "source": [
    "now that we have the right tokeniser, we need to preprocess the raw input using it and pass the tensors (in simple term a tensor is basically an array which follows some strict rules) retuned by the tokeniser to the model. In the example below, we are asking the tokeniser to tokenise the raw input  and perform:\n",
    "- *padding* - adding non attention tokens to the shorter length text so that the length of all the input text become equal\n",
    "- *truncation* - remove the input text words that exceeds the input text length limit of the model, and \n",
    "- *return type* - the type of tensor to return (`return_tensors='pt'`, where `pt` means *pytorch tensor*).\n",
    "\n",
    "> Note: the use of *padding* and *truncation* mainly depends on the type of return object you asked from the tokeniser. If you would want a tensor, then having *padding* and *truncation* set is the right way because tensor only accept rectangular shapes (think matrices).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f374baea",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_inputs = [\n",
    "    \"It is really hot here in Italy and going outside means being drenched in your own sweat and it does not feel good.\",\n",
    "    \"The weather in my hometown during summer is really nice!\",\n",
    "]\n",
    "\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors='pt')\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed76078",
   "metadata": {},
   "source": [
    "As we can see, the output itself is a dictionary containing two keys, `input_ids` and `attention_mask`. \n",
    "- *input_ids*, numerical representations of your tokens. Here it contains two rows of integers (one for each sentence) and,\n",
    "- *attention_mask*, in simple term is to tell the model later which tokens needs attention.\n",
    "\n",
    ">  Note: there is also one key value instance that you get from the tokeniser, *token_type_ids*, these tell the model which part of the input is sentence A and which is sentence B."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5f829a",
   "metadata": {},
   "source": [
    "### Going through the model\n",
    "We use the `AutoModel` class this time to load the model by using its `from_pretrained()` function. And this way of using the basic *AutoModel* class, when we pass the input to the model it only returns a *hidden states*/*features*/*high-dimensional vector* which represents the contextual understanding of that input by the Transformer model and not a solid quantity that can be analysed, and that is beacause the model this way does not have the specific head that can convert the *hidden states*/*features*/*high-dimensional vector* into a quantity that can be analysed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b8b66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f61ce20",
   "metadata": {},
   "source": [
    "The returned *hidden state*, generally have three dimensions:\n",
    "- Batch size: The number of sequences processed at a time (2 in our example).\n",
    "- Sequence length: The length of the numerical representation of the sequence (16 in our example).\n",
    "- Hidden size: The vector dimension of each model inputb, i.e., basically the output values of the model's *hidden states*/*features*/*high-dimensional vector*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8e0f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714c5176",
   "metadata": {},
   "source": [
    "### Model heads: Making sense out of numbers\n",
    "So, a model head basically take the *hidden states*/*features*/*high-dimensional vector* model output (in this case the size of it is *768*) and project them onto a different dimension that can be analysed.\n",
    "<br />\n",
    "<br />\n",
    "![A Model Worflow](data/chapter_2/model_workflow.png \"A Model Worflow\")\n",
    ">In this diagram, the model is represented by its embeddings layer and the subsequent layers. The embeddings layer converts each input ID in the tokenized input into a vector that represents the associated token. The subsequent layers manipulate those vectors using the attention mechanism to produce the final representation of the sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cbbf35",
   "metadata": {},
   "source": [
    "Now, instead of *AutoModel* class, we will use the *AutoModelForSequenceClassification* which comes with the sequence classification head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbe5fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n",
    "outputs = model(**inputs)\n",
    "\n",
    "print(outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f27566e",
   "metadata": {},
   "source": [
    "Now, because the model already comes with a head this time, the dimensionality of the output will be much lower: now the outputs vectors containing two values (one per label, i.e., Negative, Positive).ince we have just two sentences and two labels, the result we get from our model is of shape 2 x 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af55893a",
   "metadata": {},
   "source": [
    "### Postprocessing The Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa345b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d293d979",
   "metadata": {},
   "source": [
    "However, even now the model's output does not make sense and that's because the outputs are *logits* (a logit is a raw, unnormalized score) and to convert the *logits* to the right numerical value we have to pass them through a *linear/non linear activation function* (i.e., a function that can normalise them), for example, **softmax**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaf2489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a405c20",
   "metadata": {},
   "source": [
    "Now we can see that the model predicted `[0.9971, 0.0002]` for the first sentence and `[0.0001, 0.9998]` for the second one, these are recognizable probability scores. Now, to get the labels corresponding to each position, we can inspect the id2label attribute of the model config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ade31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e7698d",
   "metadata": {},
   "source": [
    "Now we can conclude that the model predicted the following:\n",
    "\n",
    "- First sentence: NEGATIVE: 0.9971, POSITIVE: 0.0002 -> NEGATIVE\n",
    "- Second sentence: NEGATIVE: 0.0001, POSITIVE: 0.9998 -> POSITIVE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc157ef6",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab13f06a",
   "metadata": {},
   "source": [
    "Similarly to:\n",
    "```\n",
    "from transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\"bert-base-cased\")\n",
    "```\n",
    "\n",
    "We can also directly load the model if we know the right model name.\n",
    "Furthermore, can also save a model in our local machine by calling the `save_pretrained()` method of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59df895f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "outputs  = model(**inputs)\n",
    "\n",
    "# save the model\n",
    "model.save_pretrained('data/chapter_2/bert_model')\n",
    "\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0bec3c",
   "metadata": {},
   "source": [
    "Two files get saved in the given directory *config.json* and *model.safetensors* that defines the saved model:\n",
    "- *config.json* - It stores all the necessary attributes needed to build the model architecture.\n",
    "- *model.safetensors* - It's the state dictionary; it contains all your model’s weights.\n",
    "\n",
    "\n",
    "Furthermore, we can load the model simply b calling the `from_pretrained()` method of `AutoModel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e94890",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained('data/chapter_2/bert_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c8fd01",
   "metadata": {},
   "source": [
    "# Tokenisation Encoding Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08f7a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04eee6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input = tokenizer(\"How are you?\", \"How's your holiday going?\")\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa99dd0c",
   "metadata": {},
   "source": [
    " Here we can see when we encode multiple sentences at once, the *token_type_ids* is also returned to specify which token id belongs to which sentence and that is because if we try to decode the input ids back to their orignal state of two sentences. It comes as a one long single sentence with some custom special tokens `[CLS]` and `[SEP]`, which are model specific and mostly used to tell the model the starting and ending of a sequence to be processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177963dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(encoded_input[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a590a68",
   "metadata": {},
   "source": [
    "Similarly to loading a know model directly from its class we can load a tokeniser as well without needing the `AutoModel` class. And similarly we can call the `save_pretrained()` to save the tokeniser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ba3ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "input = tokenizer(\"I stay till late in the library because it is open till late and I am not so productive when staying back in my room.\")\n",
    "\n",
    "tokenizer.save_pretrained(\"data/chapter_2/tokeniser\")\n",
    "\n",
    "print(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa12f842",
   "metadata": {},
   "source": [
    "# Tokeniser Pipeline\n",
    "We also know by now what happens when we call the `tokenzer()` to convert a text into machine readable input format. The tokeniser takes the raw text, break it down into tokens (depending on the tokensier vocabulary), add any special tokens depending on the model and then finally convert the tokens into input ids. \n",
    "![\"Tokeniser Workflow\"](data/chapter_2/tokeniser_workflow.png  \"The Tokeniser Workflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9173810",
   "metadata": {},
   "source": [
    "### Splitting text into tokens\n",
    "We can use the `tokenize()` method of a given tokeniser to simply convert the raw text into the tokensier vocabulary based tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e305952",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe88103c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['how', 'was', 'your', 'day', 'today', 'and', 'how', \"'\", 's', 'the', 'weather', 'there', '?']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(\"How was your day today and how's the weather there?\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c1b8ae",
   "metadata": {},
   "source": [
    "different tokeniser models follow different rules and conventions when converting the raw text into tokens. The example below we can see that `albert-base-v1` likes to put '*_*' infront of all the tokens that have space in front of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c6a1217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁you', '▁know', '▁in', '▁florence', ',', '▁in', '▁the', '▁summer', '▁it', '▁is', '▁really', '▁humid', '!']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('albert-base-v1')\n",
    "tokens = tokenizer.tokenize(\"You know in Florence, in the summer it is really humid!\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bbf27c",
   "metadata": {},
   "source": [
    "### token mapping\n",
    "to convert the tokens to their respective token ids we can use the `convert_tokens_to_ids()` function of the same tokeniser we used to convert the raw text to tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98ac43b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['can', \"'\", 't', 'think', 'of', 'an', 'example', 'or', 'what', 'to', 'ask']\n",
      "[2064, 1005, 1056, 2228, 1997, 2019, 2742, 2030, 2054, 2000, 3198]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokens = tokenizer.tokenize(\"Can't think of an example or what to ask\")\n",
    "print(tokens)\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d903f0",
   "metadata": {},
   "source": [
    "we can see that here we are atill missing the special tokens and one can add them by simply calling the `prepare_for_model()` method of that tokeniser. So,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "599e3281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2064, 1005, 1056, 2228, 1997, 2019, 2742, 2030, 2054, 2000, 3198, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "[CLS] can ' t think of an example or what to ask [SEP]\n"
     ]
    }
   ],
   "source": [
    "finalised_input = tokenizer.prepare_for_model(input_ids)\n",
    "print(finalised_input)\n",
    "\n",
    "decoded_text = tokenizer.decode(finalised_input['input_ids'])\n",
    "print(decoded_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
