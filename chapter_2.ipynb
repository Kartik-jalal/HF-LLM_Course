{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "220f39b7",
   "metadata": {},
   "source": [
    "# Replicating The pipeline() Function Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a590859",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\n",
    "    [\n",
    "        \"It is really hot here in Italy and going outside means being drenched in your own sweat and it does not feel good.\",\n",
    "        \"The weather in my hometown during summer is really nice!\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d3d1e4",
   "metadata": {},
   "source": [
    "When we run the above cell, we obtain\n",
    "```python\n",
    "[{'label': 'NEGATIVE', 'score': 0.9997081160545349},\n",
    " {'label': 'POSITIVE', 'score': 0.9998784065246582}]\n",
    "```\n",
    "Now under the hood, the pipeline function is conducting all the tasks neccessary to use the specified model (in this case it is the default model, which is *distilbert-base-uncased-finetuned-sst-2-english*) for the prediction. The steps are, *preprocessing (tokenisation)*, *then passing that input to the model*, and finally, *postprocessing*:\n",
    "<br />\n",
    "<br />\n",
    "![Pipeline Steps](data/chapter_2/pipeline_steps.png \"The Three Steps!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8bd522",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b994e298",
   "metadata": {},
   "source": [
    "### Preprocessing with a tokenizer\n",
    "\n",
    "Every model have their own way of splitting the text input into tokens, mapping them to interger and additing any additional input value that is needed for the model to process. Therefore, when fine_tuning/inference a pretrained model one needs to make sure the tokenisation methods fellows the same rule and it can be done by using the `from_pretrained()` method by the `AutoTokenizer` class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d199cdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdccf8d",
   "metadata": {},
   "source": [
    "now that we have the right tokeniser, we need to preprocess the raw input using it and pass the tensors (in simple term a tensor is basically an array which follows some strict rules) retuned by the tokeniser to the model. In the example below, we are asking the tokeniser to tokenise the raw input  and perform:\n",
    "- *padding* - adding non attention tokens to the shorter length text so that the length of all the input text become equal\n",
    "- *truncation* - remove the input text words that exceeds the input text length limit of the model, and \n",
    "- *return type* - the type of tensor to return (`return_tensors='pt'`, where `pt` means *pytorch tensor*).\n",
    "\n",
    "> Note: the use of *padding* and *truncation* mainly depends on the type of return object you asked from the tokeniser. If you would want a tensor, then having *padding* and *truncation* set is the right way because tensor only accept rectangular shapes (think matrices).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f374baea",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_inputs = [\n",
    "    \"It is really hot here in Italy and going outside means being drenched in your own sweat and it does not feel good.\",\n",
    "    \"The weather in my hometown during summer is really nice!\",\n",
    "]\n",
    "\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors='pt')\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed76078",
   "metadata": {},
   "source": [
    "As we can see, the output itself is a dictionary containing two keys, `input_ids` and `attention_mask`. \n",
    "- *input_ids*, numerical representations of your tokens. Here it contains two rows of integers (one for each sentence) and,\n",
    "- *attention_mask*, in simple term is to tell the model later which tokens needs attention.\n",
    "\n",
    ">  Note: there is also one key value instance that you get from the tokeniser, *token_type_ids*, these tell the model which part of the input is sentence A and which is sentence B."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5f829a",
   "metadata": {},
   "source": [
    "### Going through the model\n",
    "We use the `AutoModel` class this time to load the model by using its `from_pretrained()` function. And this way of using the basic *AutoModel* class, when we pass the input to the model it only returns a *hidden states*/*features*/*high-dimensional vector* which represents the contextual understanding of that input by the Transformer model and not a solid quantity that can be analysed, and that is beacause the model this way does not have the specific head that can convert the *hidden states*/*features*/*high-dimensional vector* into a quantity that can be analysed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b8b66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f61ce20",
   "metadata": {},
   "source": [
    "The returned *hidden state*, generally have three dimensions:\n",
    "- Batch size: The number of sequences processed at a time (2 in our example).\n",
    "- Sequence length: The length of the numerical representation of the sequence (16 in our example).\n",
    "- Hidden size: The vector dimension of each model inputb, i.e., basically the output values of the model's *hidden states*/*features*/*high-dimensional vector*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8e0f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714c5176",
   "metadata": {},
   "source": [
    "### Model heads: Making sense out of numbers\n",
    "So, a model head basically take the *hidden states*/*features*/*high-dimensional vector* model output (in this case the size of it is *768*) and project them onto a different dimension that can be analysed.\n",
    "<br />\n",
    "<br />\n",
    "![A Model Worflow](data/chapter_2/model_workflow.png \"A Model Worflow\")\n",
    ">In this diagram, the model is represented by its embeddings layer and the subsequent layers. The embeddings layer converts each input ID in the tokenized input into a vector that represents the associated token. The subsequent layers manipulate those vectors using the attention mechanism to produce the final representation of the sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cbbf35",
   "metadata": {},
   "source": [
    "Now, instead of *AutoModel* class, we will use the *AutoModelForSequenceClassification* which comes with the sequence classification head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbe5fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n",
    "outputs = model(**inputs)\n",
    "\n",
    "print(outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f27566e",
   "metadata": {},
   "source": [
    "Now, because the model already comes with a head this time, the dimensionality of the output will be much lower: now the outputs vectors containing two values (one per label, i.e., Negative, Positive).ince we have just two sentences and two labels, the result we get from our model is of shape 2 x 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af55893a",
   "metadata": {},
   "source": [
    "### Postprocessing The Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa345b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d293d979",
   "metadata": {},
   "source": [
    "However, even now the model's output does not make sense and that's because the outputs are *logits* (a logit is a raw, unnormalized score) and to convert the *logits* to the right numerical value we have to pass them through a *linear/non linear activation function* (i.e., a function that can normalise them), for example, **softmax**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaf2489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a405c20",
   "metadata": {},
   "source": [
    "Now we can see that the model predicted `[0.9971, 0.0002]` for the first sentence and `[0.0001, 0.9998]` for the second one, these are recognizable probability scores. Now, to get the labels corresponding to each position, we can inspect the id2label attribute of the model config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ade31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e7698d",
   "metadata": {},
   "source": [
    "Now we can conclude that the model predicted the following:\n",
    "\n",
    "- First sentence: NEGATIVE: 0.9971, POSITIVE: 0.0002 -> NEGATIVE\n",
    "- Second sentence: NEGATIVE: 0.0001, POSITIVE: 0.9998 -> POSITIVE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc157ef6",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab13f06a",
   "metadata": {},
   "source": [
    "Similarly to:\n",
    "```python\n",
    "from transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\"bert-base-cased\")\n",
    "```\n",
    "\n",
    "We can also directly load the model if we know the right model name.\n",
    "Furthermore, can also save a model in our local machine by calling the `save_pretrained()` method of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59df895f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "outputs  = model(**inputs)\n",
    "\n",
    "# save the model\n",
    "model.save_pretrained('data/chapter_2/bert_model')\n",
    "\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0bec3c",
   "metadata": {},
   "source": [
    "Two files get saved in the given directory *config.json* and *model.safetensors* that defines the saved model:\n",
    "- *config.json* - It stores all the necessary attributes needed to build the model architecture.\n",
    "- *model.safetensors* - It's the state dictionary; it contains all your model’s weights.\n",
    "\n",
    "\n",
    "Furthermore, we can load the model simply b calling the `from_pretrained()` method of `AutoModel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e94890",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained('data/chapter_2/bert_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c8fd01",
   "metadata": {},
   "source": [
    "# Tokenisation Encoding Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08f7a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04eee6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input = tokenizer(\"How are you?\", \"How's your holiday going?\")\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa99dd0c",
   "metadata": {},
   "source": [
    " Here we can see when we encode multiple sentences at once, the *token_type_ids* is also returned to specify which token id belongs to which sentence and that is because if we try to decode the input ids back to their orignal state of two sentences. It comes as a one long single sentence with some custom special tokens `[CLS]` and `[SEP]`, which are model specific and mostly used to tell the model the starting and ending of a sequence to be processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177963dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(encoded_input[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a590a68",
   "metadata": {},
   "source": [
    "Similarly to loading a know model directly from its class we can load a tokeniser as well without needing the `AutoModel` class. And similarly we can call the `save_pretrained()` to save the tokeniser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ba3ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "input = tokenizer(\"I stay till late in the library because it is open till late and I am not so productive when staying back in my room.\")\n",
    "\n",
    "tokenizer.save_pretrained(\"data/chapter_2/tokeniser\")\n",
    "\n",
    "print(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa12f842",
   "metadata": {},
   "source": [
    "# Tokeniser Pipeline\n",
    "We also know by now what happens when we call the `tokenzer()` to convert a text into machine readable input format. The tokeniser takes the raw text, break it down into tokens (depending on the tokensier vocabulary), add any special tokens depending on the model and then finally convert the tokens into input ids. \n",
    "<br />\n",
    "<br />\n",
    "![\"Tokeniser Workflow\"](data/chapter_2/tokeniser_workflow.png  \"The Tokeniser Workflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9173810",
   "metadata": {},
   "source": [
    "### Splitting text into tokens\n",
    "We can use the `tokenize()` method of a given tokeniser to simply convert the raw text into the tokensier vocabulary based tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e305952",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe88103c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(\"How was your day today and how's the weather there?\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c1b8ae",
   "metadata": {},
   "source": [
    "different tokeniser models follow different rules and conventions when converting the raw text into tokens. The example below we can see that `albert-base-v1` likes to put '*_*' infront of all the tokens that have space in front of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6a1217",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('albert-base-v1')\n",
    "tokens = tokenizer.tokenize(\"You know in Florence, in the summer it is really humid!\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bbf27c",
   "metadata": {},
   "source": [
    "### token mapping\n",
    "to convert the tokens to their respective token ids we can use the `convert_tokens_to_ids()` function of the same tokeniser we used to convert the raw text to tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ac43b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokens = tokenizer.tokenize(\"Can't think of an example or what to ask\")\n",
    "print(tokens)\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d903f0",
   "metadata": {},
   "source": [
    "we can see that here we are atill missing the special tokens and one can add them by simply calling the `prepare_for_model()` method of that tokeniser. So,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599e3281",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalised_input = tokenizer.prepare_for_model(input_ids)\n",
    "print(finalised_input)\n",
    "\n",
    "decoded_text = tokenizer.decode(finalised_input['input_ids'])\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6169f9",
   "metadata": {},
   "source": [
    "# Handling Multiple Sequences\n",
    "\n",
    "When we are using a transformer model, we need to make sure that the input we passed to the main needs to be in a batch (i.e., inside a list) no matter the total number of actual text sequences to process by it. And that is because the architure of a transformer model is built in a way that it only accepts a batch of inputs as a input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2f6e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = 'The weather after midnight is really nice!'\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "ids = tokenizer.prepare_for_model(ids) \n",
    "input_ids = torch.tensor(ids['input_ids'])\n",
    "\n",
    "print(input_ids)\n",
    "\n",
    "model(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334fba95",
   "metadata": {},
   "source": [
    "in the code above the last model line failed because it was expecting a input like `tensor([[input ids, ..], ....])` (i.e., a tensor batch) but it received a single dimension `tensor([input ids, ..])`. Therefore we need to add the input_ids in a list first and then convert it to a torch tensor and then pass it to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50b96bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = 'The weather after midnight is really nice!'\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "ids = tokenizer.prepare_for_model(token_ids) \n",
    "input_ids = torch.tensor([ids['input_ids']])\n",
    "\n",
    "print(input_ids)\n",
    "model(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420b754a",
   "metadata": {},
   "source": [
    "So now we can add more input ids to our tensor batch and the model will predict for them as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ce1f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(\n",
    "    [ids['input_ids'], ids['input_ids']]\n",
    ")\n",
    "\n",
    "print(input_ids)\n",
    "model(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedee7fc",
   "metadata": {},
   "source": [
    "Now if you remember earlier we also talked about padding and turnication. And to find out what kind of padding token we need to add for shorter text input and how long a sequence needs to be before turnication, we have to call the tokeniser `tokenizer.pad_token_type_id`, `tokenizer.special_tokens_map['pad_token']` and `tokenizer.max_len_single_sentence`. However, then remember in the case of padding we also have to pass the attention mask so that the model will know to not give attention to the padding tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0ec77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_id = tokenizer.pad_token_type_id\n",
    "padding_token = tokenizer.special_tokens_map['pad_token']\n",
    "max_length = tokenizer.max_len_single_sentence\n",
    "print(f\"padding token '{padding_token}' and max length per sequence '{max_length}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18350d7e",
   "metadata": {},
   "source": [
    "For simplicity, in the below example we will only look at padding - based on max input sequence, turnication - based on max tokenizer allowd length, and attention mask but only considering the `[PAD]` token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fddcf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = ['The weather after midnight is really nice!', 'How are you?']\n",
    "\n",
    "# turnicate based on the allowed max length\n",
    "sequences = [sequence[:max_length] for sequence in sequences]\n",
    "\n",
    "# converting to tokens\n",
    "sequences_tokens = [tokenizer.tokenize(sequence) for sequence in sequences]\n",
    "print(f\"token:\\n{sequences_tokens}\")\n",
    "\n",
    "# add padding based on max token length sequnece\n",
    "max_token_length_sequence  = len(max(sequences_tokens, key=len))\n",
    "for sequence_tokens in sequences_tokens:\n",
    "    if len(sequence_tokens) == max_token_length_sequence:\n",
    "        continue\n",
    "    \n",
    "    num_of_pad_token = max_token_length_sequence - len(sequence_tokens)\n",
    "    sequence_tokens.extend([padding_token]*num_of_pad_token)\n",
    "\n",
    "print(f\"Padded tokens: {sequences_tokens}\")\n",
    "\n",
    "# converting to token ids\n",
    "sequences_token_ids = [tokenizer.convert_tokens_to_ids(sequence_token) for sequence_token in sequences_tokens]\n",
    "print(f\"Token ids :\\n{sequences_token_ids}\")\n",
    "\n",
    "# adding special character and getting the attention mask\n",
    "inputs = [tokenizer.prepare_for_model(sequence_token_ids) for sequence_token_ids in sequences_token_ids]\n",
    "    \n",
    "print(f\"Added special characters and attention mask:\\n{inputs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4fb68f",
   "metadata": {},
   "source": [
    "Here if we look at the attention mask for the second sequence it has 1 for all the token ids but that is wrong because we know we have added some padding ids there and instead of this `[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]` it should look like `[1, 1, 1, 1, 1, 0, 0, 0, 0, 1]`. This is one of drawback of doing all of this manually that you also have to create the attention mask manually with having special character in mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7cf1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixing attention mask\n",
    "for i in range(len(inputs)):\n",
    "    attention_mask = []\n",
    "    for input_id in inputs[i]['input_ids']:\n",
    "        if input_id == padding_id:\n",
    "            attention_mask.append(0)\n",
    "        else:\n",
    "            attention_mask.append(1)\n",
    "\n",
    "    # update the attention_mask\n",
    "    inputs[i]['attention_mask'] = attention_mask\n",
    "\n",
    "\n",
    "print(f\"Updated inputs:\\n {inputs}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f06499c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_ids = torch.tensor([item['input_ids'] for item in inputs])\n",
    "attention_masks = torch.tensor([item['attention_mask'] for item in inputs])\n",
    "\n",
    "outputs = model(batched_ids, attention_mask=attention_masks)\n",
    "print(f\"Logits:\\n\\t{outputs.logits}\\n\\n\")\n",
    "\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "print(f\"Prediction {model.config.id2label}:\\n\\t {predictions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcef36b",
   "metadata": {},
   "source": [
    "So,\n",
    "\n",
    "| Sequence | Negative | Positive | Prediction |\n",
    "|----------|----------|----------|------------|\n",
    "|    1     |   0.001  |    0.9   | Positive   |\n",
    "|    2     |   0.9    |    0.02  | Negative   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e1ec2e",
   "metadata": {},
   "source": [
    "# Putting It All Togther\n",
    "\n",
    "Now that we have seen all the individual steps, we actually don't need to remember them because the *transformers* API methods handles all of this of us if we call them directly instead of calling their in-built methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824e9593",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "checkpoint = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequences = ['The weather after midnight is really nice!', 'How are you?']\n",
    "\n",
    "# the padding can be set to \"True\"/\"longest\" and \"max-length\" with seprate max_length parameter which takes an int\n",
    "tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors='pt')\n",
    "output = model(**tokens)\n",
    "print(output.logits)\n",
    "\n",
    "prediction = torch.nn.functional.softmax(output.logits, dim=-1)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017b7465",
   "metadata": {},
   "source": [
    "> Note: for  `torch.nn.functional.softmax(output.logits, dim=-1)` by specifying `dim=-1` we are telling the softmax function to perform the softmax on the last dimension of the tensor which are  `[-4.1945,  4.5471]` and `[1.9830, -1.5163]` in out example above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b3a037",
   "metadata": {},
   "source": [
    "# Optimized Inference Deployment\n",
    "\n",
    "Now that we know the fundamentals of how the LLM works. Let's now have a quick overview of how can you deploy these model in production enviorment for users to use them for inference.\n",
    "\n",
    "There are three advanced frameworkds for optimising LLM deployments:\n",
    "- **Text Generation Inference** (**TGI**)\n",
    "- **vLLM**\n",
    "- **llama.cpp**\n",
    "\n",
    "|           |  Memory Management and Performance  | Deployment and Integration  |\n",
    "|-----------|-------------------------------------|-----------------------------|\n",
    "| *TGI*       | Uses **Flash Attention 2**, optimizing memory and speed, suitable for fast inference. | Built-in production-ready monitoring and security, stable and secure deployments.  |\n",
    "| *vLLM*      | Uses **PagedAttention**, enabling efficient memory management for faster inference on large models. | Highly customizable and developer-friendly, good for scenarios requiring adaptability and optimization.  |\n",
    "| *llama.cpp* | Utilizes **quantization techniques**, making it suitable for resource-limited environments. | Focuses on simplicity, ease of use, and portability, ideal for lightweight, diverse deployments. |\n",
    "\n",
    "If you would like to see how to deploy using these methods have a look at [HuggingFace - Optimized Inference Deployment](https://huggingface.co/learn/llm-course/chapter2/8?fw=pt)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a55675",
   "metadata": {},
   "source": [
    "# The End!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
