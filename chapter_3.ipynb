{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bacc729e",
   "metadata": {},
   "source": [
    "# Processing The Data\n",
    "\n",
    "To access a dataset we will use the `datasets` lib and for this example we will be using the *MRPC* (*Microsoft Research Paraphrase Corpus*) dataset. The dataset consists of 5,801 pairs of sentences, with a label indicating if they are paraphrases or not (i.e., if both sentences mean the same thing). Furthermore, this is one of the 10 datasets composing the *GLUE* benchmark, which is an academic benchmark that is used to measure the performance of ML models across 10 different text classification tasks. So, we can use this dataset to finetune a *Bert* model (`bert-base-uncased`) to classify paraphrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862ad1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# download the mrpc dataset\n",
    "raw_datasets = load_dataset(\n",
    "    path=\"glue\",\n",
    "    name=\"mrpc\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4617ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadec1ea",
   "metadata": {},
   "source": [
    "As we can see, we get a `DatasetDict` object, which contains three datasets in it, one for training, one for validation and one of testing. Each of them have '*sentence1*', '*sentence2*', '*label*' and '*ids*' as their columns and there are *3668* rows in the training dataset, *408* rows in the validation dataset and *1725* rows in the testing dataset.\n",
    "\n",
    "Now to access any of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d5f961",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_dataset = raw_datasets['train']\n",
    "raw_train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec133067",
   "metadata": {},
   "source": [
    "we see the label column has a integer value of 1, now to see what it corresponds to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e074609",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_dataset.features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd5f700",
   "metadata": {},
   "source": [
    "As we can see, the label column is of type *ClassLabel* and **0** corresponds to **not_equivalent**, and **1** corresponds to **equivalent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db35f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(\"Data at index 15 in the training dataset:\") \n",
    "pprint(raw_train_dataset[15]) \n",
    "print(\"\\nat index 800:\") \n",
    "pprint(raw_train_dataset[800])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25a809f",
   "metadata": {},
   "source": [
    "> Note: As we can see from above the index value when accessing the data  `raw_train_dataset[15]` is not always as same as the *idx* value and that could be because of how the whole main dataset was split into train/valid/test datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d884376f",
   "metadata": {},
   "source": [
    "# Preprocessing The Data\n",
    "\n",
    "In our data we have two sequences as a pair that needs to be processed by the model for classification. But that also means that the tokeniser have to convert the sequences into tokens as a pair and the good thing is the tokensier does that for us by itself, we simply have to pass them togther:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5362778",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "inputs = tokenizer(\"This is the first sentence.\", \"This is the second one.\")\n",
    "pprint(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360e4709",
   "metadata": {},
   "source": [
    "This time we see that the tokeniser returns an additional feature *token_type_ids*,  this is what tells the model which part of the input is the first sentence and which is the second sentence. So,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091e815e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(inputs['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8d0774",
   "metadata": {},
   "source": [
    "```python\n",
    "['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']\n",
    "[      0,      0,    0,     0,       0,          0,   0,       0,      1,    1,     1,        1,     1,   1,       1]\n",
    "```\n",
    "\n",
    "But as we saw earlier, the test dataset alone has 3668 data in it and even though the data is not really big, fitting the data directly to the tokensier is not a good practice because it can easily cause *RAM Out-Of-Memory* issue. And also passing only the sequences to the tokeniser will only return the `input_ids`, `attention_mask`, and `token_type_ids` as the input for the model and this way we will lose other important info that we had in out orignal dataset like `label`. Therefore, we will use `datasets` in-built `map()` method. The map() method works by applying a function on each element of the dataset, so let’s define a function that tokenizes our inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9166b9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokeniser_function(data):\n",
    "    return tokenizer(data['sentence1'], data[\"sentence2\"], truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ca00eb",
   "metadata": {},
   "source": [
    "This function takes a data dictionary and returns a new dictionary with the keys `input_ids`, `attention_mask`, and `token_type_ids`. This will allow us to use the option `batched=True` in our call to `map()`, which will greatly speed up the tokenisation, because this tokeniser can be very fast, but only if we give it lots of inputs at once and using `batched=True` in our call to `map()` passes multiple elements of our dataset at once to the `tokeniser_function()`, and not on each element separately\n",
    "\n",
    "Furthermore, we can also see that we've left the `padding` parameter out in our tokenisation function for now. This is because padding all the samples to the maximum length is not efficient: it’s better to pad the samples when we’re building a batch, as then we only need to pad to the maximum length in that batch, and not the maximum length in the entire dataset. This can save a lot of time and processing power when the inputs have very variable lengths!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5c9db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_datasets = raw_datasets.map(tokeniser_function, batched=True)\n",
    "tokenizer_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cfc990",
   "metadata": {},
   "source": [
    "As we can see, we haven't lost any columns from out data and that there are new columns added (Note that we could also have changed existing fields if our preprocessing function returned a new value for an existing key in the dataset to which we applied `map()`).\n",
    "\n",
    " Also it was really quick to process as well, however, you can make the whole process more faster by passing `num_proc` argument to the `map()` function, as this allows multiprocessing, but since `tokenizers()` already works on multiple threads, there is no use of it here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329096d1",
   "metadata": {},
   "source": [
    "# Dynamic padding\n",
    "Now we will need to do is pad all the examples to the length of the longest element when we batch elements together before passing the input to the model. This technique is refer as **dynamic padding**.\n",
    "\n",
    "Even though this way of padding makes things go faster when utalising a CPU or GPU, that is always not the case when using a accelerator resource like a TPU and that is because TPUs prefer fixed shapes, even when that requires extra padding.\n",
    "\n",
    "![\"Dynamic Padding\"](data/chapter_3/dynamic_padding.png \"Dynamic Padding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f892b0",
   "metadata": {},
   "source": [
    "Here we will use function that is responsible for putting together samples inside a batch, AKA **collate function**. The *collate function* will also apply the correct amount of padding to the items of the dataset we want to batch together. Fortunately, the Transformers library provides us with such a function via `DataCollatorWithPadding`. It takes a tokenizer when you instantiate it (to know which padding token to use, and whether the model expects padding to be on the left or on the right of the inputs) and will do everything you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275383c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1218be0",
   "metadata": {},
   "source": [
    "let's try it on a subset of out dataset and assume it as a single batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3289a128",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = tokenizer_datasets[\"train\"][:10]\n",
    "# filter out the unnecessary columns\n",
    "samples = {k: v for k, v in samples.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"]}\n",
    "[len(x) for x in samples[\"input_ids\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dd948a",
   "metadata": {},
   "source": [
    "Here we can see inside this batch the squences are of different lengths, and to add padding to make them all to the max-length inside this particular batch (which is 67), we simply need to pass the samples to the `data_collator()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6abbb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = data_collator(samples)\n",
    "pprint({k: v.shape for k, v in batch.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ce9149",
   "metadata": {},
   "source": [
    "### Example\n",
    "Let's try to preprocess the **GLUE SST-2** dataset:\n",
    "![\"The GLUE SST-2 Datset\"](data/chapter_3/glue_sst2.png \"The GLUE SST-2 Datset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59835540",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\n",
    "    path='glue',\n",
    "    name='sst2'\n",
    ")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005e5656",
   "metadata": {},
   "source": [
    "As we can see, there is only one sentence per data. So, the `tokenisation_function()` only need to work one sentence per data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f97fc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = 'bert-base-uncased'\n",
    "\n",
    "tokeniser = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenisation_function(data):\n",
    "    return tokeniser(data['sentence'], truncation=True)\n",
    "\n",
    "tokenised_datasets = raw_datasets.map(tokenisation_function, batched=True)\n",
    "tokenised_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b0a05b",
   "metadata": {},
   "source": [
    "Now let's clean the data by removing the unnecessary columns and changing the name of the *label* columns to *labels* and finally convert the datatype to torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb80a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenised_datasets = tokenised_datasets.remove_columns(\n",
    "    column_names=['idx', 'sentence']\n",
    ")\n",
    "tokenised_datasets = tokenised_datasets.rename_column(\n",
    "    original_column_name='label',\n",
    "    new_column_name='labels'\n",
    ")\n",
    "tokenised_datasets = tokenised_datasets.with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8d135f",
   "metadata": {},
   "source": [
    "Now to apply padding using the `data_collator` on the bases of batch we will use the `DataLoader()` function from the class `torch.utils.data`, which creates batches on the given data and passing it to the **collate function**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdff0492",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokeniser)\n",
    "train_dataloader = DataLoader(\n",
    "    tokenised_datasets['train'],\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "for step, batch in enumerate(train_dataloader):\n",
    "    print(batch[\"input_ids\"].shape)\n",
    "    if step > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a908b881",
   "metadata": {},
   "source": [
    "# Fine-tuning a model with the Trainer API\n",
    "\n",
    "The Transformer library provides a **Trainer** API, that allows us to easily fine-tune a transformer model on any dataset. It simply takes the *model*, *dataset*, \n",
    "*training hyperparameter* and can perform training using any types of available resources, *CPU*, *GPUs* and *TPUs*. It can also compute predictions and also evaluate the model if the metrics is provided. Furthermore, it can also handle the later stages of data preprocessing for use, such as, *dynamic padding*, given the *tokeniser* and the *data collator*  are provided.\n",
    "\n",
    "![\"The Trainer API\"](data/chapter_3/trainer.png \"The Trainer API\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e017b51",
   "metadata": {},
   "source": [
    "Now let's run an example on the **Trainer** API using the same *GLUE MRPC* dataset.\n",
    "\n",
    "> Note: If you don’t have a GPU set up, you can get access to free GPUs or TPUs on [Google Colab](https://colab.research.google.com/), as otherwise, it will run very slowly on a CPU.\n",
    "\n",
    "\n",
    "Let's first set-up the preprocessing process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce43e574",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "raw_datasets = load_dataset('glue', 'mrpc')\n",
    "checkpoint = 'bert-base-uncased'\n",
    "tokeniser = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenisation_function(data):\n",
    "    return tokeniser(data['sentence1'], data['sentence2'], truncation=True)\n",
    "\n",
    "tokenised_datasets = raw_datasets.map(tokenisation_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokeniser)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f9ecdd",
   "metadata": {},
   "source": [
    "### Training\n",
    "First we set the *hyperparameters* that the *trainer* needs to know, to make sure the training and evaluation goes the right way. So we will use the **TrainingArguments** class by *transformers* to define all the *hyperparameters*. For this example, we only need to give the path to where save the model, as well as the checkpoints along the way.For all the rest, you can leave the defaults, which should work pretty well for a basic fine-tuning. <br />\n",
    "Furthermore, If you want to automatically upload your model to the Hub during training, pass along push_to_hub=True in the TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7761ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='data/chapter_3/model/'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f48221c",
   "metadata": {},
   "source": [
    "Now we will define our model. However, this time when we specify to the **AutoModelForSequenceClassification** class that we want the checkpoint of '*bert-base-uncased*' model, we will get a *warning*. This is because *bert-base-uncased* has not been pretrained on classifying sequence, so the head of the pretrained model has been discarded and a new head suitable for sequence classification has been added instead. The warnings indicate that some weights were not used (the ones corresponding to the dropped pretraining head) and that some others were randomly initialized (the ones for the new head). It concludes by encouraging you to train the model, which is exactly what we are going to do now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15cb55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1053692",
   "metadata": {},
   "source": [
    "Now we will pass all what we have defined so far, `model`, `training_args`, `tokenised_dataset[train]`, `tokenised_dataset[validation]`, `data_collator`, and the `tokeniser` as the processing_class parameter (it simply tells the *Trainer* which tokeniser to use for processing).\n",
    "> Note: When you pass a tokenizer as the processing_class, the default data_collator used by the Trainer will be a DataCollatorWithPadding. You can skip the data_collator=data_collator line in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55115cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenised_datasets['train'],\n",
    "    eval_dataset=tokenised_datasets['validation'],\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokeniser\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e42a97",
   "metadata": {},
   "source": [
    "Now the main bit, to fine-tune the model on our dataset, we just have to call the `train()` method of our *Trainer*. However, this will only report the training loss every 500 steps and won't give any evaluation info, and that is because when we initialised the `TrainingArguments` we didn't define the `eval_strategy` to either *steps* or *epoch* (i.e., at the end of every n steps or end of an epoch evaluate) and also for the `Trainer` we didn't define the `compute_metrics()` function which will also on every *evaluation stage* compute a metric to descript the performance of the model (this is necassary because otherwise at every *evaluation stage* we would just got the loss, which is not a very intuitive number)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2efdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6decfe2",
   "metadata": {},
   "source": [
    "# Evalution\n",
    "\n",
    "Let's build the `compute_metrics()` function to see a detailed evaluation info at the end every *epoch*. Now, for this to work the `compute_metric()` function needs to take the `EvalPrediction` object, that is retuned by the `Trainer` either when its `predict()` function gets called up or it's performing an evaluation. The `EvalPrediction` object basically contains three fields `predictions` and `label_ids` and `metrics`; `predictions` contains the logits for every predicton, `label_ids` tells the original label that comes with the data itself (not in the case of inference) and `metrics` is a dictionary, where the key is the names of the metrics returned its value is floats quantity. Once we complete our `compute_metrics()` function and pass it to the Trainer, the `metrics` field will also contain the metrics returned by `compute_metrics()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91a8cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "predictions = trainer.predict(tokenised_datasets[\"test\"])\n",
    "print(predictions._fields)\n",
    "\n",
    "print(predictions.predictions.shape, predictions.label_ids.shape)\n",
    "pprint(predictions.metrics)\n",
    "\n",
    "pprint(predictions.predictions[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e3ee3f",
   "metadata": {},
   "source": [
    "As you can see, predictions is a two-dimensional array with shape 408 x 2 (408 being the number of elements in the dataset we used and 2 being the logits predicted of every 408 elements).Now, to transform them into predictions that we can compare to our labels, we need to take the index where the logits are at (i.e., that last index '-1' in our case) and take the index of the  maximum logit value out of the 2 (taking the maximum value here is only due to becaue we have simple single hard label classification, and we can also use a softmax or other probabilistic function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee2250a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "preds = np.argmax(predictions.predictions, axis=-1)\n",
    "preds[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee5504e",
   "metadata": {},
   "source": [
    "We can now compare the original labels `predictions.label_ids` with the predicted one from the above. To further evaluate the results based over matrics, we will rely on the metrics from the `Evaluate` library. We can load the metrics associated with the MRPC dataset as easily as we loaded the dataset, this time with the `evaluate.load()` function. The object returned has a `compute()` method we can use to do the metric calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e38af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(path=\"glue\", config_name=\"mrpc\")\n",
    "metric.compute(predictions=preds, references=predictions.label_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde8dfb6",
   "metadata": {},
   "source": [
    "Here, we can see our model has an accuracy of *85.62%* on the validation set and an F1 score of *89.54*. Those are the two metrics used to evaluate results on the *MRPC* dataset for the *GLUE* benchmark. The table in the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf) reported an F1 score of 88.9 for the base model. That was the **uncased** model while we are currently using the **cased** model, which explains the better result.\n",
    "\n",
    "Now, let's wrap everything inside our `compute_metrics()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd7e57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "    pred_logits, labels = eval_preds\n",
    "    preds = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    return metric.compute(predictions=preds, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d14e80",
   "metadata": {},
   "source": [
    "Let's now upgrade our `TrainingArguments()` with `eval_strategy` set to **epoch** and pass our `compute_metrics()` to the compute_metrics parameter of the `Trainer()`. So now the new `model` we initialised below will get evaluated over the metrics define in `compute_metrics()` at the end of every epoch cycle.\n",
    "> Note: this time we won't pass `data_collator` because as we discussed previously we are already passing the `tokenizer` for the `processing_class` parameter meaning the `Trainer()` by itself will use `DataCollatorWithPadding` with the  `tokenizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a337d2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments('data/chapter_3/model', eval_strategy=\"epoch\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenised_datasets[\"train\"],\n",
    "    eval_dataset=tokenised_datasets[\"validation\"],\n",
    "    processing_class=tokeniser,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6207a26e",
   "metadata": {},
   "source": [
    "Furthermore, the `Trainer` comes with many built-in features that make modern deep learning best practices accessible: \n",
    "\n",
    "**Mixed Precision Training**: Use fp16=True in your training arguments for faster training and reduced memory usage:\n",
    "\n",
    "```python\n",
    "training_args = TrainingArguments(\n",
    "    \"test-trainer\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    fp16=True,  # Enable mixed precision\n",
    ")\n",
    "```\n",
    "\n",
    "> Mixed precision training uses 16-bit floats for forward pass and 32-bit for gradients, improving speed and reducing memory usage.\n",
    "\n",
    "**Gradient Accumulation**: For effective larger batch sizes when GPU memory is limited:\n",
    "\n",
    "```python\n",
    "training_args = TrainingArguments(\n",
    "    \"test-trainer\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,  # Effective batch size = 4 * 4 = 16\n",
    ")\n",
    "```\n",
    "> This allows you to simulate larger batch sizes by accumulating gradients over multiple forward passes.\n",
    "\n",
    "**Learning Rate Scheduling**: The Trainer uses linear decay by default, but you can customize this:\n",
    "\n",
    "```python\n",
    "training_args = TrainingArguments(\n",
    "    \"test-trainer\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    lr_scheduler_type=\"cosine\",  # Try different schedulers\n",
    ")\n",
    "```\n",
    "\n",
    ">Note: Modern features like mixed precision (fp16=True) and gradient accumulation can significantly improve training efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54349686",
   "metadata": {},
   "source": [
    "#  A Full Training Loop in PyTorch\n",
    "\n",
    "Writing you own training loop without relying on the *Trainer API* is quite essential, because this way, we can easily customise each steps of the training loop to our needs and easier to debug.\n",
    "\n",
    "![\"Full Training Loop\"](data/chapter_3/full_training_loop.png \"Full Training Loop\")\n",
    "\n",
    "During training, in simple words, the **model**, a giant *formula* with many adjustable knobs (**model's weights**), is shown lots of *flash-cards* that pair an input with the correct answer. The *model* makes a guess for each *card*, and a single number called the **loss** tells the *model* how wrong that guess was. Using *clever math*, the model work out which knobs should turn and by how much (i.e., the **gradients**) to shrink the loss, and an **optimizer** performs those tiny adjustments. We then move to the next card and repeat the *cycle—guess*, measure *error*, tweak—thousands of times. Over all these loops the *model* gradually tunes its knobs so that its guesses become consistently accurate, meaning it has learned the **patterns hidden** in the training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1b4e9b",
   "metadata": {},
   "source": [
    "Let's first prepare the data as same as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409accf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokeniser = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenisation_function(data):\n",
    "    return tokeniser(data[\"sentence1\"], data[\"sentence2\"], truncation=True)\n",
    "\n",
    "tokenised_datasets = raw_datasets.map(tokenisation_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokeniser)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2cf0f8",
   "metadata": {},
   "source": [
    "Now we will let refine the dataset by removing the unnecessary columns and converting the '*label*' column name to '*labels*' and converting the data format to *torch* because that is what the model takes as the conventional input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330f4392",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenised_datasets = tokenised_datasets.remove_columns(\n",
    "    column_names=[\"sentence1\", \"sentence2\", \"idx\"]\n",
    ")\n",
    "tokenised_datasets = tokenised_datasets.rename_column(\n",
    "    original_column_name=\"label\",\n",
    "    new_column_name=\"labels\"\n",
    ")\n",
    "tokenised_datasets.set_format(\"torch\")\n",
    "# let's see\n",
    "tokenised_datasets[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56972ca3",
   "metadata": {},
   "source": [
    "Now let's devide the data in batches using the `DataLoader` lib with our *collate* function for the right padding:\n",
    "> Note: For the training dataset we will have  `suffle = True`, to makes sure during each epoch the data inside the batches get reshuffled. These kind of small tweaks that changes the postion of the data can make significant performance improvement when it comes to machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50e010b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenised_datasets[\"train\"], shuffle=True, batch_size=8, collate_fn=data_collator\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenised_datasets[\"validation\"], batch_size=8, collate_fn=data_collator\n",
    ")\n",
    "\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    pprint({k: v.shape for k, v in batch.items()})\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e36f70",
   "metadata": {},
   "source": [
    "Let's now initialise our *model*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173f998f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ecf4b4",
   "metadata": {},
   "source": [
    "For testing let's pass a batch to the model:\n",
    "\n",
    "> Note: All Transformers models will return the loss when labels are provided, and we also get the logits (two for each input in our batch, so a tensor of size 8 x 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cbbf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dataloader:\n",
    "    outputs = model(**batch)\n",
    "    break\n",
    "\n",
    "print(outputs.loss, outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850cee85",
   "metadata": {},
   "source": [
    "Now, let's define a *optimiser*, we will use **AdamW** which is also the default *optimser* that is used by the *Trainer API*: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7819883",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimiser = AdamW(\n",
    "    params=model.parameters(),\n",
    "    lr=5e-5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1521dba5",
   "metadata": {},
   "source": [
    "As we can see, we have passed two parameters to the *AdamW* optimiser, `params` - he model’s weights (the “knobs”) that the optimizer will adjust, and `lr` - the learning rate, which sets how big each step is when we move the weights in the direction suggested by the gradient.\n",
    "\n",
    "The learning rate controls how big a jump the optimizer makes when it updates the model’s weights, and that ideal jump size changes during training: early on, large jumps help the model move quickly toward a good region of solutions, but once it gets close, those same large jumps can cause it to overshoot or wobble around the optimum; conversely, a small fixed learning rate might be safe near the end but would make early progress painfully slow. Now to overcome the issue that comes with `lr`, we need a *Learning Rate Sacheduler*, which will solves this by starting with a higher learning rate for fast initial learning and then gradually lowering it so the optimizer can take finer, steadier steps as it zeroes in on the best weights. For this we will use the `get_scheduler` by `transformers` and we will use a *linear decay* (i.e., a straight line decay towards 0 from `5e-5`). To properly define it, we need to know the number of training steps we will take, which is the number of epochs we want to run multiplied by the number of training batches (which is the length of our training dataloader). The Trainer uses three epochs by default, so we will follow that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f755e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimiser,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "print(num_training_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6997ea6",
   "metadata": {},
   "source": [
    "### The training loop\n",
    "\n",
    "One last thing: we will want to use the GPU if we have access to one (on a CPU, training might take several hours instead of a couple of minutes and I would suggest using *Google Colab* for the free GPU use there). To do this, we define a device we will put our model and our batches on:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c52e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41bd778",
   "metadata": {},
   "source": [
    "Now let's write the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2ecf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a handy progress-bar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Tell the progress-bar, how many total update steps we expect.\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "# Put the model in *training* mode extra training configs (like turns on dropout, etc. ).\n",
    "model.train()\n",
    "\n",
    "# go through the entire dataset several times (“epochs”)\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # fetch a batch\n",
    "    for batch in train_dataloader:\n",
    "        # Move every tensor in the batch onto the GPU (or CPU) we’re using.\n",
    "        # `k` is the column name (e.g. \"input_ids\"), `v` is the tensor itself.\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        # Forward pass → the model makes a prediction and\n",
    "        # also returns the loss (how wrong the prediction was).\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass → compute gradients (how each weight\n",
    "        # should change to reduce that loss).\n",
    "        loss.backward()\n",
    "\n",
    "        # Take one optimizer step:\n",
    "        #   • Looks at the gradients\n",
    "        #   • Nudges the weights (\"knobs\") in the right direction\n",
    "            # First,\n",
    "            # Put a speed-limit on the update: if the combined size (L2-norm) of all gradients\n",
    "            # exceeds 1.0, scale them down so it is exactly 1.0.  This “gradient clipping”\n",
    "            # prevents an out-of-control batch from giving the model an enormous, unstable shove.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            # Then, step\n",
    "        optimiser.step()\n",
    "        \n",
    "        # Update the learning-rate scheduler so the “step size”\n",
    "        # gets a bit smaller as training progresses.\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        # Reset gradients to zero so they don’t accumulate\n",
    "        # into the next batch.\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        # Tell tqdm we’ve finished one training step\n",
    "        # so it can advance the progress bar.\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0800c2",
   "metadata": {},
   "source": [
    "Now let's add the evaluation loop as well to get some metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76909d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "\n",
    "# Put the model in *evaluation* mode and turn off extra training configs (like turns off dropout, etc. ).\n",
    "model.eval()\n",
    "\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "    # We’re only doing a forward pass, so turn off gradient tracking to\n",
    "    # save memory and speed things up.\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        references = batch[\"labels\"]\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        # Hand this batch’s predictions + references to the metric object;\n",
    "        # it will store them internally until we ask for the final score.\n",
    "        metric.add_batch(predictions=predictions, references=references)\n",
    "\n",
    "# Calculate and return the overall Accuracy and F1 once all batches are processed.\n",
    "metric.compute()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c61019c",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Let's try to build the training loop again but this time for the **SST-2** data and also evaluate the model after every epoch during the training (for the *SST-2* dataset only the *accuracy* metric is provided by *GLUE*), because this is one of the reasons why epoch idea is used when training so that you can evalute the model performance on every epoch and if its performance starts going bad we stop the training and take the last best performance epoch model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6efa03",
   "metadata": {},
   "source": [
    "Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f318153",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "raw_datasets = load_dataset(path=\"glue\", name=\"sst2\")\n",
    "\n",
    "pprint(raw_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01c5627",
   "metadata": {},
   "source": [
    "*tokenise* the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03cbf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"bert-base-uncased\"\n",
    "tokeniser = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenisation_function(data):\n",
    "    return tokeniser(data[\"sentence\"], truncation=True)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokeniser)\n",
    "\n",
    "tokenised_datasets = raw_datasets.map(tokenisation_function, batched=True)\n",
    "\n",
    "pprint(tokenised_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c699f7",
   "metadata": {},
   "source": [
    "refine the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a3b995",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenised_datasets = tokenised_datasets.remove_columns(\n",
    "    column_names=[\"sentence\", \"idx\"]\n",
    ")\n",
    "tokenised_datasets = tokenised_datasets.rename_column(\n",
    "    original_column_name=\"label\",\n",
    "    new_column_name=\"labels\"\n",
    ")\n",
    "tokenised_datasets.set_format(\"torch\")\n",
    "tokenised_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff048307",
   "metadata": {},
   "source": [
    "intialise the *dataloader*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5226ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=tokenised_datasets[\"train\"],\n",
    "    batch_size=30,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    dataset=tokenised_datasets[\"validation\"],\n",
    "    batch_size=30,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    pprint({k : v.shape for k, v in batch.items()})\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab0834b",
   "metadata": {},
   "source": [
    "initialise the *model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890da601",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    outputs = model(**batch)\n",
    "    print(outputs.loss, outputs.logits.shape)\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9052b6",
   "metadata": {},
   "source": [
    "set the *optimiser* and *learning rate scheduler*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f670fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "\n",
    "optimiser = AdamW(\n",
    "    params=model.parameters(),\n",
    "    lr=5e-5\n",
    ")\n",
    "\n",
    "num_epochs = 2\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimiser,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "print(num_training_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc63f5d0",
   "metadata": {},
   "source": [
    "put the *model* on the avaiable *device*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bbe30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd99071",
   "metadata": {},
   "source": [
    "create a *evaluation* function that will get called everytime a epoch is finsihed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b37a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "def perform_evaluation():\n",
    "    metric = evaluate.load(\"glue\", \"sst2\")\n",
    "\n",
    "    # set the model to evaluation\n",
    "    model.eval()\n",
    "\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k : v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "            logits = outputs.logits\n",
    "            refs = batch[\"labels\"]\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            metric.add_batch(predictions=preds, references=refs)\n",
    "\n",
    "    pprint(metric.compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4103b1d3",
   "metadata": {},
   "source": [
    "the final *training* loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe78757",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimiser.step()\n",
    "\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    perform_evaluation()\n",
    "    # we need to set it back to training mode \n",
    "    # because during training it gets put in \n",
    "    # eval() mode\n",
    "    model.train()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599c4cbe",
   "metadata": {},
   "source": [
    "let's test the evaluation, it should print same as the last line of above cell output here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d8d5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e8ae53",
   "metadata": {},
   "source": [
    "# Supercharge your training loop with Accelerate\n",
    "\n",
    "The training loop we defined earlier works fine on a single CPU or GPU. But using the `Accelerator` from the `Accelerate` library, with just a few adjustments we can enable distributed training on multiple GPUs or TPUs. When you instantiates an `Accelerator` object, it looks at your enviornment and initialise the proper distributed setup, so you can remove the lines that put the model on the device (or, if you prefer, change them to use `accelerator.device` instead of `device`).\n",
    "\n",
    "Then the main bulk of the work is done in the line that sends the dataloaders, the model, and the optimizer to `accelerator.prepare()`. This will wrap those objects in the proper container to make sure your distributed training works as intended. The remaining changes to make are removing the line that puts the batch on the device (again, if you want to keep this you can just change it to use `accelerator.device`) and replacing `loss.backward()` with `accelerator.backward(loss)`.\n",
    "\n",
    "Furthermore, during evaluation just before passing the `predictions` and `refernces` to the `metric.add_batch()` and then compute the metric with `metric.compute()`. We need to *gather* them using `accelerator.gather(torch.argmax(logits, dim=-1))` and `accelerator.gather(batch[\"labels\"])`.\n",
    "\n",
    "> Note: If you are using *Google Collab*, in order to benefit from the speed-up offered by Cloud TPUs, we recommend padding your samples to a fixed length with the `padding=\"max_length\"` and `max_length` arguments of the tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec44d5f4",
   "metadata": {},
   "source": [
    "Lastely, we wiil to define the training loop inside a `training_function()` and to start the training  run\n",
    "```python\n",
    "from accelerate import notebook_launcher\n",
    "\n",
    "notebook_launcher(training_function)\n",
    "```\n",
    "> Note: Putting this in a `train.py` script will make that script runnable on any kind of distributed setup. To try it out in your distributed setup, run the command `accelerate config` which will prompt you to answer a few questions and dump your answers in a configuration file used by this command `accelerate launch train.py`, which will launch the distributed training.\n",
    "\n",
    "\n",
    "So,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61139366",
   "metadata": {},
   "source": [
    "First, prepare the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e818e9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding, AutoModelForSequenceClassification\n",
    "from pprint import pprint\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "\n",
    "raw_datasets = load_dataset(path=\"glue\", name=\"mrpc\")\n",
    "\n",
    "pprint(raw_datasets)\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokeniser = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenisation_function(data):\n",
    "    return tokeniser(data['sentence1'], data['sentence2'], truncation=True)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokeniser)\n",
    "tokenised_datasets = raw_datasets.map(tokenisation_function, batched=True)\n",
    "\n",
    "pprint(tokenised_datasets)\n",
    "\n",
    "tokenised_datasets = tokenised_datasets.remove_columns(\n",
    "    column_names=['sentence1', 'sentence2', 'idx']\n",
    ")\n",
    "tokenised_datasets = tokenised_datasets.rename_column(\n",
    "    original_column_name='label',\n",
    "    new_column_name='labels'\n",
    ")\n",
    "tokenised_datasets.set_format(\"torch\")\n",
    "pprint(tokenised_datasets['train'])\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=tokenised_datasets['train'], \n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    dataset=tokenised_datasets['validation'],\n",
    "    batch_size=8,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    pprint({k: v.shape for k, v in batch.items()})\n",
    "    break\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    outputs = model(**batch)\n",
    "    print(outputs.loss, outputs.logits.shape)\n",
    "    break\n",
    "\n",
    "optimiser = AdamW(\n",
    "    params=model.parameters(),\n",
    "    lr=5e-5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b26853f",
   "metadata": {},
   "source": [
    "Then, initiate the `Accelerator` and `lr_scheduler`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff809a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from transformers import get_scheduler\n",
    "\n",
    "# initiate the Accelerator\n",
    "accelerator = Accelerator()\n",
    "\n",
    "# send the data and model to the accelerator\n",
    "train_dl, eval_dl, model, optimiser = accelerator.prepare(\n",
    "    train_dataloader, eval_dataloader, model, optimiser\n",
    ")\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dl)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimiser,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa8e699",
   "metadata": {},
   "source": [
    "Now, define the *evaluation* function with `accelerator.gather()` to gather predictions and refernces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3398bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "def perform_evaluation():\n",
    "    metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for batch in eval_dl:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            metric.add_batch(\n",
    "                predictions=accelerator.gather(predictions),\n",
    "                references=accelerator.gather(batch[\"labels\"])\n",
    "            )\n",
    "\n",
    "    pprint(metric.compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259b1adf",
   "metadata": {},
   "source": [
    "Finally, define the training loop `training_function()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c133826",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "def training_function():\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in train_dl:\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            accelerator.backward(loss)\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimiser.step()\n",
    "            lr_scheduler.step()\n",
    "            optimiser.zero_grad()\n",
    "\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        perform_evaluation()\n",
    "        model.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1127c924",
   "metadata": {},
   "source": [
    "Launch the training_function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ffc7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import notebook_launcher\n",
    "\n",
    "notebook_launcher(training_function, num_processes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbc7a2e",
   "metadata": {},
   "source": [
    "# Understanding Learning Curves\n",
    "\n",
    "Now that we have learned how to fine-tune a model using both the *Trainer API* and *custom training loop*, it's crucial to understand how to interpret the results using the **Learning curves**. <br />\n",
    "**Learning curves** are visual representations of our model’s performance metrics and they helps us in evaluating our model's performance over time during training. This also helps in identifying any potential issues before the underlying issue causes the model performance to go down. <br />\n",
    "The two most important curves to monitor are:\n",
    "+ *Loss curves*: Show how the model’s error (loss) changes over training steps or epochs.\n",
    "+ *Accuracy curves*: Show the percentage of correct predictions over training steps or epochs.\n",
    "\n",
    "\n",
    "!['The Learning curves.'](data/chapter_3/learning_curves.png \"The Learning curves.\")\n",
    "\n",
    "So, these curves help us understand whether our model is learning effectively and can guide us in making adjustments to improve performance. In Transformers, these metrics are individually computed for each batch and then logged to the disk. We can then use libraries like **livelossplot**, **tensorboard**, **wandb**, etc., to visualize these curves and track our model’s performance over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad2ac42",
   "metadata": {},
   "source": [
    "### Loss curve\n",
    "In a typical successful training run, you’ll see a loss curve similar characteristics below:\n",
    "+ *High initial loss*: The model starts without optimization, so predictions are initially poor.\n",
    "+ *Decreasing loss*: As training progresses, the loss should generally decrease.\n",
    "+ *Convergence*: Eventually, the loss stabilizes at a low value, indicating that the model has learned the patterns in the data.\n",
    "\n",
    "![\"A Healthy Loss Curve\"](data/chapter_3/loss_curve.png \"A Healthy Loss Curve\")\n",
    "\n",
    "Here we can see, the loss curve shows, initially, the loss is high and then it gradually decreases, indicating that the model is improving. A decrease in the loss value suggests that the model is making better predictions, as the loss represents the error between the predicted output and the true output. \n",
    "\n",
    "### Accuracy curve\n",
    "Unlike loss curves, accuracy curves should generally increase as the model learns and can typically include more steps than the loss curve.\n",
    "+ *Start low*: Initial accuracy should be low, as the model has not yet learned the patterns in the data.\n",
    "+ *Increase with training*: Accuracy should generally improve as the model learns, if it is able to learn the patterns in the data.\n",
    "+ *May show plateaus*: Accuracy often increases in discrete jumps rather than smoothly, as the model makes predictions that are close to the true labels.\n",
    "\n",
    "![\"A Healthy Accuracy Curve\"](data/chapter_3/accuracy_curve.png \"A Healthy Accuracy Curve\")\n",
    "\n",
    "The accuracy curve, represents, the accuracy curve begins at a low value and increases as training progresses. Accuracy measures the proportion of correctly classified instances. So, as the accuracy curve rises, it signifies that the model is making more correct predictions.\n",
    "\n",
    "> The reason why a loss curve is much smoother than a accuracy cruve is because, for example, in a binary classifier distinguishing cats (0) from dogs (1), if the model predicts 0.3 for an image of a dog (true value 1), this is rounded to 0 and is an incorrect classification. If in the next step it predicts 0.4, it’s still incorrect. The loss will have decreased because 0.4 is closer to 1 than 0.3, but the accuracy remains unchanged, creating a plateau. The accuracy will only jump up when the model predicts a value greater than 0.5 that gets rounded to 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d08146d",
   "metadata": {},
   "source": [
    "### Convergence\n",
    "\n",
    "We can tell if a model has learned the patterns in the data and that, now it can be used to make predictions on new data, if both the loss and accuracy curves have converged to a stable performance. So, **convergence** occurs when the model’s performance stabilizes and the loss and accuracy curves level off.\n",
    "\n",
    "![\"A converged curve\"](data/chapter_3/converge_curve.png \"A converged curve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5081fc83",
   "metadata": {},
   "source": [
    "# Interpreting Learning Curve Patterns\n",
    "Different curve shapes reveal different aspects of our model’s training  and so during and after the training process, we should monitor the following key indicators:\n",
    "\n",
    "+ **During Training**\n",
    "    1. **Loss convergence**: Is the loss still decreasing or has it plateaued?\n",
    "    2. **Overfitting signs**: Is validation loss starting to increase while training loss decreases?\n",
    "    3. **Learning rate**: Are the curves too erratic (LR too high) or too flat (LR too low)?\n",
    "    4. **Stability**: Are there sudden spikes or drops that indicate problems?\n",
    "\n",
    "\n",
    "+ **After Training**\n",
    "    1. **Final performance**: Did the model reach acceptable performance levels?\n",
    "    2. **Efficiency**: Could the same performance be achieved with fewer epochs?\n",
    "    3. **Generalization**: How close are training and validation performance?\n",
    "    4. **Trends**: Would additional training likely improve performance?\n",
    "\n",
    "\n",
    "### Healthy Learning curves\n",
    "Characteristics of healthy curves:\n",
    "\n",
    "+ **Smooth decline in loss**: Both training and validation loss decrease steadily\n",
    "+ **Close training/validation performance**: Small gap between training and validation metrics\n",
    "+ **Convergence**: Curves level off, indicating the model has learned the patterns\n",
    "\n",
    "![\"Healthy Learning curves\"](data/chapter_3/learning_curves.png \"Healthy Learning curves\")\n",
    "\n",
    "\n",
    "### Overfitting\n",
    "Overfitting occurs when the model learns too much from the training data and is unable to generalize to different data (represented by the validation set).\n",
    "\n",
    "![\"A Overfitted Model Learning Curve\"](data/chapter_3/overfitting.png \"A Overfitted Model Learning Curve\")\n",
    "\n",
    "+ **Symptoms**:\n",
    "    - Training loss contniues to decrese while validation loss increases or plateaus.\n",
    "    - Large gap between training and validation accuracy.\n",
    "    - Training accuracy much higher than validation accuracy.\n",
    "\n",
    "+ **Solution for overfitting**:\n",
    "    - **Regularisation**:  Add dropout, weight decay, or other regularisation techniques.\n",
    "    - **Early stopping**: Stop traininig when validation performance stops improving.\n",
    "    - **Data augmentation**: Increase training data diversity.\n",
    "    - **Reduce Model Complexity**: Use a smaller model or fewer paramaters \n",
    "\n",
    "\n",
    "### Underfitting\n",
    "\n",
    "Underfitting occurs when the model is too simple to capture the underlying patterns in the data. This can happen for several reasons:\n",
    "\n",
    "+ The model is too small or lacks capacity to learn the patterns\n",
    "+ The learning rate is too low, causing slow learning\n",
    "+ The dataset is too small or not representative of the problem\n",
    "+ The model is not properly regularized\n",
    "\n",
    "![\"A Underfitted Model Learning Curve\"](data/chapter_3/underfitting.png \"A Underfitted Model Learning Curve\")\n",
    "\n",
    "+ **Symtoms**:\n",
    "    - Both training and validation loss remain high.\n",
    "    - Model performance plateaus early in training.\n",
    "    - Training accuracy is lower than expected.\n",
    "\n",
    "+ **Solutions for underfitting**\n",
    "    - **Increase model capacity**: Use a larger model ot more parameters.\n",
    "    - **Train longer**: Increase the number of epochs.\n",
    "    - **Adjust learning rate**: Try different learning rates.\n",
    "    - **Check data quality**: Ensure your data is properly preprocessed.\n",
    "\n",
    "### Erratic Learning Curves\n",
    "Erratic learning curves occur when the model is not learning effectively. This can happen for several reasons:\n",
    "+ The learning rate is too high, causing the model to overshoot the optimal parameters.\n",
    "+ The batch size is too small, causing the model to learn slowly\n",
    "+ The model is not properly regularized, causing it to overfit to the training data\n",
    "+ The dataset is not properly preprocessed, causing the model to learn from noise\n",
    "\n",
    "![\"Erratic Learning Curves\"](data/chapter_3/erratic_learning_curves.png \"Erratic Learning Curves\")\n",
    "\n",
    "+ **Symptoms**:\n",
    "    - Frequent fluctuations in loss or accuracy.\n",
    "    - Curves show high variance or instability.\n",
    "    - Performance oscillates without clear trend\n",
    "\n",
    "+ **Solutions for erractic curves**:\n",
    "    - **Lower Learning Rate**: Reduce step size for more stable training.\n",
    "    - **Increase Batch Size**: Larger batches provide more stable gradients.\n",
    "    - **Gradient Clipping**: Precent exploding gradients.\n",
    "    - **Better data preprocessing**: Ensure consistent data quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f29fc33",
   "metadata": {},
   "source": [
    "# Full Training Loop Example\n",
    "\n",
    "Here we will use the **livelossplot** lib to plot the learning curve for simplicity and perform evaluation after every epoch.\n",
    "\n",
    "Prepare the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fb6ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding, AutoModelForSequenceClassification\n",
    "from torch.utils.data import DataLoader\n",
    "from pprint import pprint\n",
    "\n",
    "raw_datasets = load_dataset(path=\"glue\", name=\"mrpc\")\n",
    "\n",
    "pprint(raw_datasets)\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokeniser = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokeniser)\n",
    "\n",
    "def tokenisation_function(data):\n",
    "    return tokeniser(data['sentence1'], data['sentence2'], truncation=True)\n",
    "\n",
    "tokenised_datasets = raw_datasets.map(tokenisation_function, batched=True)\n",
    "\n",
    "pprint(tokenised_datasets)\n",
    "\n",
    "tokenised_datasets = tokenised_datasets.remove_columns(\n",
    "    column_names=[\"sentence1\", \"sentence2\", \"idx\"]\n",
    ")\n",
    "tokenised_datasets = tokenised_datasets.rename_column(\n",
    "    original_column_name = \"label\",\n",
    "    new_column_name = \"labels\"\n",
    ")\n",
    "tokenised_datasets.set_format(\"torch\")\n",
    "\n",
    "pprint(tokenised_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d03d1a9",
   "metadata": {},
   "source": [
    "Now to make sure that our model *converge* to a good stabe performance, we will need to define how the data will be given to the model properly, i.e., the batch size for both the training dataset and evaluation dataset.\n",
    "+ **Training batch** must hold data + gradients in GPU memory; keeping it around 8-32 samples leaves room for the gradients and keeps some randomness (“noise”) that often helps the model generalise.\n",
    "\n",
    "+ **Evaluation batch** only does a forward pass—no gradients—so memory use is lower; stuffing in as many samples as fit (64, 128, or all) finishes validation faster without hurting accuracy.\n",
    "\n",
    "> **Practical rule**: start with the largest train batch that fits your GPU and still gives good validation scores (often 16 ± 8); then crank the eval batch way up because speed is the only concern there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8411e32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 16\n",
    "eval_batch_size = min(128, len(tokenised_datasets[\"validation\"]))\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=tokenised_datasets['train'],\n",
    "    batch_size=train_batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    dataset=tokenised_datasets[\"validation\"],\n",
    "    batch_size=eval_batch_size,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "print(f\"So there are, {len(train_dataloader)} batches of size {train_batch_size} in the training dataset, and\\n {len(eval_dataloader)} batches of size {eval_batch_size} in the evaluation dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960fe67b",
   "metadata": {},
   "source": [
    "Now that we have the right batch size for both *training* and *vaidation* for our model. We now need to focus on selecting the right **hyper-parameters**:\n",
    "+ **Learning-rate (2 × 10⁻⁵)** – Think of this as how big a step the optimizer takes each time it adjusts the weights; 2e-5 is small enough to avoid wild jumps yet big enough for BERT-base to learn a GLUE-sized task in a handful of epochs, and you tune it by doubling or halving if the validation curve diverges or crawls.\n",
    "\n",
    "+ **Warm-up (10 % of steps)** – During the first 10 % of updates the learning-rate ramps smoothly from zero to its full value, preventing a sudden kick that can blow up the loss; a 5–10 % ramp is a widely used safe zone, and you shorten it on very long runs or lengthen it if training spans only a few hundred steps.\n",
    "\n",
    "+ **Linear LR decay** – After warm-up the learning-rate is reduced a little each step until it hits zero, which makes later updates gentler and helps the model settle into a minimum; linear decay is the simplest schedule that works about as well as fancier shapes, so use it unless you have evidence cosine or one-cycle clearly beats it on your dataset.\n",
    "\n",
    "+ **Epochs (5 passes)** – An epoch is one full sweep through the training data; around five passes (≈1 000 updates with a 16-sample batch) are usually enough for a small set like MRPC to converge without starting to memorise, and the reliable way to pick this number is to watch the validation metric and stop when it stops improving for two epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bdccd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "\n",
    "optimiser = AdamW(\n",
    "    params=model.parameters(),\n",
    "    lr=2e-5\n",
    ")\n",
    "\n",
    "accelerator = Accelerator()\n",
    "train_dl, eval_dl, model, optimiser = accelerator.prepare(\n",
    "    train_dataloader, eval_dataloader, model, optimiser\n",
    ")\n",
    "\n",
    "num_epochs = 5\n",
    "num_training_steps = num_epochs * len(train_dl)\n",
    "\n",
    "num_warmup_steps = (10 * num_training_steps)//100\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimiser,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35645822",
   "metadata": {},
   "source": [
    "Now the final training loop with the evaluation process. Here, we need to make sure **gradients flow only through the parts of the code that actually teach the model**: keep them on during the forward-and-backward pass, then immediately cut them off for anything else.\n",
    "\n",
    "+ Wrap the entire validation loop in `torch.no_grad()` because we’re just measuring, not learning—this skips gradient bookkeeping and slashes memory use.\n",
    "\n",
    "+ After the backward pass in training, call `loss.detach().item()` and `logits.detach()` before saving them so they don’t drag the whole computation graph into your Python lists.\n",
    "\n",
    "+ Using `no_grad()` again around metric calculations prevents PyTorch from building a second, useless graph while you tally accuracy or F1.\n",
    "\n",
    "Doing these three things keeps GPU RAM from creeping up, speeds every batch, and guarantees that only the intended updates influence your learning curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5aea96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from livelossplot import PlotLosses\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "def perform_evaluation():\n",
    "    \"\"\"\n",
    "    Perform evaluation on the validation set\n",
    "    \"\"\"\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    eval_epoch_loss = []\n",
    "\n",
    "    eval_metric = evaluate.load(path=\"glue\", config_name=\"mrpc\")\n",
    "\n",
    "    for batch in eval_dl:\n",
    "        # Disable gradient computation for evaluation (saves memory and computation)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "            # Store loss inside no_grad for memory efficiency\n",
    "            eval_epoch_loss.append(outputs.loss.item())\n",
    "\n",
    "            # Get predictions for metrics (logits already created without gradients)\n",
    "            logits = outputs.logits\n",
    "            refs = batch[\"labels\"]\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            # Add batch to evaluation metric\n",
    "            eval_metric.add_batch(\n",
    "                predictions=accelerator.gather(preds),\n",
    "                references=accelerator.gather(refs)\n",
    "            )\n",
    "    \n",
    "    eval_avg_loss = sum(eval_epoch_loss) / len(eval_epoch_loss)\n",
    "    eval_pred_stats = eval_metric.compute()\n",
    "    \n",
    "    return eval_avg_loss, eval_pred_stats\n",
    "\n",
    "\n",
    "def training_function():\n",
    "\n",
    "    # intialise the plotter for the learning curve\n",
    "    plotter = PlotLosses(mode='notebook')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "         # Ensure model is in training mode\n",
    "        model.train()\n",
    "        train_epoch_loss = []\n",
    "\n",
    "        # Create fresh metrics for each epoch to avoid accumulation across epochs\n",
    "        train_metric = evaluate.load(path=\"glue\", config_name=\"mrpc\")\n",
    "\n",
    "        for batch in train_dl:\n",
    "            #  FORWARD PASS (keep gradients attached)\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            # BACKWARD PASS (while gradients are still attached)\n",
    "            accelerator.backward(loss)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimiser.step()\n",
    "            lr_scheduler.step()\n",
    "            optimiser.zero_grad()\n",
    "\n",
    "            # METRICS COMPUTATION (after backward pass is complete)\n",
    "            with torch.no_grad():\n",
    "                # Detach loss for logging to prevent keeping computation graph in memoory\n",
    "                train_epoch_loss.append(loss.detach().item())\n",
    "\n",
    "                # Detach logits for metric computation (no gradients needed for metrics)\n",
    "                logits = outputs.logits.detach()\n",
    "                # No need to detach labels (they don't have gradients)\n",
    "                refs = batch['labels']\n",
    "                preds = torch.argmax(logits, dim=-1)\n",
    "                \n",
    "                # Add batch to training metric\n",
    "                train_metric.add_batch(\n",
    "                    predictions=accelerator.gather(preds),\n",
    "                    references=accelerator.gather(refs)\n",
    "                )\n",
    "\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        # COMPUTE TRAINING METRICS\n",
    "        train_avg_loss = sum(train_epoch_loss)/len(train_epoch_loss)\n",
    "        train_pred_stats = train_metric.compute()\n",
    "\n",
    "        # EVALUATION PHASE\n",
    "        eval_avg_loss, eval_pred_stats = perform_evaluation()\n",
    "\n",
    "        # set back to train mode\n",
    "        model.train()\n",
    "\n",
    "        # update live learning curve\n",
    "        plotter.update({\n",
    "            'loss': train_avg_loss,\n",
    "            'val_loss' : eval_avg_loss,\n",
    "            'acc' : train_pred_stats['accuracy'],\n",
    "            'val_acc' : eval_pred_stats['accuracy'],\n",
    "            'f1' : train_pred_stats['f1'],\n",
    "            'val_f1' : eval_pred_stats['f1']\n",
    "        })\n",
    "        plotter.send()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df06fcc",
   "metadata": {},
   "source": [
    "Finally, launch the training with accelerator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5eda88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import notebook_launcher\n",
    "\n",
    "notebook_launcher(training_function, num_processes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a65534a",
   "metadata": {},
   "source": [
    "We can see from the learning curves above, that the model is being overfitted because the validation loss starts incresing, while only the training loss kept on decresing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9f5487",
   "metadata": {},
   "source": [
    "# Trainer API Example\n",
    "This time we will use the `tensorboard` lib to visualise the learning curves and integate it with the `Trainer` API.\n",
    "\n",
    "First, let's prepare the data (same as before but this time we don't need the `DataLoader`, `DataCollatorWithPadding` because we are using the `Trainer` API):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdc14c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from pprint import pprint\n",
    "\n",
    "raw_datasets = load_dataset(path=\"glue\", name=\"mrpc\")\n",
    "\n",
    "pprint(raw_datasets)\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokeniser = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenisation_function(data):\n",
    "    return tokeniser(data['sentence1'], data['sentence2'], truncation=True)\n",
    "\n",
    "tokenised_datasets = raw_datasets.map(tokenisation_function, batched=True)\n",
    "\n",
    "pprint(tokenised_datasets)\n",
    "\n",
    "tokenised_datasets = tokenised_datasets.remove_columns(\n",
    "    column_names=[\"sentence1\", \"sentence2\", \"idx\"]\n",
    ")\n",
    "tokenised_datasets = tokenised_datasets.rename_column(\n",
    "    original_column_name = \"label\",\n",
    "    new_column_name = \"labels\"\n",
    ")\n",
    "tokenised_datasets.set_format(\"torch\")\n",
    "\n",
    "pprint(tokenised_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ce57ba",
   "metadata": {},
   "source": [
    "define a metric function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db5a643",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "    pred_logits, labels = eval_preds\n",
    "    preds = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    return metric.compute(predictions=preds, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c91696d",
   "metadata": {},
   "source": [
    "Finally, set the `TrainingArguments` and pass it to the `Trainer` API. Below, we are also using the `EarlyStoppingCallback` functonality by `transformers` to do a early stopping if the validation loss (it know which metric to take, when you define `metric_for_best_model` parameter in the `TrainingArguments`) starts to increase, i.e., in the case of *overfitting*.\n",
    "\n",
    "> After starting the cell below, please open the terminal and go to this repo directory. Now, first activate the python venv :\n",
    ">```bash\n",
    ">source .venv/bin/activate\n",
    ">```\n",
    ">then, run the following command to start the tensoboard server and open the link it prompts to see the learning curve plots:\n",
    ">```bash\n",
    ">tensorboard --logdir data/chapter_3/model_results\n",
    ">```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47642b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import EarlyStoppingCallback # for detecting overfitting with early stopping\n",
    "\n",
    "train_batch_size = 16\n",
    "eval_batch_size = min(128,  len(tokenised_datasets[\"validation\"]))\n",
    "num_epochs = 5\n",
    "\n",
    "total_training_steps = num_epochs * (len(tokenised_datasets[\"train\"])//train_batch_size)\n",
    "num_warmup_steps = int(0.1 * total_training_steps)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"data/chapter_3/model_results\",\n",
    "    eval_strategy=\"steps\", \n",
    "    learning_rate=2e-5,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    eval_steps=50,\n",
    "    save_steps=100,\n",
    "    fp16=True, # reduce memory usuage\n",
    "    warmup_steps=num_warmup_steps,\n",
    "    logging_steps=10,  # Log metrics every 10 steps\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=train_batch_size,\n",
    "    per_device_eval_batch_size=eval_batch_size,\n",
    "    report_to=\"tensorboard\",  # Send logs to tensorboard\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenised_datasets[\"train\"],\n",
    "    eval_dataset=tokenised_datasets[\"validation\"],\n",
    "    processing_class=tokeniser,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)] # early stopping to prevent overfitting after 3 steps of bad performance\n",
    ")\n",
    "\n",
    "# Train and automatically log metrics\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca7c366",
   "metadata": {},
   "source": [
    "# The End!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
