{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bacc729e",
   "metadata": {},
   "source": [
    "# Processing The Data\n",
    "\n",
    "To access a dataset we will use the `datasets` lib and for this example we will be using the *MRPC* (*Microsoft Research Paraphrase Corpus*) dataset. The dataset consists of 5,801 pairs of sentences, with a label indicating if they are paraphrases or not (i.e., if both sentences mean the same thing). Furthermore, this is one of the 10 datasets composing the *GLUE* benchmark, which is an academic benchmark that is used to measure the performance of ML models across 10 different text classification tasks. So, we can use this dataset to finetune a *Bert* model (`bert-base-uncased`) to classify paraphrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862ad1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# download the mrpc dataset\n",
    "raw_datasets = load_dataset(\n",
    "    path=\"glue\",\n",
    "    name=\"mrpc\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4617ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadec1ea",
   "metadata": {},
   "source": [
    "As we can see, we get a `DatasetDict` object, which contains three datasets in it, one for training, one for validation and one of testing. Each of them have '*sentence1*', '*sentence2*', '*label*' and '*ids*' as their columns and there are *3668* rows in the training dataset, *408* rows in the validation dataset and *1725* rows in the testing dataset.\n",
    "\n",
    "Now to access any of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d5f961",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_dataset = raw_datasets['train']\n",
    "raw_train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec133067",
   "metadata": {},
   "source": [
    "we see the label column has a integer value of 1, now to see what it corresponds to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e074609",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_dataset.features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd5f700",
   "metadata": {},
   "source": [
    "As we can see, the label column is of type *ClassLabel* and **0** corresponds to **not_equivalent**, and **1** corresponds to **equivalent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db35f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(\"Data at index 15 in the training dataset:\") \n",
    "pprint(raw_train_dataset[15]) \n",
    "print(\"\\nat index 800:\") \n",
    "pprint(raw_train_dataset[800])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25a809f",
   "metadata": {},
   "source": [
    "> Note: As we can see from above the index value when accessing the data  `raw_train_dataset[15]` is not always as same as the *idx* value and that could be because of how the whole main dataset was split into train/valid/test datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d884376f",
   "metadata": {},
   "source": [
    "# Preprocessing The Data\n",
    "\n",
    "In our data we have two sequences as a pair that needs to be processed by the model for classification. But that also means that the tokeniser have to convert the sequences into tokens as a pair and the good thing is the tokensier does that for us by itself, we simply have to pass them togther:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5362778",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "inputs = tokenizer(\"This is the first sentence.\", \"This is the second one.\")\n",
    "pprint(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360e4709",
   "metadata": {},
   "source": [
    "This time we see that the tokeniser returns an additional feature *token_type_ids*,  this is what tells the model which part of the input is the first sentence and which is the second sentence. So,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091e815e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(inputs['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8d0774",
   "metadata": {},
   "source": [
    "```python\n",
    "['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']\n",
    "[      0,      0,    0,     0,       0,          0,   0,       0,      1,    1,     1,        1,     1,   1,       1]\n",
    "```\n",
    "\n",
    "But as we saw earlier, the test dataset alone has 3668 data in it and even though the data is not really big, fitting the data directly to the tokensier is not a good practice because it can easily cause *RAM Out-Of-Memory* issue. And also passing only the sequences to the tokeniser will only return the `input_ids`, `attention_mask`, and `token_type_ids` as the input for the model and this way we will lose other important info that we had in out orignal dataset like `label`. Therefore, we will use `datasets` in-built `map()` method. The map() method works by applying a function on each element of the dataset, so let’s define a function that tokenizes our inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9166b9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokeniser_function(data):\n",
    "    return tokenizer(data['sentence1'], data[\"sentence2\"], truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ca00eb",
   "metadata": {},
   "source": [
    "This function takes a data dictionary and returns a new dictionary with the keys `input_ids`, `attention_mask`, and `token_type_ids`. This will allow us to use the option `batched=True` in our call to `map()`, which will greatly speed up the tokenisation, because this tokeniser can be very fast, but only if we give it lots of inputs at once and using `batched=True` in our call to `map()` passes multiple elements of our dataset at once to the `tokeniser_function()`, and not on each element separately\n",
    "\n",
    "Furthermore, we can also see that we've left the `padding` parameter out in our tokenisation function for now. This is because padding all the samples to the maximum length is not efficient: it’s better to pad the samples when we’re building a batch, as then we only need to pad to the maximum length in that batch, and not the maximum length in the entire dataset. This can save a lot of time and processing power when the inputs have very variable lengths!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5c9db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_datasets = raw_datasets.map(tokeniser_function, batched=True)\n",
    "tokenizer_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cfc990",
   "metadata": {},
   "source": [
    "As we can see, we haven't lost any columns from out data and that there are new columns added (Note that we could also have changed existing fields if our preprocessing function returned a new value for an existing key in the dataset to which we applied `map()`).\n",
    "\n",
    " Also it was really quick to process as well, however, you can make the whole process more faster by passing `num_proc` argument to the `map()` function, as this allows multiprocessing, but since `tokenizers()` already works on multiple threads, there is no use of it here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329096d1",
   "metadata": {},
   "source": [
    "# Dynamic padding\n",
    "Now we will need to do is pad all the examples to the length of the longest element when we batch elements together before passing the input to the model. This technique is refer as **dynamic padding**.\n",
    "\n",
    "Even though this way of padding makes things go faster when utalising a CPU or GPU, that is always not the case when using a accelerator resource like a TPU and that is because TPUs prefer fixed shapes, even when that requires extra padding.\n",
    "\n",
    "![\"Dynamic Padding\"](data/chapter_3/dynamic_padding.png \"Dynamic Padding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f892b0",
   "metadata": {},
   "source": [
    "Here we will use function that is responsible for putting together samples inside a batch, AKA **collate function**. The *collate function* will also apply the correct amount of padding to the items of the dataset we want to batch together. Fortunately, the Transformers library provides us with such a function via `DataCollatorWithPadding`. It takes a tokenizer when you instantiate it (to know which padding token to use, and whether the model expects padding to be on the left or on the right of the inputs) and will do everything you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275383c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1218be0",
   "metadata": {},
   "source": [
    "let's try it on a subset of out dataset and assume it as a single batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3289a128",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = tokenizer_datasets[\"train\"][:10]\n",
    "# filter out the unnecessary columns\n",
    "samples = {k: v for k, v in samples.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"]}\n",
    "[len(x) for x in samples[\"input_ids\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dd948a",
   "metadata": {},
   "source": [
    "Here we can see inside this batch the squences are of different lengths, and to add padding to make them all to the max-length inside this particular batch (which is 67), we simply need to pass the samples to the `data_collator()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6abbb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = data_collator(samples)\n",
    "pprint({k: v.shape for k, v in batch.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ce9149",
   "metadata": {},
   "source": [
    "### Example\n",
    "Let's try to preprocess the **GLUE SST-2** dataset:\n",
    "![\"The GLUE SST-2 Datset\"](data/chapter_3/glue_sst2.png \"The GLUE SST-2 Datset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59835540",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\n",
    "    path='glue',\n",
    "    name='sst2'\n",
    ")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005e5656",
   "metadata": {},
   "source": [
    "As we can see, there is only one sentence per data. So, the `tokenisation_function()` only need to work one sentence per data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f97fc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = 'bert-base-uncased'\n",
    "\n",
    "tokeniser = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenisation_function(data):\n",
    "    return tokeniser(data['sentence'], truncation=True)\n",
    "\n",
    "tokenised_datasets = raw_datasets.map(tokenisation_function, batched=True)\n",
    "tokenised_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b0a05b",
   "metadata": {},
   "source": [
    "Now let's clean the data by removing the unnecessary columns and changing the name of the *label* columns to *labels* and finally convert the datatype to torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb80a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenised_datasets = tokenised_datasets.remove_columns(\n",
    "    column_names=['idx', 'sentence']\n",
    ")\n",
    "tokenised_datasets = tokenised_datasets.rename_column(\n",
    "    original_column_name='label',\n",
    "    new_column_name='labels'\n",
    ")\n",
    "tokenised_datasets = tokenised_datasets.with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8d135f",
   "metadata": {},
   "source": [
    "Now to apply padding using the `data_collator` on the bases of batch we will use the `DataLoader()` function from the class `torch.utils.data`, which creates batches on the given data and passing it to the **collate function**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdff0492",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokeniser)\n",
    "train_dataloader = DataLoader(\n",
    "    tokenised_datasets['train'],\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "for step, batch in enumerate(train_dataloader):\n",
    "    print(batch[\"input_ids\"].shape)\n",
    "    if step > 5:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
