{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "821c3ec5",
   "metadata": {},
   "source": [
    "# Pipelines\n",
    "A pre trained model pipeline method to directly perform the required task on the given or default model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7295d425",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c72f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifies a sentimental text\n",
    "classifier = pipeline(task=\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895f3df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_res1 = classifier(\"If something is bad then it is good!\")\n",
    "classifier_res2 = classifier(\"If something is good then it is bad!\")\n",
    "\n",
    "print(f'{classifier_res1}\\n{classifier_res2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa71f1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = pipeline(task='summarization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0712a629",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"./data/chapter_1/Still_I_Rise.txt\")\n",
    "still_i_rise = f.read()\n",
    "\n",
    "summarizer(still_i_rise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e19ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifies over the given labels\n",
    "zero_shot_classifier = pipeline(\"zero-shot-classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18a5f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_classifier(\n",
    "    \"This is a course about the Transformers library\",\n",
    "    candidate_labels=[\"education\", \"politics\", \"business\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae356bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline(\"text-generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b6d7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate two sentences of 15 words each.\n",
    "generator_results = generator(\n",
    "    'This laptop is the',\n",
    "    num_return_sequences = 2,\n",
    "    max_length=15,\n",
    "    max_new_tokens=15,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "for generator_res in generator_results:\n",
    "    print(generator_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0e772f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in the blank also here unlike above we using a specific model and not the default\n",
    "# Models can be found here, Select the task on the left side and then model on the right:\n",
    "# https://huggingface.co/models?pipeline_tag=fill-mask&sort=trending\n",
    "unmasker = pipeline(task=\"fill-mask\", model='google-bert/bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650257fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_k specifies how many possibilities you want to be displayed\n",
    "possibilities = unmasker(\"This is the [MASK] thing in this world.\", top_k=4)\n",
    "for possibility in possibilities:\n",
    "    print(possibility) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c06dec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Named entity recognition (NER) task is used for linking the input text to their correspond to entities, \n",
    "# such as persons, locations, or organizations.\n",
    "# Here, we are using grouped_entities=True because we want to tell the pipeline to regroup together the parts\n",
    "#  of the sentence that correspond to the same entity, e.g.,\n",
    "# For a text with \"Hugging Face\", the NER needs to recognise \"Hugging” and “Face” as a single organization, \n",
    "# even though the name consists of multiple words\n",
    "ner = pipeline(\"ner\", grouped_entities=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c224592c",
   "metadata": {},
   "outputs": [],
   "source": [
    "recognitions = ner(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")\n",
    "for recognition in recognitions:\n",
    "    print(recognition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0b8765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NER example in part of speech tagging. Part-of-speech (POS) tagging is the process of\n",
    "#  labeling each word in a text with its corresponding grammatical category, such as noun,\n",
    "#  verb, adjective, or adverb, considering both its definition and its context within a sentence.\n",
    "# https://huggingface.co/QCRI/bert-base-multilingual-cased-pos-english\n",
    "ner_pos = pipeline(task='ner', model='QCRI/bert-base-multilingual-cased-pos-english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47da1c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "recognitions_pos = ner_pos(\"Leo is a cat and he lives with his owner in a flat in London.\")\n",
    "\n",
    "for recognition_pos in recognitions_pos:\n",
    "    print(recognition_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ae96fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The question-answering pipeline answers questions from a given context. So,\n",
    "# it does generate a new answer and simply extract the info from the given context.\n",
    "question_answerer = pipeline(\"question-answering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70071b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answerer(\n",
    "    question=\"Where do I work?\",\n",
    "    context=\"My name is Sylvain and I work at Hugging Face in Brooklyn\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364fc88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate from french to english, with the given model:\n",
    "# https://huggingface.co/Helsinki-NLP/opus-mt-fr-en\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7603a017",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator(\"je m'appelle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1704eb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image classification model to classify object in it. We are using this model here:\n",
    "# https://huggingface.co/google/vit-base-patch16-224\n",
    "image_classifier = pipeline(\n",
    "    task=\"image-classification\", model=\"google/vit-base-patch16-224\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c68a630",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = image_classifier(\"./data/chapter_1/Ok_I_Pull_Up.jpg\")\n",
    "\n",
    "# it should be capybara,but I guess they didn't train the model on capybara images that much\n",
    "for res in results:\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb39238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate from audio to text\n",
    "transcriber = pipeline(\n",
    "    task=\"automatic-speech-recognition\", model=\"openai/whisper-large-v3\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce810a25",
   "metadata": {},
   "source": [
    ">Note: you will also need to have ffmpeg in system for the  **automatic-speech-recognition** task pipeline to work. Have a look here regarding ffmpeg:<br />\n",
    "https://ffmpeg.org/download.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da3a367",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcriber(\n",
    "    './data/chapter_1/please_please.mp3'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b253d76",
   "metadata": {},
   "source": [
    "# Tokenisers\n",
    "Tokenisers are mainly used for the task of converting raw input text to a machine readable input. Mainly, there are three type of tokenisation methods:\n",
    "- Word Based Tokenisation\n",
    "- Character Based Tokenisation\n",
    "- Subword Based Tokenisation\n",
    "\n",
    "Even though both Word and Character based tokenisation are very intuitive, they both come with their own limitations; in word-based, very large vocabulaties size leads to memory and time complexity and reducing the vocabulary can lead to large quantity of out-of-vocabulary tokens, also there is loss of meaning across very simalar words (for example, let, let's), and in the character-based, very long character sequences, less meaningful individual character tokens compared to a word token (e.g., 't' compare to 'take'). Therefore, a middle ground of both the tecnhiques is preferred, also known as  Subword Based Tokenisation. <br />\n",
    "Subword tokenization algorithms rely on the principle that frequently used words should not be split into smaller subwords, but rare words should be decomposed into meaningful subwords. For instance \"annoyingly\" might be considered a rare word and could be decomposed into \"annoying\" and \"ly\". Both \"annoying\" and \"ly\" as stand-alone subwords would appear more frequently while at the same time the meaning of \"annoyingly\" is kept by the composite meaning of \"annoying\" and \"ly\". Subword tokenization allows the model to have a reasonable vocabulary size while being able to learn meaningful context-independent representations. In addition, subword tokenization enables the model to process words it has never seen before, by decomposing them into known subwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5c79ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subword based tokenisation\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5e38e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "tokenizer.tokenize(\"Now I know about tokenisation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb15f5b4",
   "metadata": {},
   "source": [
    "> Note: *##* means that this token should be attached to the previous one.\n",
    "\n",
    "After tokenisation, the embedding conversion process take please which converts the tokens into number."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5dfaba",
   "metadata": {},
   "source": [
    "# Bias And Limitations\n",
    "\n",
    "Using pretrained model to fine tune always comes with the bias over the data they were originally trained on and also their archetecture limitations, and so, fine tuning the model on your data won’t make this intrinsic bias disappear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f4dbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d709596",
   "metadata": {},
   "outputs": [],
   "source": [
    "unmasker = pipeline(\"fill-mask\", model=\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a03d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = unmasker(\"This man works as a [MASK].\", top_k=5)\n",
    "print([r[\"token_str\"] for r in result])\n",
    "\n",
    "result = unmasker(\"This woman works as a [MASK].\", top_k=5)\n",
    "print([r[\"token_str\"] for r in result])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ec6043",
   "metadata": {},
   "source": [
    "# The End!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
